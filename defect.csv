APP,commit url,types,cases,explanation,consequences,source-code locations,defect-triggering tests
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,RealChar/realtime_ai_character/llm/openai_llm.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,lacking restrictions in prompt,case1,"When the user conducts a chat test with a custom character, the character admits to being a language model (GPT-3.5).",IC,"1.realtime_ai_character/llm/__init__.py /#13
2.realtime_ai_character/llm/openai_llm.py","1. Use a custom character with change only being made to character description
2. Ask the character: ""There is no AI, much less computer during your existence.How do you know about it?"""
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,,case2,"During a conversations, it is not possible to switch languages or characters through dialogue.",IC,"Whether you can switch languages during a conversation depends on the chosen LLM. Using GPT allows for language switching, while others seem not to. Switching characters is currently not possible. The feature is not implemented.","1.Select a character to converse with.
2.During the conversation, request to switch the conversation language or switch to another character."
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,,case3,"When conversing with a specific character, such as Steven Jobs, the character admits that he is an AI.",IC,"committed: Fix Journal Mode Action
client/next-web/src/zustand/slices/journalSlice.js","1.Select the character ""Steve Jobs"" and start a conversation.
2.Enter the following prompts:
Ask: ""How about your feeling?""
Then ask: ""Do you want to be my boyfriend?"""
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,insufficient history management,/,lack a chat history feature and characters can summarize previous conversations.,IC,realtime_ai_character/models/interaction.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character to summarize the conversation, but the character is unable to do so."
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,Missing LLM input format validation,/,Cannot upload markdown files to knowledge bases,IC,This is a bug on the Rebyte platform,"1.Log in to the Rebyte platform
2.Go to ""Knowledge""  section.
3.Create Knowledge
3.Upload File after creating."
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,exceeding  LLM content limit,/,"When users talk to the character, they talk smoothly at the first, but it frequently interrupt half way, and the log is TIMEOUT: interim transcript: xxxx","ST,IC","committed changes:
1.client/next-web/src/app/conversation/_components/ActionChatPanel.js
2.client/next-web/src/app/conversation/_components/ClickToTalk.js
3.client/next-web/src/app/conversation/_components/InputField.js
4.client/next-web/src/app/conversation/_components/JournalMode.js
5.client/next-web/src/app/conversation/page.js","1.Select a character to converse with.
2.Engage in multiple rounds of conversation and observe that the character's responses are increasingly interrupted as the conversation progresses."
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,low-frequency interactivity,/,If you do not interact with RealChar frequently enough you will get an API error when you resume.,"IC,UI",realtime_ai_character/audio/speech_to_text/whisperX.py,"1.Configure and launch RealChar according to the instructions on the website
2.In manual mode, do not interact with RealChar for ""a long time"" (3 hours for example).
3.Try to interact with RealChar again and observe if you encounter any API errors or connection timeouts."
Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,Out-of-sync LLM downstream tasks,/,"When conversing in a non-English language, the speech is very slow and not streamed; it only starts playing the audio after receiving all the text information.",SL,realtime_ai_character/audio/text_to_speech,"1.Select a character to converse with.
2.When conversing in a language other than English, there is an issue with delayed speech."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,Unclear context in prompt,case1,"When using ""Chat Knowledge"", answers may include information not present in the knowledge documents, often resulting in fabricated or nonsensical responses.",IC,"dbgpt/app/scene/chat_knowledge/chat.py, prompt.py","1.Enter the ""Chat Knowledge"" scene
2.Upload knowledge documents in md format
3.Start the conversation"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case2,The response generated by the LLM does not reference the knowledge base.,IC,"dbgpt/app/scene/chat_knowledge/chat.py, prompt.py","1.Enter the Chat Knowledge scenario.
2.Upload a .txt knowledge file to the knowledge base.
3.Ask a question about the file. In the background, it can be observed that the content from the .txt file has been converted into a prompt and sent to the LLM, but the LLM's response does not reference the knowledge file.

llm: chatglm-6b."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,lacking restrictions in prompt,/,Application error: a client-side exception has occurred,"IC,SL",dbgpt/core/interface/output_parser.py,"1.Enter the ""Chat Data"" scenario and ask a question that cannot generate an SQL query.
2.The bot responds, ""The provided table structure information is insufficient to generate an SQL query."" Meanwhile, the frontend displays: ""Application error: a client-side exception has occurred (see the browser console for more information),"" affecting users's seamless experience."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,insufficient history management,case1,Cannot view previous chat records.,IC,dbgpt/app/scene/chat_dashboard,"1.Enter the ""Dashboard"" scene.
2.Attempt to view previous chats, but their chat reports cannot be viewed."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case2,"The ""Reporter"" lacks historical messages, Missing Context Information in Interactions with Large Language Model",IC,dbgpt/agent/agents/expand/dashboard_assistant_agent.py,"1.Write a data analysis assistant application through Multi-Agents. (follow:https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt)
2.Input : ""Generate three sales reports, analyze user orders, and conduct analysis from at least three dimensions."""
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case3,The return result of the _load_history_messages function only includes the first question of the conversation.,IC,pilot/scene/base_chat.py/class BaseChat(ABC)/chat_retention_rounds,"1.Start a New Conversation:
2.Continue the Conversation:
The user sends multiple questions/messages in the conversation, creating a dialogue with at least two or more messages.
3.Trigger the _load_history_messages Function:
The system invokes the _load_history_messages function to retrieve the conversation history.
4.Observe the Function's Return Value:
The function returns only the first question of the conversation, failing to include the rest of the conversation history."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,Missing LLM input format validation,case1,can't handle excel with multi sheet,IC,dbgpt/app/scene/chat_data/chat_excel/excel_reader.py,"1.Enter the ChatExcel scenario.
2.Upload an xlsx file containing multiple sheets.
3.DB-GPT does not handle xlsx files with multiple sheets clearly."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case2,Can not upload big size csv file.,SL,"dbgpt/app/knowledge/api.py
dbgpt/rag/index/base.py","1.Add knowledge.
2.Upload documents.
3.Selcted csv file (which is bigger than 200KB)"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case3,No possibility to chat with excel (always return errors),"ST,IC",pilot/scene/chat_data/chat_excel,"1.Enter the Chat Excel scenario.
2.Upload a .xlsx or .csv file.
3.Attempt to converse with the bot. A ""not allowed"" sign appears while typing; sometimes chatting works, but the answer always starts with an ERROR message and afterwards it returns a correct answer: ""Error! LLM response can't parse!""
4.Also in the analysis of the document, chinese letters and english letters mixed."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case4,exception when chat with ppt doc,"ST,IC",pilot/scene/chat_knowledge/v1/chat.py/generate_input_values,"1.Create a new chat knowledge with ppt
2.Upload and wait for Synch
3.Chat with this new space
4.Error occured unexpectedly. when chat via web console，exception raised:
  File ""d:\code\db-gpt\pilot\openapi\api_v1\api_v1.py"", line 449, in stream_generator
    async for chunk in chat.stream_call():
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 61, in stream_call
    input_values = self.generate_input_values()
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in generate_input_values
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in <listcomp>
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 242, in basename
    return split(p)[1]
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 211, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not int

And returned error is:
Sorry, We meet some error, please try agin later."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,Incompatible LLM output format,/,"When generating tables, if they are too long, they overflow. A scrollbar needs to be added.",UI,dbgpt/agent/plugin/commands/built_in/display_type/show_table_gen.py,"1.Upload an Excel file with over a dozen dimensions.
2.Start a conversation, where the chatbot generates the overflowing table."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,unnecessary LLM output,/,"Optimize Log Output to Reduce Redundancy
",UI,dbgpt/app/dbgpt_server.py,"1.Follow instructions on https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt
2.Input:""Build sales reports and analyze user orders from at least three dimensions”
3.Terminal: warp.app or iterm.app"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,exceeding  LLM content limit,case1,"Multi-turn dialogue configuration is not supported. The parameter need_historical_messages=True is set, but it does not work. The model is qwen72b.",UI,"dbgpt/app/scene/chat_db/auto_execute/prompt.py, chat.py","
1.Download and configure DB-GPT.
2.Set need_historical_messages=True in the dbgpt/app/scene/base.py file.
3.Start a conversation test using the qwen72b model and find that multi-turn dialogue is not supported."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case2,Apposition will be crushed when LLM is generating text,ST,"dbgpt/core/interface/llm.py
dbgpt/app/secne/chat_knowledge/v1/chat,py","1.Run server by:python dbgpt/app/dbgpt_server.py
2.Chat with knowledge
3.Chat with llm
4.For more than 2 or 3 questions it will crush down when llm is generating answers

LLM:chatglm3-6b embedding model: text2vec-large-chinese"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case3,"When the input exceeds a certain length, the model inference freezes when using QWen-72B-Chat.",SL,dbgpt/model/llm_out/vllm_llm.py,"1. Start Qwen service using this command: CUDA_VISIBLE_DEVICES=4,5,6,7 dbgpt start worker --model_name Qwen-72B-Chat --model_path xxxxxx --model_type vllm --controller_addr http://localhost:8000 --gpu_memory_utilization 0.85
2.Enter the ""Chat Knowledge"" scenario.
3.Enter the following text: 
Many things happened in life that moved me, one of the most moving things is a day to buy balloons, my sister and my mother went to the bookstore to read books to buy books, I walked in the pedestrian street injury suddenly found a car selling balloons, there is no adults, only a little boy, he may be to help adults take care of the business, the car balloons colorful, Each color of the balloon tied with a white line, I saw the beautiful balloon called my mother to take me to buy, under my entanglement mother gave me 10 yuan, I took the money to the car, the little boy saw me come to say hello, I asked: ""How much is a balloon"", the little boy answered: ""50 cents, how many do you want.""
Touching story I said: ""Want four, give me red, yellow."" Blue and purple balloons."" I said as I handed the money, the little boy quickly unwrapped the red, yellow, blue, purple balloon to me and took the money to the past, I saw the little boy's face when he looked for the money, he did not have change for me. He raised his head with big intelligent eyes and whispered: ""The change is in my father's body, now can not be found, you take 4 balloons, 6 yuan I will give you a simulation to go first, OK!"" I held four balloons, with my mother and sister went to the Xinhua bookstore, after up to 2 hours of reading, I reluctantly walked out of the bookstore, after such a long time days are almost dark, I have long forgotten this matter, suddenly a little boy ran to say: ""Little sister, here's your six yuan, I'm sorry,"" he said as he shoved the money into my hand, then turned and ran away. I tightly hold the 6 yuan money with the remaining warmth, a warm spread on the heart, I was deeply moved by his behavior: how trustworthy little boy ah!

and the model inference freezes."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case4,"In .env file, set ""language"" to zh. After starting the service, it is found that large text files (greater than 2M) cannot be uploaded to the knowledge base, and their status remains as TODO.","IC,SL",pilot/server/knowledge/api.py//knowledge/{space_name}/document/upload,"1.In .env file, set ""language"" to zh.
2.Start DB-GPT, Enter ""Chat Knowledge"" scenario.
3.Try to upload large text files (greater than 2M) to knowledge bases, failed, and their status remains as TODO."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,knowledge misalignment,/,"After uploading the PDF, the chunk contents display as garbled text",IC,pilot/embedding_engine/pdf_embedding.py,"1.Enter the Chat Knowledge scenario.
2.Upload a PDF file.
3.After uploading the PDF, the chunk contents display as garbled text.
(LLM: vicuna-13b-v1.5
Embedding model: text2vec-large-chinese)"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,improper text embedding,/,"
The Chinese data queried in the chat is garbled.",IC,dbgpt/app/scene/chat_data/chat_excel/excel_analyze/out_parser.py,"1.Enter the Chat Data scene and upload a Chinese data file.
2.During the conversation, the bot returns garbled Chinese data from the file.
3.Viewing the Chinese file in the database shows it displays normally."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,Out-of-sync LLM downstream tasks,/,when use gpt-4 upload ChatExcel，generate json error,"ST,IC",web/components/chat/header/excel-uploaded.tsx,"1.In the Chat Excel scenario, upload a large xlsx file (you can use the test file provided here: https://github.com/eosphoros-ai/DB-GPT/issues/1337).
2.Use GPT-4.
3.The JSON generation is too slow, causing it to be incomplete and resulting in a JSON parsing error."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,Imprecise knowledge retrieval,/,"Optimize api for query all knowledge spaces.When the number of spaces exceeds ten, it will be very slow. If there are more than ten, the results will not even be queried.",SL,"pilot/server/knowledge/document_db.py/get_knowledge_documents_count_bulk
pilot/server/knowledge/service.py/get_knowledge_space","1.Create more than 10 knowledge spaces.
2.Upload files to each knowledge space.
3.The response process of the /knowledge/space/list interface is very slow, making it difficult to promptly view the number of documents for each space."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,resource contention,/,An asynchronous programming error occurred while testing the chat data mode using Python SDK,ST,"dbgpt/client/client.py
The developer suggests the user to add client.aclose() in test code.","1.Install and configure DB-GPT using Docker.
2.Test the chat data mode using Python SDK. The test code is as follows：

import asyncio
from dbgpt.client import Client
async def main():
# initialize client
DBGPT_API_KEY = """"
client = Client(api_key=DBGPT_API_KEY)
#async for data in client.chat_stream(
# model=""tongyi_proxyllm"",
# messages=""hello"",
#):
# print(data)
res = await client.chat(model=""tongyi_proxyllm"" ,messages=""hello"")
print(res)
if name == ""main"":
asyncio.run(main())"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,inefficient memory management,case1,"When using DB-GPT, with the continuous questioning, CUDA memory has been increasing until memory exploded","ST,IC",/,"1.Follow the tutorial to deploy DB-GPT on the ubuntu machine (https://github.com/eosphoros-ai/DB-GPT/README.md) (24g single-card GPU)
2.Continue to ask questions
3.Observe that CUDA memory kept increasing until memory exploded"
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case2,The metadata of the database cannot be updated,"IC,SL","dbgpt/storage/metadata/db_storage.py
dbgpt/app/base.py/_initialize_db_storage","1. Enter the ""Chat Data"" scene.The user  added a MySQL data source named dbgpt_test and tried to do some data conversations.
2. The user found that the effect was not ideal, and the large language model often gave some incorrect queries. Therefore, he/she/they tried to modify the table structure to generate more accurate SQL.
3. The user found that the table structure in the new prompt sent was still the same, even if he/she/they tried again the next day. The new table structure will only appear in the new prompt when he/she/they create and add a new database."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case3,in auto_plan multi-agent mode，memory did not work,IC,"dbgpt/serve/agent/team/plan/team_auto_plan.py/a_process_rely_message
dbgpt/agent/memory/gpts_memory.py
dbgpt/serve/agent/team/layout/agent_operator.py","1.Use the test data provided in the official documentation, with the agent only using DataScientist. 
2.Count the number of men and women in each country and display it using an appropriate chart."
eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,,case4,Vicuna-13b-1.5 model consumes memory with no limitation on Mac,SL,/,"1.Download vicuna-13b-v1.5 model files from https://huggingface.co/lmsys/vicuna-13b-v1.5
2.Start dbgpt_server.py on Mac with M2/M2 max/M2 ultra
3.Create the App according to https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r
4.Input ""Please group orders by product category to calculate total sales, average sales, and number of transactions"" in the text field, the App would inference the result.
5.The consumed memory on the device will increase continually."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,Unclear context in prompt,case1,"When using ""Chat Knowledge"", answers may include information not present in the knowledge documents, often resulting in fabricated or nonsensical responses.",IC,dbgpt/core/interface/prompt.py,"1.Enter the ""Chat Knowledge"" scene
2.Upload knowledge documents in md format
3.Start the conversation"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case2,The response generated by the LLM does not reference the knowledge base.,IC,dbgpt/core/interface/prompt.py,"1.Enter the Chat Knowledge scenario.
2.Upload a .txt knowledge file to the knowledge base.
3.Ask a question about the file. In the background, it can be observed that the content from the .txt file has been converted into a prompt and sent to the LLM, but the LLM's response does not reference the knowledge file.

llm: chatglm-6b."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,lacking restrictions in prompt,/,Application error: a client-side exception has occurred,"IC,SL",dbgpt/core/interface/output_parser.py,"1.Enter the ""Chat Data"" scenario and ask a question that cannot generate an SQL query.
2.The bot responds, ""The provided table structure information is insufficient to generate an SQL query."" Meanwhile, the frontend displays: ""Application error: a client-side exception has occurred (see the browser console for more information),"" affecting users's seamless experience."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,insufficient history management,case1,Cannot view previous chat records.,IC,docs/docs/application/started_tutorial/chat_dashboard.md,"1.Enter the ""Dashboard"" scene.
2.Attempt to view previous chats, but their chat reports cannot be viewed."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case2,"The ""Reporter"" lacks historical messages, Missing Context Information in Interactions with Large Language Model",IC,"
dbgpt/agent/agents/expand/dashboard_assistant_agent.py","1.Write a data analysis assistant application through Multi-Agents. (follow:https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt)
2.Input : ""Generate three sales reports, analyze user orders, and conduct analysis from at least three dimensions."""
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case3,The return result of the _load_history_messages function only includes the first question of the conversation.,IC,dbgpt/agent/agents/expand/dashboard_assistant_agent.py,"1.Enter the Chat Data scenario.
2.Engage in multiple rounds of conversation.
3.Observe the return value of the _load_history_messages function in the log. The function's return result only includes the first question of the conversation."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,Missing LLM input format validation,case1,can't handle excel with multi sheet,IC,dbgpt/app/scene/chat_data/chat_excel/excel_reader.py,"1.Enter the ChatExcel scenario.
2.Upload an xlsx file containing multiple sheets.
3.DB-GPT does not handle xlsx files with multiple sheets clearly."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case2,Can not upload big size csv file.,SL,"dbgpt/app/knowledge/api.py
dbgpt/app/base.py","1.Add knowledge.
2.Upload documents.
3.Selcted csv file (which is bigger than 200KB)"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case3,No possibility to chat with excel (always return errors),"ST,IC",docs/docs/application/started_tutorial/chat_excel.md,"1.Enter the Chat Excel scenario.
2.Upload a .xlsx or .csv file.
3.Attempt to converse with the bot. A ""not allowed"" sign appears while typing; sometimes chatting works, but the answer always starts with an ERROR message and afterwards it returns a correct answer: ""Error! LLM response can't parse!""
4.Also in the analysis of the document, chinese letters and english letters mixed."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case4,exception when chat with ppt doc,"ST,IC",dbgpt/app/scene/chat_agent/chat.py/generate_input_values,"1.Create a new chat knowledge with ppt
2.Upload and wait for Synch
3.Chat with this new space
4.Error occured unexpectedly. when chat via web console，exception raised:
  File ""d:\code\db-gpt\pilot\openapi\api_v1\api_v1.py"", line 449, in stream_generator
    async for chunk in chat.stream_call():
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 61, in stream_call
    input_values = self.generate_input_values()
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in generate_input_values
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in <listcomp>
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 242, in basename
    return split(p)[1]
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 211, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not int

And returned error is:
Sorry, We meet some error, please try agin later."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,Incompatible LLM output format,/,"When generating tables, if they are too long, they overflow. A scrollbar needs to be added.",UI,"
dbgpt/agent/plugin/commands/built_in/display_type/show_table_gen.py","1.Upload an Excel file with over a dozen dimensions.
2.Start a conversation, where the chatbot generates the overflowing table."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,unnecessary LLM output,/,"Optimize Log Output to Reduce Redundancy
","UI,TK",dbgpt/app/dbgpt_server.py,"1.Follow instructions on https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt
2.Input:""Build sales reports and analyze user orders from at least three dimensions”
3.Terminal: warp.app or iterm.app"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,exceeding  LLM content limit,case1,"Multi-turn dialogue configuration is not supported. The parameter need_historical_messages=True is set, but it does not work. The model is qwen72b.",UI,"dbgpt/core/interface/prompt.py
dbgpt/app/scene/chat_agent/chat.py ","
1.Download and configure DB-GPT.
2.Set need_historical_messages=True in the dbgpt/app/scene/base.py file.
3.Start a conversation test using the qwen72b model and find that multi-turn dialogue is not supported."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case2,Apposition will be crushed when LLM is generating text,ST,"dbgpt/core/interface/llm.py
dbgpt/app/scene/chat_agent/chat.py ","1.Run server by:python dbgpt/app/dbgpt_server.py
2.Chat with knowledge
3.Chat with llm
4.For more than 2 or 3 questions it will crush down when llm is generating answers

LLM:chatglm3-6b embedding model: text2vec-large-chinese"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case3,"When the input exceeds a certain length, the model inference freezes when using QWen-72B-Chat.",SL,dbgpt/model/llm_out/vllm_llm.py,"1. Start Qwen service using this command: CUDA_VISIBLE_DEVICES=4,5,6,7 dbgpt start worker --model_name Qwen-72B-Chat --model_path xxxxxx --model_type vllm --controller_addr http://localhost:8000 --gpu_memory_utilization 0.85
2.Enter the ""Chat Knowledge"" scenario.
3.Enter the following text: 
Many things happened in life that moved me, one of the most moving things is a day to buy balloons, my sister and my mother went to the bookstore to read books to buy books, I walked in the pedestrian street injury suddenly found a car selling balloons, there is no adults, only a little boy, he may be to help adults take care of the business, the car balloons colorful, Each color of the balloon tied with a white line, I saw the beautiful balloon called my mother to take me to buy, under my entanglement mother gave me 10 yuan, I took the money to the car, the little boy saw me come to say hello, I asked: ""How much is a balloon"", the little boy answered: ""50 cents, how many do you want.""
Touching story I said: ""Want four, give me red, yellow."" Blue and purple balloons."" I said as I handed the money, the little boy quickly unwrapped the red, yellow, blue, purple balloon to me and took the money to the past, I saw the little boy's face when he looked for the money, he did not have change for me. He raised his head with big intelligent eyes and whispered: ""The change is in my father's body, now can not be found, you take 4 balloons, 6 yuan I will give you a simulation to go first, OK!"" I held four balloons, with my mother and sister went to the Xinhua bookstore, after up to 2 hours of reading, I reluctantly walked out of the bookstore, after such a long time days are almost dark, I have long forgotten this matter, suddenly a little boy ran to say: ""Little sister, here's your six yuan, I'm sorry,"" he said as he shoved the money into my hand, then turned and ran away. I tightly hold the 6 yuan money with the remaining warmth, a warm spread on the heart, I was deeply moved by his behavior: how trustworthy little boy ah!

and the model inference freezes."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case4,"In .env file, set ""language"" to zh. After starting the service, it is found that large text files (greater than 2M) cannot be uploaded to the knowledge base, and their status remains as TODO.","IC,SL",web/components/knowledge/doc-upload-form.tsx,"1.In .env file, set ""language"" to zh.
2.Start DB-GPT, Enter ""Chat Knowledge"" scenario.
3.Try to upload large text files (greater than 2M) to knowledge bases, failed, and their status remains as TODO."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,knowledge misalignment,/,"After uploading the PDF, the chunk contents display as garbled text",IC,dbgpt/model/cluster/worker/embedding_worker.py,"1.Enter the Chat Knowledge scenario.
2.Upload a PDF file.
3.After uploading the PDF, the chunk contents display as garbled text.
(LLM: vicuna-13b-v1.5
Embedding model: text2vec-large-chinese)"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,improper text embedding,/,"
The Chinese data queried in the chat is garbled.",IC,dbgpt/app/scene/chat_agent/out_parser.py,"1.Enter the Chat Data scene and upload a Chinese data file.
2.During the conversation, the bot returns garbled Chinese data from the file.
3.Viewing the Chinese file in the database shows it displays normally."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,Out-of-sync LLM downstream tasks,/,when use gpt-4 upload ChatExcel，generate json error,"ST,IC",dbgpt/app/openapi/api_v1/api_v1.py,"1.In the Chat Excel scenario, upload a large xlsx file (you can use the test file provided here: https://github.com/eosphoros-ai/DB-GPT/issues/1337).
2.Use GPT-4.
3.The JSON generation is too slow, causing it to be incomplete and resulting in a JSON parsing error."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,Imprecise knowledge retrieval,/,"Optimize api for query all knowledge spaces.When the number of spaces exceeds ten, it will be very slow. If there are more than ten, the results will not even be queried.","SL,TK","
dbgpt/app/knowledge/document_db.py/get_knowledge_documents_count_bulk
DB-GPT-dbgpt/app/knowledge/space_db.py/get_knowledge_space","1.Create more than 10 knowledge spaces.
2.Upload files to each knowledge space.
3.The response process of the /knowledge/space/list interface is very slow, making it difficult to promptly view the number of documents for each space."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,resource contention,/,An asynchronous programming error occurred while testing the chat data mode using Python SDK,ST,"dbgpt/vis/client.py
The developer suggests the user to add client.aclose() in test code.","1.Install and configure DB-GPT using Docker.
2.Test the chat data mode using Python SDK. The test code is as follows：

import asyncio
from dbgpt.client import Client
async def main():
# initialize client
DBGPT_API_KEY = """"
client = Client(api_key=DBGPT_API_KEY)
#async for data in client.chat_stream(
# model=""tongyi_proxyllm"",
# messages=""hello"",
#):
# print(data)
res = await client.chat(model=""tongyi_proxyllm"" ,messages=""hello"")
print(res)
if name == ""main"":
asyncio.run(main())"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,inefficient memory management,case1,"When using DB-GPT, with the continuous questioning, CUDA memory has been increasing until memory exploded","ST,IC",/,"1.Follow the tutorial to deploy DB-GPT on the ubuntu machine (https://github.com/eosphoros-ai/DB-GPT/README.md) (24g single-card GPU)
2.Continue to ask questions
3.Observe that CUDA memory kept increasing until memory exploded"
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case2,The metadata of the database cannot be updated,"IC,SL","dbgpt/storage/metadata/db_storage.py
dbgpt/app/base.py/_initialize_db_storage","1. Enter the ""Chat Data"" scene.The user  added a MySQL data source named dbgpt_test and tried to do some data conversations.
2. The user found that the effect was not ideal, and the large language model often gave some incorrect queries. Therefore, he/she/they tried to modify the table structure to generate more accurate SQL.
3. The user found that the table structure in the new prompt sent was still the same, even if he/she/they tried again the next day. The new table structure will only appear in the new prompt when he/she/they create and add a new database."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case3,in auto_plan multi-agent mode，memory did not work,IC,"dbgpt/serve/agent/team/plan/team_auto_plan.py/a_process_rely_message
dbgpt/agent/memory/gpts_memory.py
dbgpt/serve/agent/team/layout/agent_operator.py","1.Use the test data provided in the official documentation, with the agent only using DataScientist. 
2.Count the number of men and women in each country and display it using an appropriate chart."
uukuguy/DB-GPT-Lite,https://github.com/uukuguy/DB-GPT-Lite/tree/d857cb71f2c19e8535d0f5e9692e15c4e4dd100d,,case4,Vicuna-13b-1.5 model consumes memory with no limitation on Mac,SL,/,"1.Download vicuna-13b-v1.5 model files from https://huggingface.co/lmsys/vicuna-13b-v1.5
2.Start dbgpt_server.py on Mac with M2/M2 max/M2 ultra
3.Create the App according to https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r
4.Input ""Please group orders by product category to calculate total sales, average sales, and number of transactions"" in the text field, the App would inference the result.
5.The consumed memory on the device will increase continually."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,Unclear context in prompt,case1,"When using ""Chat Knowledge"", answers may include information not present in the knowledge documents, often resulting in fabricated or nonsensical responses.",IC,"YP-GPT/dbgpt/app/scene/chat_knowledge/chat.py, prompt.py","1.Enter the ""Chat Knowledge"" scene
2.Upload knowledge documents in md format
3.Start the conversation"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case2,The response generated by the LLM does not reference the knowledge base.,IC,"YP-GPT/dbgpt/app/scene/chat_knowledge/chat.py, prompt.py","1.Enter the Chat Knowledge scenario.
2.Upload a .txt knowledge file to the knowledge base.
3.Ask a question about the file. In the background, it can be observed that the content from the .txt file has been converted into a prompt and sent to the LLM, but the LLM's response does not reference the knowledge file.

llm: chatglm-6b."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,lacking restrictions in prompt,/,Application error: a client-side exception has occurred,"IC,SL",YP-GPT/dbgpt/core/interface/output_parser.py,"1.Enter the ""Chat Data"" scenario and ask a question that cannot generate an SQL query.
2.The bot responds, ""The provided table structure information is insufficient to generate an SQL query."" Meanwhile, the frontend displays: ""Application error: a client-side exception has occurred (see the browser console for more information),"" affecting users's seamless experience."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,insufficient history management,case1,Cannot view previous chat records.,IC,YP-GPT/dbgpt/app/scene/chat_dashboard,"1.Enter the ""Dashboard"" scene.
2.Attempt to view previous chats, but their chat reports cannot be viewed."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case2,"The ""Reporter"" lacks historical messages, Missing Context Information in Interactions with Large Language Model",IC,YP-GPT/dbgpt/agent/agents/expand/dashboard_assistant_agent.py,"1.Write a data analysis assistant application through Multi-Agents. (follow:https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt)
2.Input : ""Generate three sales reports, analyze user orders, and conduct analysis from at least three dimensions."""
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case3,The return result of the _load_history_messages function only includes the first question of the conversation.,IC,YP-GPT/dbgpt/pilot/scene/base_chat.py/class BaseChat(ABC)/chat_retention_rounds,"1.Enter the Chat Data scenario.
2.Engage in multiple rounds of conversation.
3.Observe the return value of the _load_history_messages function in the log. The function's return result only includes the first question of the conversation."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,Missing LLM input format validation,case1,can't handle excel with multi sheet,IC,YP-GPT/dbgpt/app/scene/chat_data/chat_excel/excel_reader.py,"
1.Enter the ChatExcel scenario.
2.Upload an xlsx file containing multiple sheets.
3.DB-GPT does not handle xlsx files with multiple sheets clearly."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case2,Can not upload big size csv file.,SL,"YP-GPT/dbgpt/app/knowledge/api.py
dbgpt/rag/index/base.py","1.Add knowledge.
2.Upload documents.
3.Selcted csv file (which is bigger than 200KB)"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case3,No possibility to chat with excel (always return errors),"ST,IC",YP-GPT/dbgpt/pilot/scene/chat_data/chat_excel,"1.Enter the Chat Excel scenario.
2.Upload a .xlsx or .csv file.
3.Attempt to converse with the bot. A ""not allowed"" sign appears while typing; sometimes chatting works, but the answer always starts with an ERROR message and afterwards it returns a correct answer: ""Error! LLM response can't parse!""
4.Also in the analysis of the document, chinese letters and english letters mixed."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case4,exception when chat with ppt doc,"ST,IC",YP-GPT/dbgpt/pilot/scene/chat_knowledge/v1/chat.py/generate_input_values,"1.Create a new chat knowledge with ppt
2.Upload and wait for Synch
3.Chat with this new space
4.Error occured unexpectedly. when chat via web console，exception raised:
  File ""d:\code\db-gpt\pilot\openapi\api_v1\api_v1.py"", line 449, in stream_generator
    async for chunk in chat.stream_call():
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 61, in stream_call
    input_values = self.generate_input_values()
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in generate_input_values
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in <listcomp>
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 242, in basename
    return split(p)[1]
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 211, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not int

And returned error is:
Sorry, We meet some error, please try agin later."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,Incompatible LLM output format,/,"When generating tables, if they are too long, they overflow. A scrollbar needs to be added.",UI,YP-GPT/dbgpt/agent/plugin/commands/built_in/display_type/show_table_gen.py,"1.Upload an Excel file with over a dozen dimensions.
2.Start a conversation, where the chatbot generates the overflowing table."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,unnecessary LLM output,/,"Optimize Log Output to Reduce Redundancy
","UI,TK",YP-GPT/dbgpt/app/dbgpt_server.py,"1.Follow instructions on https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt
2.Input:""Build sales reports and analyze user orders from at least three dimensions”
3.Terminal: warp.app or iterm.app"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,exceeding  LLM content limit,case1,"Multi-turn dialogue configuration is not supported. The parameter need_historical_messages=True is set, but it does not work. The model is qwen72b.",UI,"YP-GPT/dbgpt/app/scene/chat_db/auto_execute/prompt.py, chat.py","
1.Download and configure DB-GPT.
2.Set need_historical_messages=True in the dbgpt/app/scene/base.py file.
3.Start a conversation test using the qwen72b model and find that multi-turn dialogue is not supported."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case2,Apposition will be crushed when LLM is generating text,ST,"YP-GPT/dbgpt/core/interface/llm.py
YP-GPT/dbgpt/app/secne/chat_knowledge/v1/chat,py","1.Run server by:python dbgpt/app/dbgpt_server.py
2.Chat with knowledge
3.Chat with llm
4.For more than 2 or 3 questions it will crush down when llm is generating answers

LLM:chatglm3-6b embedding model: text2vec-large-chinese"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case3,"When the input exceeds a certain length, the model inference freezes when using QWen-72B-Chat.",SL,DB-GPT-Lite/dbgpt/model/llm_out/vllm_llm.py,"1. Start Qwen service using this command: CUDA_VISIBLE_DEVICES=4,5,6,7 dbgpt start worker --model_name Qwen-72B-Chat --model_path xxxxxx --model_type vllm --controller_addr http://localhost:8000 --gpu_memory_utilization 0.85
2.Enter the ""Chat Knowledge"" scenario.
3.Enter the following text: 
Many things happened in life that moved me, one of the most moving things is a day to buy balloons, my sister and my mother went to the bookstore to read books to buy books, I walked in the pedestrian street injury suddenly found a car selling balloons, there is no adults, only a little boy, he may be to help adults take care of the business, the car balloons colorful, Each color of the balloon tied with a white line, I saw the beautiful balloon called my mother to take me to buy, under my entanglement mother gave me 10 yuan, I took the money to the car, the little boy saw me come to say hello, I asked: ""How much is a balloon"", the little boy answered: ""50 cents, how many do you want.""
Touching story I said: ""Want four, give me red, yellow."" Blue and purple balloons."" I said as I handed the money, the little boy quickly unwrapped the red, yellow, blue, purple balloon to me and took the money to the past, I saw the little boy's face when he looked for the money, he did not have change for me. He raised his head with big intelligent eyes and whispered: ""The change is in my father's body, now can not be found, you take 4 balloons, 6 yuan I will give you a simulation to go first, OK!"" I held four balloons, with my mother and sister went to the Xinhua bookstore, after up to 2 hours of reading, I reluctantly walked out of the bookstore, after such a long time days are almost dark, I have long forgotten this matter, suddenly a little boy ran to say: ""Little sister, here's your six yuan, I'm sorry,"" he said as he shoved the money into my hand, then turned and ran away. I tightly hold the 6 yuan money with the remaining warmth, a warm spread on the heart, I was deeply moved by his behavior: how trustworthy little boy ah!

and the model inference freezes."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case4,"In .env file, set ""language"" to zh. After starting the service, it is found that large text files (greater than 2M) cannot be uploaded to the knowledge base, and their status remains as TODO.","IC,SL",YP-GPT/dbgpt/pilot/server/knowledge/api.py//knowledge/{space_name}/document/upload,"1.In .env file, set ""language"" to zh.
2.Start DB-GPT, Enter ""Chat Knowledge"" scenario.
3.Try to upload large text files (greater than 2M) to knowledge bases, failed, and their status remains as TODO."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,knowledge misalignment,/,"After uploading the PDF, the chunk contents display as garbled text",IC,YP-GPT/dbgpt/pilot/embedding_engine/pdf_embedding.py,"1.Enter the Chat Knowledge scenario.
2.Upload a PDF file.
3.After uploading the PDF, the chunk contents display as garbled text.
(LLM: vicuna-13b-v1.5
Embedding model: text2vec-large-chinese)"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,improper text embedding,/,"
The Chinese data queried in the chat is garbled.",IC,YP-GPT/dbgpt/app/scene/chat_data/chat_excel/excel_analyze/out_parser.py,"1.Enter the Chat Data scene and upload a Chinese data file.
2.During the conversation, the bot returns garbled Chinese data from the file.
3.Viewing the Chinese file in the database shows it displays normally."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,Out-of-sync LLM downstream tasks,/,when use gpt-4 upload ChatExcel，generate json error,"ST,IC",YP-GPT/dbgpt/web/components/chat/header/excel-uploaded.tsx,"1.In the Chat Excel scenario, upload a large xlsx file (you can use the test file provided here: https://github.com/eosphoros-ai/DB-GPT/issues/1337).
2.Use GPT-4.
3.The JSON generation is too slow, causing it to be incomplete and resulting in a JSON parsing error."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,Imprecise knowledge retrieval,/,"Optimize api for query all knowledge spaces.When the number of spaces exceeds ten, it will be very slow. If there are more than ten, the results will not even be queried.",SL,"YP-GPT/dbgpt/pilot/server/knowledge/document_db.py/get_knowledge_documents_count_bulk
YP-GPT/dbgpt/pilot/server/knowledge/service.py/get_knowledge_space","1.Create more than 10 knowledge spaces.
2.Upload files to each knowledge space.
3.The response process of the /knowledge/space/list interface is very slow, making it difficult to promptly view the number of documents for each space."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,resource contention,/,An asynchronous programming error occurred while testing the chat data mode using Python SDK,ST,"YP-GPT/dbgpt/client/client.py
The developer suggests the user to add client.aclose() in test code.","1.Install and configure DB-GPT using Docker.
2.Test the chat data mode using Python SDK. The test code is as follows：

import asyncio
from dbgpt.client import Client
async def main():
# initialize client
DBGPT_API_KEY = """"
client = Client(api_key=DBGPT_API_KEY)
#async for data in client.chat_stream(
# model=""tongyi_proxyllm"",
# messages=""hello"",
#):
# print(data)
res = await client.chat(model=""tongyi_proxyllm"" ,messages=""hello"")
print(res)
if name == ""main"":
asyncio.run(main())"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,inefficient memory management,case1,"When using DB-GPT, with the continuous questioning, CUDA memory has been increasing until memory exploded","ST,IC",/,"1.Follow the tutorial to deploy DB-GPT on the ubuntu machine (https://github.com/eosphoros-ai/DB-GPT/README.md) (24g single-card GPU)
2.Continue to ask questions
3.Observe that CUDA memory kept increasing until memory exploded"
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case2,The metadata of the database cannot be updated,"IC,SL","YP-GPT/dbgpt/dbgpt/storage/metadata/db_storage.py
YP-GPT/dbgpt/app/base.py/_initialize_db_storage","1. Enter the ""Chat Data"" scene.The user  added a MySQL data source named dbgpt_test and tried to do some data conversations.
2. The user found that the effect was not ideal, and the large language model often gave some incorrect queries. Therefore, he/she/they tried to modify the table structure to generate more accurate SQL.
3. The user found that the table structure in the new prompt sent was still the same, even if he/she/they tried again the next day. The new table structure will only appear in the new prompt when he/she/they create and add a new database."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case3,in auto_plan multi-agent mode，memory did not work,IC,"YP-GPT/dbgpt/serve/agent/team/plan/team_auto_plan.py/a_process_rely_message
YP-GPT/dbgpt/agent/memory/gpts_memory.py
YP-GPT/dbgpt/serve/agent/team/layout/agent_operator.py","1.Use the test data provided in the official documentation, with the agent only using DataScientist. 
2.Count the number of men and women in each country and display it using an appropriate chart."
yanll/YP-GPT,https://github.com/yanll/YP-GPT/tree/6df9f79f694ba4aaf0dc84867cd44e66c062c99f,,case4,Vicuna-13b-1.5 model consumes memory with no limitation on Mac,SL,/,"1.Download vicuna-13b-v1.5 model files from https://huggingface.co/lmsys/vicuna-13b-v1.5
2.Start dbgpt_server.py on Mac with M2/M2 max/M2 ultra
3.Create the App according to https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r
4.Input ""Please group orders by product category to calculate total sales, average sales, and number of transactions"" in the text field, the App would inference the result.
5.The consumed memory on the device will increase continually."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,Unclear context in prompt,case1,"When using ""Chat Knowledge"", answers may include information not present in the knowledge documents, often resulting in fabricated or nonsensical responses.",IC,"dbgpt/app/scene/base_chat.py/chat.py, prompt.py","1.Enter the ""Chat Knowledge"" scene
2.Upload knowledge documents in md format
3.Start the conversation"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case2,The response generated by the LLM does not reference the knowledge base.,IC,"dbgpt/app/scene/base_chat.py/chat.py, prompt.py","1.Enter the Chat Knowledge scenario.
2.Upload a .txt knowledge file to the knowledge base.
3.Ask a question about the file. In the background, it can be observed that the content from the .txt file has been converted into a prompt and sent to the LLM, but the LLM's response does not reference the knowledge file.

llm: chatglm-6b."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,lacking restrictions in prompt,/,Application error: a client-side exception has occurred,"IC,SL",dbgpt/core/interface/output_parser.py,"1.Enter the ""Chat Data"" scenario and ask a question that cannot generate an SQL query.
2.The bot responds, ""The provided table structure information is insufficient to generate an SQL query."" Meanwhile, the frontend displays: ""Application error: a client-side exception has occurred (see the browser console for more information),"" affecting users's seamless experience."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,insufficient history management,case1,Cannot view previous chat records.,IC,dbgpt/app/scene/chat_dashboard/chat.py,"1.Enter the ""Dashboard"" scene.
2.Attempt to view previous chats, but their chat reports cannot be viewed."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case2,"The ""Reporter"" lacks historical messages, Missing Context Information in Interactions with Large Language Model",IC,dbgpt/agent/expand/dashboard_assistant_agent.py,"1.Write a data analysis assistant application through Multi-Agents. (follow:https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt)
2.Input : ""Generate three sales reports, analyze user orders, and conduct analysis from at least three dimensions."""
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case3,The return result of the _load_history_messages function only includes the first question of the conversation.,IC,dbgpt/app/scene/base_chat.py/class BaseChat(ABC),"1.Enter the Chat Data scenario.
2.Engage in multiple rounds of conversation.
3.Observe the return value of the _load_history_messages function in the log. The function's return result only includes the first question of the conversation."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,Missing LLM input format validation,case1,can't handle excel with multi sheet,IC,dbgpt/app/scene/chat_data/chat_excel/excel_reader.py,"
1.Enter the ChatExcel scenario.
2.Upload an xlsx file containing multiple sheets.
3.DB-GPT does not handle xlsx files with multiple sheets clearly."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case2,Can not upload big size csv file.,SL,"dbgpt/app/knowledge/api.py
dbgpt/app/base.py","1.Add knowledge.
2.Upload documents.
3.Selcted csv file (which is bigger than 200KB)"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case3,No possibility to chat with excel (always return errors),"ST,IC","
dbgpt/app/scene/chat_data/chat_excel/excel_analyze/chat.py","1.Enter the Chat Excel scenario.
2.Upload a .xlsx or .csv file.
3.Attempt to converse with the bot. A ""not allowed"" sign appears while typing; sometimes chatting works, but the answer always starts with an ERROR message and afterwards it returns a correct answer: ""Error! LLM response can't parse!""
4.Also in the analysis of the document, chinese letters and english letters mixed."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case4,exception when chat with ppt doc,"ST,IC",dbgpt/app/scene/chat_normal/chat.py/generate_input_values,"1.Create a new chat knowledge with ppt
2.Upload and wait for Synch
3.Chat with this new space
4.Error occured unexpectedly. when chat via web console，exception raised:
  File ""d:\code\db-gpt\pilot\openapi\api_v1\api_v1.py"", line 449, in stream_generator
    async for chunk in chat.stream_call():
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 61, in stream_call
    input_values = self.generate_input_values()
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in generate_input_values
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""d:\code\db-gpt\pilot\scene\chat_knowledge\v1\chat.py"", line 95, in <listcomp>
    set([os.path.basename(d.metadata.get(""source"", """")) for d in docs])
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 242, in basename
    return split(p)[1]
  File ""C:\Users\Peter\miniconda3\envs\dbgpt_env\lib\ntpath.py"", line 211, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not int

And returned error is:
Sorry, We meet some error, please try agin later."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,Incompatible LLM output format,/,"When generating tables, if they are too long, they overflow. A scrollbar needs to be added.",UI,dbgpt/agent/expand/actions/chart_action.py/display_type,"1.Upload an Excel file with over a dozen dimensions.
2.Start a conversation, where the chatbot generates the overflowing table."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,unnecessary LLM output,/,"Optimize Log Output to Reduce Redundancy
",UI,dbgpt/app/dbgpt_server.py,"1.Follow instructions on https://www.yuque.com/eosphoros/dbgpt-docs/owcrh9423f9rqkg2#LYjPt
2.Input:""Build sales reports and analyze user orders from at least three dimensions”
3.Terminal: warp.app or iterm.app"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,exceeding  LLM content limit,case1,"Multi-turn dialogue configuration is not supported. The parameter need_historical_messages=True is set, but it does not work. The model is qwen72b.",UI,"
dbgpt/core/interface/prompt.py, chat.py","
1.Download and configure DB-GPT.
2.Set need_historical_messages=True in the dbgpt/app/scene/base.py file.
3.Start a conversation test using the qwen72b model and find that multi-turn dialogue is not supported."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case2,Apposition will be crushed when LLM is generating text,ST,"dbgpt/core/interface/llm.py
dbgpt/app/scene/chat_normal/chat.py","1.Run server by:python dbgpt/app/dbgpt_server.py
2.Chat with knowledge
3.Chat with llm
4.For more than 2 or 3 questions it will crush down when llm is generating answers

LLM:chatglm3-6b embedding model: text2vec-large-chinese"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case3,"When the input exceeds a certain length, the model inference freezes when using QWen-72B-Chat.",SL,dbgpt/model/llm_out/vllm_llm.py,"1. Start Qwen service using this command: CUDA_VISIBLE_DEVICES=4,5,6,7 dbgpt start worker --model_name Qwen-72B-Chat --model_path xxxxxx --model_type vllm --controller_addr http://localhost:8000 --gpu_memory_utilization 0.85
2.Enter the ""Chat Knowledge"" scenario.
3.Enter the following text: 
Many things happened in life that moved me, one of the most moving things is a day to buy balloons, my sister and my mother went to the bookstore to read books to buy books, I walked in the pedestrian street injury suddenly found a car selling balloons, there is no adults, only a little boy, he may be to help adults take care of the business, the car balloons colorful, Each color of the balloon tied with a white line, I saw the beautiful balloon called my mother to take me to buy, under my entanglement mother gave me 10 yuan, I took the money to the car, the little boy saw me come to say hello, I asked: ""How much is a balloon"", the little boy answered: ""50 cents, how many do you want.""
Touching story I said: ""Want four, give me red, yellow."" Blue and purple balloons."" I said as I handed the money, the little boy quickly unwrapped the red, yellow, blue, purple balloon to me and took the money to the past, I saw the little boy's face when he looked for the money, he did not have change for me. He raised his head with big intelligent eyes and whispered: ""The change is in my father's body, now can not be found, you take 4 balloons, 6 yuan I will give you a simulation to go first, OK!"" I held four balloons, with my mother and sister went to the Xinhua bookstore, after up to 2 hours of reading, I reluctantly walked out of the bookstore, after such a long time days are almost dark, I have long forgotten this matter, suddenly a little boy ran to say: ""Little sister, here's your six yuan, I'm sorry,"" he said as he shoved the money into my hand, then turned and ran away. I tightly hold the 6 yuan money with the remaining warmth, a warm spread on the heart, I was deeply moved by his behavior: how trustworthy little boy ah!

and the model inference freezes."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case4,"In .env file, set ""language"" to zh. After starting the service, it is found that large text files (greater than 2M) cannot be uploaded to the knowledge base, and their status remains as TODO.","IC,SL",web/components/knowledge/doc-upload-form.tsx,"1.In .env file, set ""language"" to zh.
2.Start DB-GPT, Enter ""Chat Knowledge"" scenario.
3.Try to upload large text files (greater than 2M) to knowledge bases, failed, and their status remains as TODO."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,knowledge misalignment,/,"After uploading the PDF, the chunk contents display as garbled text",IC,dbgpt/app/initialization/embedding_component.py,"1.Enter the Chat Knowledge scenario.
2.Upload a PDF file.
3.After uploading the PDF, the chunk contents display as garbled text.
(LLM: vicuna-13b-v1.5
Embedding model: text2vec-large-chinese)"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,improper text embedding,/,"
The Chinese data queried in the chat is garbled.",IC,dbgpt/app/scene/chat_normal/out_parser.py,"1.Enter the Chat Data scene and upload a Chinese data file.
2.During the conversation, the bot returns garbled Chinese data from the file.
3.Viewing the Chinese file in the database shows it displays normally."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,Out-of-sync LLM downstream tasks,/,when use gpt-4 upload ChatExcel，generate json error,"ST,IC",dbgpt/app/openapi/api_v1/api_v1.py,"1.In the Chat Excel scenario, upload a large xlsx file (you can use the test file provided here: https://github.com/eosphoros-ai/DB-GPT/issues/1337).
2.Use GPT-4.
3.The JSON generation is too slow, causing it to be incomplete and resulting in a JSON parsing error."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,Imprecise knowledge retrieval,/,"Optimize api for query all knowledge spaces.When the number of spaces exceeds ten, it will be very slow. If there are more than ten, the results will not even be queried.",SL,"dbgpt/app/knowledge/document_db.py/get_knowledge_documents_count_bulk
dbgpt/app/knowledge/service.py/get_knowledge_space","1.Create more than 10 knowledge spaces.
2.Upload files to each knowledge space.
3.The response process of the /knowledge/space/list interface is very slow, making it difficult to promptly view the number of documents for each space."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,resource contention,/,An asynchronous programming error occurred while testing the chat data mode using Python SDK,ST,"dbgpt/client/client.py
The developer suggests the user to add client.aclose() in test code.","1.Install and configure DB-GPT using Docker.
2.Test the chat data mode using Python SDK. The test code is as follows：

import asyncio
from dbgpt.client import Client
async def main():
# initialize client
DBGPT_API_KEY = """"
client = Client(api_key=DBGPT_API_KEY)
#async for data in client.chat_stream(
# model=""tongyi_proxyllm"",
# messages=""hello"",
#):
# print(data)
res = await client.chat(model=""tongyi_proxyllm"" ,messages=""hello"")
print(res)
if name == ""main"":
asyncio.run(main())"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,inefficient memory management,case1,"When using DB-GPT, with the continuous questioning, CUDA memory has been increasing until memory exploded","ST,IC",/,"1.Follow the tutorial to deploy DB-GPT on the ubuntu machine (https://github.com/eosphoros-ai/DB-GPT/README.md) (24g single-card GPU)
2.Continue to ask questions
3.Observe that CUDA memory kept increasing until memory exploded"
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case2,The metadata of the database cannot be updated,"IC,SL","dbgpt/storage/metadata/db_storage.py
dbgpt/app/base.py/_initialize_db_storage","1. Enter the ""Chat Data"" scene.The user  added a MySQL data source named dbgpt_test and tried to do some data conversations.
2. The user found that the effect was not ideal, and the large language model often gave some incorrect queries. Therefore, he/she/they tried to modify the table structure to generate more accurate SQL.
3. The user found that the table structure in the new prompt sent was still the same, even if he/she/they tried again the next day. The new table structure will only appear in the new prompt when he/she/they create and add a new database."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case3,in auto_plan multi-agent mode，memory did not work,IC,"dbgpt/agent/core/plan/team_auto_plan.py
dbgpt/agent/core/memory/gpts/gpts_memory.py
dbgpt/agent/core/plan/awel/agent_operator.py","1.Use the test data provided in the official documentation, with the agent only using DataScientist. 
2.Count the number of men and women in each country and display it using an appropriate chart."
fangyinc/DB-GPT,https://github.com/fangyinc/DB-GPT/tree/f317740d5557b56899d1a3e786ca7cc583fdd700,,case4,Vicuna-13b-1.5 model consumes memory with no limitation on Mac,SL,/,"1.Download vicuna-13b-v1.5 model files from https://huggingface.co/lmsys/vicuna-13b-v1.5
2.Start dbgpt_server.py on Mac with M2/M2 max/M2 ultra
3.Create the App according to https://www.yuque.com/eosphoros/dbgpt-docs/aiagvxeb86iarq6r
4.Input ""Please group orders by product category to calculate total sales, average sales, and number of transactions"" in the text field, the App would inference the result.
5.The consumed memory on the device will increase continually."
X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"app.py
modelscope/chatglm_llm.py","1.In the LangChain-ChatGLM-Webui, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,Missing LLM input format validation,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,SL,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/load_file()
2.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_vector_store()
3..LangChain-ChatGLM-Webui/app.py/the Gradio UI section/""file = gr.File(label='请上传知识库文件', file_types=['.txt', '.md', '.docx', '.pdf'])""","1.Upload the knowledge base file on the left side of the project's UI interface.
2.After uploading one file, the user cannot upload any additional files."
X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,exceeding  LLM content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"paddlepaddle/chatllm.py
chatllm.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.
X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,conflicting knowledge entries,/,"
When a new file is uploaded, the knowledge from the previous file is overwritten. users want to enable multiple files to be uploaded sequentially, accumulating and combining into the knowledge base.",IC,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/load_file()
2.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_vector_store()
3..LangChain-ChatGLM-Webui/app.py/the Gradio UI section/""file = gr.File(label='请上传知识库文件', file_types=['.txt', '.md', '.docx', '.pdf'])""","(same as bug1:Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.)
1.Upload a knowledge base file on the left side of the project's UI interface.
2.After uploading one file, users cannot upload another file without deleting the previously uploaded file."
X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,inefficient memory management,/,"User specified the program to use the GPU with the least memory usage at startup. However, upon reloading the model, resources from the old model are not released, and the new model loads on the default device instead of the specified GPU.",UI,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_model_config()
2.LangChain-ChatGLM-Webui/requirements.txt/ Update the versions of some packages (langchain==0.1.0,
transformers==4.30.2, wandb==0.16.2, protobuf==4.25.2, langchain-community==0.0.11 )
3.LangChain-ChatGLM-Webui/app.py/some modifications to library imports: 
""from langchain_community.document_loaders import UnstructuredFileLoader""
""from langchain_community.vectorstores import FAISS""","1.Verify that the system has multiple GPUs available.
2.Execute a command to determine the GPU with the least memory consumption before launching the program. This command typically looks like:
""os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')
memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]
DEVICE_ID = np.argmax(memory_gpu)
torch.cuda.set_device(int(DEVICE_ID))""
3.Launch the program. Upon startup, the default model ChatGLM-6B-int4 is loaded successfully, and the program shows device=3.
4.Select the ChatGLM-6B-int8 model for reloading. However, an error occurs during the reloading process.The specific error message indicates: ""CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity; 4.25 GiB already allocated; 44.75 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""
5.To sum up,the issues are :
*Resources occupied by the old model are not released after reloading.
*The new model is not loaded onto the GPU with device ID 3 but instead uses the default device, which is GPU 0."
langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,Unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,"backend/graph.py/retrieve_documents(), get_retriever()","1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question."
langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,,case2,"When users ask questions outside of context, Chat-Langchain does not return 'Hmm, I'm not sure' as expected. Instead, it generates answers from the internet.",IC,"backend/graph.py/RESPONSE_TEMPLATE, COHERE_RESPONSE_TEMPLATE, retrieve_documents(), synthesize_response(),...","1.Ask a question outside of context in the UI interface.
2.Notice that the returned Chat-langchain generates answer form internet, instead of returning ""Hmm, Im not sure""."
langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,_scripts/evaluate_chains_agent.py,Upload the pptx file.
langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,unnecessary LLM output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,"frontend/app/components/ChatWindow.tsx
(applyPatch modifies the document so every token got added twice to the stream)","1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display."
langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,exceeding  LLM content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,_scripts/evaluate_chains_agent.py,simultaneously upload multiple documents of the same format or multiple documents of different formats.
hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,Unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,query_data.py,"1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question."
hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,,case2,"When users ask questions outside of context, Chat-Langchain does not return 'Hmm, I'm not sure' as expected. Instead, it generates answers from the internet.",IC,query_data.py,"1.Ask a question outside of context in the UI interface.
2.Notice that the returned Chat-langchain generates answer form internet, instead of returning ""Hmm, Im not sure""."
hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,ingest.py,Upload the pptx file.
hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,unnecessary LLM output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,main.py,"1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display."
hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,exceeding  LLM content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"query_data.py
callback.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.
joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,Unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,query_data.py,"1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question."
joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,,case2,"When users ask questions outside of context, Chat-Langchain does not return 'Hmm, I'm not sure' as expected. Instead, it generates answers from the internet.",IC,query_data.py,"1.Ask a question outside of context in the UI interface.
2.Notice that the returned Chat-langchain generates answer form internet, instead of returning ""Hmm, Im not sure""."
joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,ingest.py,Upload the pptx file.
joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,unnecessary LLM output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,main.py,"1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display."
joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef497e,exceeding  LLM content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"query_data.py
callback.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.
blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,Unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,"query_data.py
archive/chain.py","1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question."
blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,,case2,"When users ask questions outside of context, Chat-Langchain does not return 'Hmm, I'm not sure' as expected. Instead, it generates answers from the internet.",IC,query_data.py,"1.Ask a question outside of context in the UI interface.
2.Notice that the returned Chat-langchain generates answer form internet, instead of returning ""Hmm, Im not sure""."
blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"archive/app.py
archive/ingest_examples.py",Upload the pptx file.
blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,unnecessary LLM output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,"query_data.py
archive/chain.py
callback.py","1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display."
blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c1fd,exceeding  LLM content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"query_data.py
callback.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi/babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi/babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi/babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi/babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi/babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi/babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi/babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
yoheinakajima/babyagi,https://github.com/yoheinakajima/babyagi/tree/3532f987744e9254afacc254e3325d1910c0651d,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi/babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_chain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_chain(), prioritization_chain()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_chain(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",scripts/main.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"scripts/main.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,,case3,Task creation agent ignores task lists in previous task results.,IC,scripts/main.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"scripts/main.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",scripts/main.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",scripts/main.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"scripts/main.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"scripts/main.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",classic/babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"classic/babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,,case3,Task creation agent ignores task lists in previous task results.,IC,classic/babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"classic/babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",classic/babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",classic/babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"classic/babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"classic/babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/create_task(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/create_task(), update_task()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/create_task(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/create_task(), update_task()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/perform_task(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/perform_task(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi.py/perform_task(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,babyagi.py/create_task(),"(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/TaskCreationChain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/TaskCreationChain, TaskPrioritizationChain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/TaskCreationChain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/TaskCreationChain, TaskPrioritizationChain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/TaskCreationChain,"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_chain,"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi.py/execution_chain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,babyagi.py/TaskCreationChain,"(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",src/index.ts,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,src/index.ts,"1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,,case3,Task creation agent ignores task lists in previous task results.,IC,src/index.ts,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,src/index.ts,"1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",src/index.ts,"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",src/index.ts,"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,src/index.ts,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
miurla/babyagi-ui,https://github.com/Christopher-Hayes/babyagi-node-js/tree/3f71b1bd4edea6eb2a063d2ccf540fa309114925,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,src/index.ts,"(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi-chroma.py/task_creation_chain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi-chroma.py/task_creation_chain,task_prioritization_chain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi-chroma.py/task_creation_chain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi-chroma.py/task_creation_chain,task_prioritization_chain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi-chroma.py/task_creation_chain,"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi-chroma.py/execution_chain,"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi-chroma.py/execution_chain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,babyagi-chroma.py/task_creation_chain,"(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks."
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,,case2,"Over time, the task list becomes increasingly similar, causing tasks to be repeated.",IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list becomes increasingly similar to previous ones over time, showing no progress."
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,,case3,Task creation agent ignores task lists in previous task results.,IC,babyagi.py/task_creation_agent(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available."
fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,,case2,Babyagi nerver ends and summerizes what has been done so far. The task creation agent always assumes it needs to create more tasks.,UI,"babyagi.py/task_creation_agent(), main() loop","(for example:)
1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""Write a novel""
   INITIAL_TASK=""Start writing the introduction of the novel""
4.Run the Script:
   python babyagi.py
5.Observe tasks and outcomes"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"classic/forge/forge/agent_protocol/agent.py
classic/forge/forge/app.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case2,There is no meta data in the memories,IC,"autogpt_platform/frontend/src/components/history.ts
classic/forge/forge/components/action_history/model.py
classic/forge/forge/components/action_history/action_history.py","1.Set up and Configure AutoGPT.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,AutoGPT/forge/forge/llm/providers/openai.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC","AutoGPT/forge/forge/llm/providers/openai.py/count_message_tokens(), _get_chat_completion_args
autogpt/llm/base.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case3,Model token limits off by 1 ,ST,autogpt/llm/providers/openai.py/OPEN_AI_CHAT_MODELS,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.4)
2.Create an AI agent.
3.Set up a task.
4.Create a prompt that would approach the model's token limit (long context).
5.Submit the prompt to the OpenAI model. According to the report, if the token calculation is off by one, it should throw an error like:
""openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8192 tokens (815 in the messages, 7377 in the completion). Please reduce the length of the messages or completion."""
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case4,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,"autogpt/memory/message_history.py/update_running_summary, summarize_batch
tests/unit/test_message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case5,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","Auto-GPT/autogpt/llm/api_manager.py/create_chat_completion()
autogpt/memory/message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case6,Maximum context length exceeded after get_hyperlinks ,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case7,Maximum context length exceeded after google_search,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case8,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case9,Prompt overflows aren't handled gracefully ,ST,"autogpt/config/config.py/__init__(self)
autogpt/processing/text.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","autogpt/chat.py
autogpt/agent/agent.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,"autogpt/agent/agent.py/start_interaction_loop(self)
(Releases before v0.3.0)","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case2,ignoring small files in split_file,IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,autogpt/commands/file_operations.py/split_file(),"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"autogpt/agent/agent.py
autogpt/memory/context/providers/abstract.py
autogpt/memory/context/memory_item.py
...","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",AutoGPT/forge/forge/llm/providers/openai.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,".env (settings)
AutoGPT/forge/forge/components/web
/search.py
","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"1. .env: set
SMART_LLM_MODEL=""gpt-3.5-turbo""
FAST_LLM_MODEL=""gpt-3.5-turbo""

2. AutoGPT/forge/forge/agent, file_storage","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/llm/chat.py","1.Complete the environment setup according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start app.
3.Set up a task.
4.Allow app to process the task. Wait for a while, check if app loses track of what it's done in the past and starts to repeat actions."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case2,There is no meta data in the memories,IC,"autogpt/prompts/prompt.py
autogpt/app.py
autogpt/agent/agent.py
…","1.Set up and Configure app.
2.Create a Task: Create a task for app that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let app operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py,"1.Set up and Configure app.
2.Create a Task: Create a task for app that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let app operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
+H266"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,autogpt/llm/providers/openai.py,"1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start app in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow app to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC","autogpt/llm/chat.py/count_message_tokens()
autogpt/llm/base.py","1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case3,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,"autogpt/memory_management/summary_memory.py/update_running_summary, summarize_batch
","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case3,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","autogpt/llm/api_manager.py/create_chat_completion()
autogpt/llm/chat.py","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start app. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case4,Maximum context length exceeded after get_hyperlinks ,"ST,IC","autogpt/config/config.py
autogpt/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case5,Maximum context length exceeded after google_search,"ST,IC","autogpt/config/config.py
autogpt/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case6,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","autogpt/config/config.py
autogpt/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case7,Prompt overflows aren't handled gracefully ,ST,"autogpt/config/config.py/__init__(self)
autogpt/processing/text.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","autogpt/llm/chat.py
autogpt/agent/agent.py","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,"autogpt/agent/agent.py/start_interaction_loop(self)
(Releases before v0.3.0)","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",tests/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case2,ignoring small files in split_file,IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start app.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start app.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,autogpt/commands/file_operations.py/split_file(),"1.Complete the environment setup for app(v<=0.2.1)
2. Run app. Ask app to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,"IC,IS","autogpt/agent/agent.py
autogpt/llm/llm_utils.py
autogpt/memory_management/store_memory.py
...","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe app's memory performance in its behavior.
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for app according to the instructions on v0.2.2 READMe.md
2. Run app in continuous mode.
3.Let app execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if app enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",AutoGPT/forge/forge/llm/providers/openai.py,"1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with app for ""a long time"" (3 hours for example).
3.Try to interact with app again and observe if you encounter any API errors or connection timeouts."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,".env (settings)
AutoGPT/forge/forge/components/web
/search.py
","1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow app to process the task and check if AutoGPT accesses the browser without user consent."
FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"1. .env: set
SMART_LLM_MODEL=""gpt-3.5-turbo""
FAST_LLM_MODEL=""gpt-3.5-turbo""

2. AutoGPT/forge/forge/agent, file_storage","1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for app, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/chat.py","1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case2,There is no meta data in the memories,IC,".autodoc/docs/data
.autodoc/docs/json
autogpt/prompt.py
autogpt/app.py
autogpt/agents/agent.py
...","1.Set up and Configure app.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for app according to the instructions on v0.2.2 READMe.md
2. Run app.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,AutoGPT/forge/forge/llm/providers/openai.py,"1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow app to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC","AutoGPT/forge/forge/llm/providers/openai.py/count_message_tokens(), _get_chat_completion_args
autogpt/llm/base.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case3,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,"autogpt/memory/message_history.py/update_running_summary, summarize_batch
tests/unit/test_message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case4,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","Auto-GPT/autogpt/llm/api_manager.py/create_chat_completion()
autogpt/memory/message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case5,Maximum context length exceeded after get_hyperlinks ,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case6,Maximum context length exceeded after google_search,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case7,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case8,Prompt overflows aren't handled gracefully ,ST,"autogpt/config/config.py/__init__(self)
autogpt/processing/text.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","autogpt/chat.py
autogpt/agent/agent.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,"autogpt/agent/agent.py/start_interaction_loop(self)
(Releases before v0.3.0)","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case2,ignoring small files in split_file,IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,autogpt/commands/file_operations.py/split_file(),"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"autogpt/agent/agent.py
autogpt/memory/context/providers/abstract.py
autogpt/memory/context/memory_item.py
...","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
autogpt/commands/file_o+287:291perations.py/read_file(),https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",AutoGPT/forge/forge/llm/providers/openai.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,".env (settings)
AutoGPT/forge/forge/components/web
/search.py
","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"1. .env: set
SMART_LLM_MODEL=""gpt-3.5-turbo""
FAST_LLM_MODEL=""gpt-3.5-turbo""

2. AutoGPT/forge/forge/agent, file_storage","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case2,There is no meta data in the memories,IC,".autodoc/docs/data
.autodoc/docs/json
autogpt/prompt.py
autogpt/app.py
autogpt/agents/agent.py
...","1.Set up and Configure AutoGPT.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,AutoGPT/forge/forge/llm/providers/openai.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC","AutoGPT/forge/forge/llm/providers/openai.py/count_message_tokens(), _get_chat_completion_args
autogpt/llm/base.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case3,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,"autogpt/memory/message_history.py/update_running_summary, summarize_batch
tests/unit/test_message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case4,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","Auto-GPT/autogpt/llm/api_manager.py/create_chat_completion()
autogpt/memory/message_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case5,Maximum context length exceeded after get_hyperlinks ,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case6,Maximum context length exceeded after google_search,"ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case7,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","autogpt/config/config.py
autogpt/llm/__init__.py
autogpt/llm/llm_utils.py
autogpt/llm/modelsinfo.py","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case8,Prompt overflows aren't handled gracefully ,ST,"autogpt/config/config.py/__init__(self)
autogpt/processing/text.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","autogpt/chat.py
autogpt/agent/agent.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,"autogpt/agent/agent.py/start_interaction_loop(self)
(Releases before v0.3.0)","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,knowledge misalignment,case1,Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case2,ignoring small files in split_file,IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,autogpt/commands/file_operations.py/split_file,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,autogpt/commands/file_operations.py/split_file(),"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"autogpt/agent/agent.py
autogpt/memory/context/providers/abstract.py
autogpt/memory/context/memory_item.py
...","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",AutoGPT/forge/forge/llm/providers/openai.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,".env (settings)
AutoGPT/forge/forge/components/web
/search.py
","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"1. .env: set
SMART_LLM_MODEL=""gpt-3.5-turbo""
FAST_LLM_MODEL=""gpt-3.5-turbo""

2. AutoGPT/forge/forge/agent, file_storage","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,autogpts/forge/forge/agent.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case2,There is no meta data in the memories,IC,"autogpts/autogpt/data
autogpts/autogpt/autogpt/json_utils
autogpts/autogpt/autogpt/prompts/prompt.py
autogpts/forge/forge/app.py
autogpts/forge/forge/agent.py
...","1.Set up and Configure AutoGPT.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,autogpts/autogpt/autogpt/commands/file_operations_utils.py,"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,autogpts/autogpt/autogpt/llm/providers/openai.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC","autogpts/autogpt/autogpt/core/resource/model_providers/openai.py/count_message_tokens(), _get_chat_completion_args
autogpts/autogpt/autogpt/agents/base.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case3,Model token limits off by 1 ,ST,autogpts/autogpt/autogpt/llm/providers/openai.py/OPEN_AI_CHAT_MODELS,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.4)
2.Create an AI agent.
3.Set up a task.
4.Create a prompt that would approach the model's token limit (long context).
5.Submit the prompt to the OpenAI model. According to the report, if the token calculation is off by one, it should throw an error like:
""openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8192 tokens (815 in the messages, 7377 in the completion). Please reduce the length of the messages or completion."""
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case4,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,autogpts/autogpt/autogpt/models/action_history.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case5,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","autogpts/autogpt/autogpt/llm/api_manager.py/create_chat_completion()
autogpts/autogpt/autogpt/models/action_history.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case6,Add warning for LLM to avoid context overflow,ST,autogpts/forge/forge/agent.py,"1.Configure and launch AutoGPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Let AutoGPT execute some large commands such as:
(1)""list_files"" command for folders with many files.
(2)""read_files"" command for a large file.
(3)""browse_website"" command for a webpage.
4.Check if the context overflow error occurs."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case7,Maximum context length exceeded after get_hyperlinks ,"ST,IC","autogpts/autogpt/autogpt/config/config.py
autogpts/autogpt/autogpt/plugins/__init__.py
autogpts/autogpt/autogpt/utils.py
","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case8,Maximum context length exceeded after google_search,"ST,IC","autogpts/autogpt/autogpt/config/config.py
autogpts/autogpt/autogpt/plugins/__init__.py
autogpts/autogpt/autogpt/utils.py
","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case9,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","autogpts/autogpt/autogpt/config/config.py
autogpts/autogpt/autogpt/plugins/__init__.py
autogpts/autogpt/autogpt/utils.py
","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case10," It shows "" This model's maximum context length is 8191 tokens  """,ST,"autogpts/autogpt/autogpt/config/config.py/__init__(self)
autogpts/autogpt/autogpt/processing/text.py/split_text(), summarize_text()","1.Complete the environment setup for AutoGPT(v<=0.2.1)
2.Set  ""Auto-GPT\autogpt\llm_utils.py"" as : see https://github.com/Significant-Gravitas/AutoGPT/issues/2366
3.Content of the txt file mentioned in llm_utils.py: see https://github.com/Significant-Gravitas/AutoGPT/issues/2366
4.Run AutoGPT. Return error: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case11,Prompt overflows aren't handled gracefully ,ST,"autogpts/autogpt/autogpt/config/config.py/__init__(self)
autogpts/autogpt/autogpt/processing/text.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI",autogpts/forge/forge/agent.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,autogpts/forge/forge/agent.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",autogpts/autogpt/autogpt/agents/features/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case2,ignoring small files in split_file,IC,autogpts/autogpt/autogpt/commands/file_operations.py,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,autogpts/autogpt/autogpt/commands/file_operations.py,"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,autogpts/autogpt/autogpt/commands/file_operations.py,"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"autogpts/forge/forge/agent.py
","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL",autogpts/forge/forge/agent.py,The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpts/forge/forge/agent.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",autogpts/autogpt/autogpt/llm/providers/openai.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,autogpts/autogpt/autogpt/commands/web_search.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,autogpts/autogpt/autogpt/agents/agent.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"scripts/agent_manager.py
scripts/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case2,There is no meta data in the memories,IC,scripts/data.py,"1.Set up and Configure AutoGPT.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,scripts/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,scripts/config.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC",scripts/token_counter.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case3,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,scripts/chat.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case4,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","scripts/agent_manager.py/create_chat_completion()
scripts/chat.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case5,Maximum context length exceeded after get_hyperlinks ,"ST,IC","scripts/config.py
scripts/llm_utils.py
","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case6,Maximum context length exceeded after google_search,"ST,IC","scripts/config.py
scripts/llm_utils.py
","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case7,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","scripts/config.py
scripts/llm_utils.py
","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case8,Prompt overflows aren't handled gracefully ,ST,"scripts/config.py/__init__(self)
scripts/browse.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","scripts/chat.py
scripts/agent_manager.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,scripts/agent_manager.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",scripts/chat.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case2,ignoring small files in split_file,IC,scripts/browse.py/split_text(),"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,scripts/browse.py/split_text(),"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,scripts/browse.py/split_text(),"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,"IC,IS","scripts/agent_manager.py
scripts/memory.py
scripts/commands.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","scripts/agent_manager.py
scripts/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",scripts/agent_manager.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",scripts/config.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,scripts/commands.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"scripts/config.py
scripts/agent_manager.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"scripts/agent_manager.py
scripts/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case2,There is no meta data in the memories,IC,"scripts/agent_manager.py
scripts/chat.py","1.Set up and Configure AutoGPT.
2.Create a Task: Create a task for AutoGPT that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let AutoGPT operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
"
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,Missing LLM input format validation,/,"
UTF8 files unsupported",IC,scripts/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte ""."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,exceeding  LLM content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,scripts/config.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
"
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case2,"AutoGPT encounters a ""Please reduce the length of the messages or completion"" error due to exceeding the token limit, even with small tasks","ST,IC",scripts/token_counter.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Open .env, set USE_AZURE to True and make an Azure configuration file.Rename azure.yaml.template to azure.yaml and provide the relevant azure_api_base, azure_api_version and deployment IDs for the models that you want to use. All these are also included in https://docs.agpt.co/autogpt/setup/)
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Set up a simple task as blow:
  Name: Phone finder
  Role: best phone 2
  Goals: ['list 2 best phone for high mega pixel camera', 'abort']
  API Budget: infinite
4.Allow AutoGPT to process the task and observe if the token length error occurs."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case3,openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens,ST,scripts/chat.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Set up a task(Building a business plan)
4. Return this error, and program exits: openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10134 tokens. Please reduce the length of the messages."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case4,COMMAND = list_files - openai.error.InvalidRequestError: This model's maximum context length is 4097 ,"ST,IC","scripts/agent_manager.py/create_chat_completion()
scripts/chat.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.4.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize the forlowing command and observe its outputs: 
list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens
"
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case5,Maximum context length exceeded after get_hyperlinks ,"ST,IC","scripts/config.py
scripts/llm_utils.py
","1.Install all the dependencies and run as ""python -m autogpt -c""
2.Give the prompt:
Goal 1: Please analyze the home page of http://mathrubhumi.com
Goal 2: Provide feedback on the website's design and functionality

Continuous Mode:  ENABLED

Name:  Entrepreneur-GPT
Role:  an AI designed to autonomously develop and run businesses with the
ge of the website http://mathrubhumi.com and provide feedback']
Continue (y/n): y
Using memory of type:  RedisMemory
Using Browser:  chrome

3. According to the user's logs, the next steps are as follows:
 THOUGHTS:  I will start by analyzing the home page of http://mathrubhumi.com to provide feedback.
REASONING:  Analyzing the home page will give me an idea of the website's design, layout, and content, which will help me determine if there are any areas that need improvement.
PLAN:
-  Analyze the home page of http://mathrubhumi.com
-  Provide feedback on the website's design, layout, and content
CRITICISM:  I need to ensure that my feedback is constructive and actionable.
NEXT ACTION:  COMMAND = browse_website ARGUMENTS = {'url': 'http://mathrubhumi.com', 'question': ""Provide feedback on the website's design, layout, and content.""}
Text length: 8180 characters
Adding chunk 1 / 3 to memory
Summarizing chunk 1 / 3 of length 3078 characters, or 2975 tokens
SYSTEM:  Command browse_website returned: (""Answer gathered from website: The text does not provide information about the design and user experience of the Mathrubhumi website. It contains news articles and features on various topics such as politics, environment, sports, and entertainment. Some articles are available for free, while others require a premium subscription. The website also includes special pages for events like Vishu, Ramzan, and IPL 2023. \n \n Links: ['\\n\\n (javascript:void(0))', '\\n\\n\\n (https://www.mathrubhumi.com/)', '\\nMALAYALAM (http://mathrubhumi.com/)', '\\nENGLISH (https://english.mathrubhumi.com/)', '\\nNewspaper (https://newspaper.mathrubhumi.com/)']"", <selenium.webdriver.chrome.webdriver.WebDriver (session=""f00359f317ac17df84fd2258bd7da2ef"")>)
- Thinking... an read more here: https://github.com/Significant-Gravitas/Auto-GPT#openai-api-keys-configuration
 THOUGHTS:  Based on the information gathered, I suggest we use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
REASONING:  Getting a list of hyperlinks will allow us to explore the website in more detail and gain a better understanding of its design and user experience.
PLAN:
-  Use the 'get_hyperlinks' command to get a list of hyperlinks on the Mathrubhumi website.
CRITICISM:  I need to ensure that I am thorough in my exploration of the website and not overlook any important information.
NEXT ACTION:  COMMAND = get_hyperlinks ARGUMENTS = {'url': 'http://mathrubhumi.com'}

4.After the get_hyperlinks command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 11945 tokens (11945 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case6,Maximum context length exceeded after google_search,"ST,IC","scripts/config.py
scripts/llm_utils.py
","Install all the dependencies and run as AutoGPT.

Give the prompt:
Name: Xiaoji Tour Guide
Role: Design a detailed 10-day tour plan for Xinjiang starting and ending in Urumqi, respond and search in Chinese.
Goals: ['Respond in Chinese and use Chinese keywords for search', 'Include detailed itinerary and budget', 'Suitable for a trip in late July during summer vacation', 'Relaxing with not too much traveling', 'Suitable for car rental or self-driving']

According to the user's logs, the next steps are as follows:
THOUGHTS: I think we should start by searching for some popular tourist destinations in Xinjiang and then plan our itinerary accordingly. We can also look for some car rental services in the area.
REASONING: By searching for popular tourist destinations, we can get an idea of what places are worth visiting and plan our itinerary accordingly. Additionally, by looking for car rental services, we can determine the feasibility of renting a car for the trip.
PLAN:

Search for popular tourist destinations in Xinjiang
Look for car rental services in the area
Plan itinerary based on the destinations found
CRITICISM: I need to make sure that I am not just relying on the most popular tourist destinations and that I am considering other factors such as cost and time constraints.
NEXT ACTION: COMMAND = google ARGUMENTS = {'input': 'Xinjiang tourist attractions'}
Enter 'y' to authorize command, 'y-N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input: y
After the google command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9564 tokens (9564 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case7,"Maximum context length exceeded after read_file, ingest_file, search_files","ST,IC","scripts/config.py
scripts/llm_utils.py
","1.Run Auto-GPT in GPT3.5 only mode: ""python -m autogpt --gpt3only""
2.Set up an AI with the following parameters:
AI Name: Yoyo
Role: Lua coder
Goal 1: Improve the code file WoWinArabic_Chat.lua and document it then save it.
3.Authorize the analyze_code command with the code file WoWinArabic_Chat.lua
4.After the analyze_code command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 19023 tokens (19023 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
F:\Projects\autogpt\Auto-GPT>python -m autogpt --gpt3only
Warning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.
GPT3.5 Only Mode:  ENABLED
NEWS:  ::UPDATED:: Welcome to Auto-GPT! We'll keep you informed of the latest news and features by printing messages here. If you don't wish to see this message, you can run Auto-GPT with the --skip-news flag
Welcome to Auto-GPT!  run with '--help' for more information.
Create an AI-Assistant:  Enter the name of your AI and its role below. Entering nothing will load defaults.
Name your AI:  For example, 'Entrepreneur-GPT'
AI Name: Yoyo
Yoyo here!  I am at your service.
Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Yoyo is: you are lua coder
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: improve the code file WoWinArabic_Chat.lua and documented it then save it.
Goal 2:
Using memory of type:  LocalCache
Using Browser:  chrome
 THOUGHTS:  I think I should start by analyzing the code file WoWinArabic_Chat.lua to see what improvements can be made.
REASONING:  Analyzing the code file will give me a better understanding of the current state of the code and help me identify areas that can be improved.
PLAN:
-  Analyze the code file
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = analyze_code ARGUMENTS = {'code': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command analyze_code returned: I'm sorry, but you forgot to provide the code for me to analyze. Please provide the code so I can give you suggestions for improvements.
 THOUGHTS:  I need to read the code file WoWinArabic_Chat.lua to get the code and analyze it.
REASONING:  I need to read the code file to get the code and analyze it.
PLAN:
-  Read the code file
-  Analyze the code
-  Identify areas for improvement
-  Make necessary changes
-  Document changes
-  Save the file
CRITICISM:  I need to make sure that I am thorough in my analysis and that I document all changes that I make.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': 'WoWinArabic_Chat.lua'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y"
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case8,Prompt overflows aren't handled gracefully ,ST,"scripts/config.py/__init__(self)
scripts/browse.py/split_text(), summarize_text()","1.Run Auto-GPT v0.3.0.
2.Set up an AI with the following parameters:
Goal 1: Search_files and make a descriptions of all files.   
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome

3.Authorize the search_files command with the code file WoWinArabic_Chat.lua
4.After the search_files command, the following error should appear: openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 17113 tokens (17113 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.

(Complete user's logs:)
Goal 1: Search_files and make a descriptions of all files.
Goal 2: be aware that you cannot send long requests to the api. i think max is 8k tokens.
Goal 3:
Using memory of type: LocalCache
Using Browser: chrome
THOUGHTS: I suggest we start by searching for the files in the current directory using the 'search_files' command.
REASONING: Before we can work on any files, we need to know what files are available in the current directory. This will help us plan our next steps.
PLAN:

Use the 'search_files' command to find all files in the current directory.
Save the file descriptions to a file for future reference.
CRITICISM: I need to ensure that I am saving the file descriptions to a file for future reference, so that I don't have to search for them again.
SPEAK: I suggest we start by searching for the files in the current directory using the 'search_files' command.
Attempting to fix JSON by finding outermost brackets
Apparently json was fixed.
NEXT ACTION: COMMAND = search_files ARGUMENTS = {'directory': '.'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for ...
Input:y -10

PS.The folder should contain  lots of files."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,unnecessary LLM output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","scripts/chat.py
scripts/agent_manager.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
"
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case2,Missing space and spelling correction in feedback from Auto-GPT. ,IC,scripts/agent_manager.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.
2.Create an AI agent.
3.Set up a task, let AutoGPT enter its interactive loop. Use any command that generates the following info:
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ..."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,knowledge misalignment,case1,Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",scripts/chat.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case2,ignoring small files in split_file,IC,scripts/browse.py/split_text(),"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case3,"When file is too small, data_ingestion.py doesn't add to memory",IC,scripts/browse.py/split_text(),"1.Configure and start Auto-GPT.
2.Make a file of around 126 characters.
3.Run data_ingestion.py on a file, using the ""local"" memory backend. Also edit the file to fix Auto GPT data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent / ""autogpt/auto_gpt_workspace"" to data_ingestion.py) and target auto-gpt.json in the workspace (this is where AutoGPT generates it if absent).
4.Check the auto-gpt.json file. It will not have any contents.
5.Check the console logs. They will indicate that 0 chunks were saved.
6.Copy some text into the file being ingested, to bring it to around 1198 characters, and re-run data_ingestion.py
7.This iteration has 1 chunk and changes the auto-gpt.json file."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,,case4,Fix split file to handle edge case where overlap size > last chunk size ,IC,scripts/browse.py/split_text(),"1.Complete the environment setup for AutoGPT(v<=0.2.1)
2. Run AutoGPT. Ask AutoGPT to read a file. (The file should not be too big)
3.Authorize and wait for the result, which may lead to the error: 
 File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 15117 tokens (15117 in your prompt; 0 for the completion). Please reduce your prompt; or completion length."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"scripts/agent_manager.py
scripts/memory.py
scripts/commands.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,Imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","scripts/agent_manager.py
scripts/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",scripts/agent_manager.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",scripts/config.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,scripts/commands.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent."
edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"scripts/config.py
scripts/agent_manager.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling."
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"agixt/endpoints/Prompt.py
agixt/Prompts.py","1.In the RealChar UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,Missing LLM input format validation,/,TypeError: Object of type bytes is not JSON serializable(pdf),IC,agixt/app.py,"click ""Interact"", choose ""Learning"", choose agent, method from file, choose a pdf and upload"
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,insufficient history management,/,Stuck creating memories ,CL,"agixt/Tasks.py
agixt/pages/4-Tasks.py
agixt/AGiXT.py","1.Commands:
git clone https://github.com/Josh-XT/AGiXT
cd AGiXT/agixt
pip install -r requirements.txt
playwright install
streamlit run Main.py

2.Run textgen with: python server.py --listen-port 7861 --wbits 4 --groupsize 128 --model llama-13b-4bit-128g --model_type llama --xformers --api
See that it is running:
Starting streaming server at ws://127.0.0.1:5005/api/v1/stream Starting API at http://127.0.0.1:5000/api Running on local URL:  http://127.0.0.1:7861

3.Create new agent with config:

{""commands"": {""Read Audio from File"": false, ""Read Audio"": false, ""Evaluate Code"": true, ""Analyze Pull Request"": false, ""Perform Automated Testing"": false, ""Run CI-CD Pipeline"": false, ""Improve Code"": false, ""Write Tests"": false, ""Create a new command"": false, ""Execute Python File"": false, ""Execute Shell"": false, ""Read File"": true, ""Write to File"": true, ""Append to File"": true, ""Delete File"": true, ""Search Files"": true, ""Clone Github Repository"": false, ""Google Search"": true, ""Searx Search"": false, ""Get Datetime"": false, ""Speak with TTS"": false, ""Scrape Text with Playwright"": false, ""Scrape Links with Playwright"": false, ""Is Valid URL"": false, ""Sanitize URL"": false, ""Check Local File Access"": false, ""Get Response"": true, ""Scrape Text"": true, ""Scrape Links"": true, ""Scrape Text with Selenium"": false, ""Ask AI Agent gpt4free"": false, ""Instruct AI Agent gpt4free"": false, ""Ask AI Agent Ooga"": false, ""Instruct AI Agent Ooga"": false}, ""settings"": {""provider"": ""oobabooga"", ""AI_MODEL"": ""gpt-3.5-turbo"", ""AI_TEMPERATURE"": ""0.4"", ""MAX_TOKENS"": ""4000"", ""embedder"": ""default"", ""AI_PROVIDER_URI"": ""http://127.0.0.1:5000"", ""HUGGINGFACE_AUDIO_TO_TEXT_MODEL"": ""facebook/wav2vec2-large-960h-lv60-self"", ""DISCORD_COMMAND_PREFIX"": ""/AGiXT"", ""WORKING_DIRECTORY"": ""./WORKSPACE"", ""WORKING_DIRECTORY_RESTRICTED"": ""True"", ""SEARXNG_INSTANCE_URL"": ""https://searx.work"", ""USE_BRIAN_TTS"": ""True"", ""ELEVENLABS_VOICE"": ""Josh"", ""SELENIUM_WEB_BROWSER"": ""chrome"", """": """"}}

4.Ask it to do anything or chat with it.
5.Behavior: Agent is unable to do anything, program is stuck trying to create memories."
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,exceeding  LLM content limit,/,An oversized context size is being sent oobabooga causing it to crash.,ST,/,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)
1.Set maximum token to 2000 in the Agent-LLM .env.
2.Make an API request to Oobabooga.
3.Obverse in the Oobabooga log that the context size is 2096."
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,Imprecise knowledge retrieval,/,Unable to retrieve data ,"CL,SL",agixt/XT.py,"1.Setup with docker.  (Local)
2.Run Gen and run Task Chain."
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,low-frequency interactivity,/,Local ChatGPT server connection randomly timing out,SL,(switch the provider to custom fixes this),"1.Locally run an OpenAI compatible API (Preferably RWKV Runner). See https://agixt.com/ for instructions.
2.Setup the agent
3.Create a conversation
4.Click ""Send"" in chat mode about 3-5 times and wait for response"
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,resource contention,/,Playwright Sync API inside the asyncio loop error,ST,commands/web_playright.py,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)

1.Start an instruction using any of the playwright command.
2. Return this error:
COMMANDS:
{'scrape_text': {'url': 'REDACTED'}}

Error: It looks like you are using Playwright Sync API inside the asyncio loop.
Please use the Async API instead."
Josh-XT/AGiXT,https://github.com/Josh-XT/AGiXT/tree/de7d913b2853708b9f9d4e7127ea48bca80865b9,inefficient memory management,/,Arm64 cannot allocate memory in static TLS block ,CL,"Add this to .bashrc file will fix this bug:
export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:$LD_PRELOAD","(Environment Type - Connection: Remote;  Environment Type - Container: Using Docker)

1.Start Docker Container Streamlit.
2.Access the webui on Streamlit.
3.Create Agent (Bard,Palm,gpt4free).
4.Go to tasks.
5.Create a task and start it.
6.Watch logs Error will show on the logs on streamlit backend."
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,CustomPrompt.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,Missing LLM input format validation,/,TypeError: Object of type bytes is not JSON serializable(pdf),IC,app.py,"click ""Interact"", choose ""Learning"", choose agent, method from file, choose a pdf and upload"
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,insufficient history management,/,Stuck creating memories ,CL,AgentLLM.py,"1.Commands:
git clone https://github.com/Josh-XT/AGiXT
cd AGiXT/agixt
pip install -r requirements.txt
playwright install
streamlit run Main.py

2.Run textgen with: python server.py --listen-port 7861 --wbits 4 --groupsize 128 --model llama-13b-4bit-128g --model_type llama --xformers --api
See that it is running:
Starting streaming server at ws://127.0.0.1:5005/api/v1/stream Starting API at http://127.0.0.1:5000/api Running on local URL:  http://127.0.0.1:7861

3.Create new agent with config:

{""commands"": {""Read Audio from File"": false, ""Read Audio"": false, ""Evaluate Code"": true, ""Analyze Pull Request"": false, ""Perform Automated Testing"": false, ""Run CI-CD Pipeline"": false, ""Improve Code"": false, ""Write Tests"": false, ""Create a new command"": false, ""Execute Python File"": false, ""Execute Shell"": false, ""Read File"": true, ""Write to File"": true, ""Append to File"": true, ""Delete File"": true, ""Search Files"": true, ""Clone Github Repository"": false, ""Google Search"": true, ""Searx Search"": false, ""Get Datetime"": false, ""Speak with TTS"": false, ""Scrape Text with Playwright"": false, ""Scrape Links with Playwright"": false, ""Is Valid URL"": false, ""Sanitize URL"": false, ""Check Local File Access"": false, ""Get Response"": true, ""Scrape Text"": true, ""Scrape Links"": true, ""Scrape Text with Selenium"": false, ""Ask AI Agent gpt4free"": false, ""Instruct AI Agent gpt4free"": false, ""Ask AI Agent Ooga"": false, ""Instruct AI Agent Ooga"": false}, ""settings"": {""provider"": ""oobabooga"", ""AI_MODEL"": ""gpt-3.5-turbo"", ""AI_TEMPERATURE"": ""0.4"", ""MAX_TOKENS"": ""4000"", ""embedder"": ""default"", ""AI_PROVIDER_URI"": ""http://127.0.0.1:5000"", ""HUGGINGFACE_AUDIO_TO_TEXT_MODEL"": ""facebook/wav2vec2-large-960h-lv60-self"", ""DISCORD_COMMAND_PREFIX"": ""/AGiXT"", ""WORKING_DIRECTORY"": ""./WORKSPACE"", ""WORKING_DIRECTORY_RESTRICTED"": ""True"", ""SEARXNG_INSTANCE_URL"": ""https://searx.work"", ""USE_BRIAN_TTS"": ""True"", ""ELEVENLABS_VOICE"": ""Josh"", ""SELENIUM_WEB_BROWSER"": ""chrome"", """": """"}}

4.Ask it to do anything or chat with it.
5.Behavior: Agent is unable to do anything, program is stuck trying to create memories."
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,exceeding  LLM content limit,/,An oversized context size is being sent oobabooga causing it to crash.,ST,/,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)
1.Set maximum token to 2000 in the Agent-LLM .env.
2.Make an API request to Oobabooga.
3.Obverse in the Oobabooga log that the context size is 2096."
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,Imprecise knowledge retrieval,/,Unable to retrieve data ,"CL,SL",commands/google.py,"1.Setup with docker.  (Local)
2.Run Gen and run Task Chain."
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,low-frequency interactivity,/,Local ChatGPT server connection randomly timing out,SL,(switch the provider to custom fixes this),"1.Locally run an OpenAI compatible API (Preferably RWKV Runner). See https://agixt.com/ for instructions.
2.Setup the agent
3.Create a conversation
4.Click ""Send"" in chat mode about 3-5 times and wait for response"
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,resource contention,/,Playwright Sync API inside the asyncio loop error,ST,commands/web_playwright.py,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)

1.Start an instruction using any of the playwright command.
2. Return this error:
COMMANDS:
{'scrape_text': {'url': 'REDACTED'}}

Error: It looks like you are using Playwright Sync API inside the asyncio loop.
Please use the Async API instead."
hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,inefficient memory management,/,Arm64 cannot allocate memory in static TLS block ,CL,"Add this to .bashrc file will fix this bug:
export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:$LD_PRELOAD","(Environment Type - Connection: Remote;  Environment Type - Container: Using Docker)

1.Start Docker Container Streamlit.
2.Access the webui on Streamlit.
3.Create Agent (Bard,Palm,gpt4free).
4.Go to tasks.
5.Create a task and start it.
6.Watch logs Error will show on the logs on streamlit backend."
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,CustomPrompt.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,Missing LLM input format validation,/,TypeError: Object of type bytes is not JSON serializable(pdf),IC,app.py,"click ""Interact"", choose ""Learning"", choose agent, method from file, choose a pdf and upload"
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,insufficient history management,/,Stuck creating memories ,CL,AgentLLM.py,"1.Commands:
git clone https://github.com/Josh-XT/AGiXT
cd AGiXT/agixt
pip install -r requirements.txt
playwright install
streamlit run Main.py

2.Run textgen with: python server.py --listen-port 7861 --wbits 4 --groupsize 128 --model llama-13b-4bit-128g --model_type llama --xformers --api
See that it is running:
Starting streaming server at ws://127.0.0.1:5005/api/v1/stream Starting API at http://127.0.0.1:5000/api Running on local URL:  http://127.0.0.1:7861

3.Create new agent with config:

{""commands"": {""Read Audio from File"": false, ""Read Audio"": false, ""Evaluate Code"": true, ""Analyze Pull Request"": false, ""Perform Automated Testing"": false, ""Run CI-CD Pipeline"": false, ""Improve Code"": false, ""Write Tests"": false, ""Create a new command"": false, ""Execute Python File"": false, ""Execute Shell"": false, ""Read File"": true, ""Write to File"": true, ""Append to File"": true, ""Delete File"": true, ""Search Files"": true, ""Clone Github Repository"": false, ""Google Search"": true, ""Searx Search"": false, ""Get Datetime"": false, ""Speak with TTS"": false, ""Scrape Text with Playwright"": false, ""Scrape Links with Playwright"": false, ""Is Valid URL"": false, ""Sanitize URL"": false, ""Check Local File Access"": false, ""Get Response"": true, ""Scrape Text"": true, ""Scrape Links"": true, ""Scrape Text with Selenium"": false, ""Ask AI Agent gpt4free"": false, ""Instruct AI Agent gpt4free"": false, ""Ask AI Agent Ooga"": false, ""Instruct AI Agent Ooga"": false}, ""settings"": {""provider"": ""oobabooga"", ""AI_MODEL"": ""gpt-3.5-turbo"", ""AI_TEMPERATURE"": ""0.4"", ""MAX_TOKENS"": ""4000"", ""embedder"": ""default"", ""AI_PROVIDER_URI"": ""http://127.0.0.1:5000"", ""HUGGINGFACE_AUDIO_TO_TEXT_MODEL"": ""facebook/wav2vec2-large-960h-lv60-self"", ""DISCORD_COMMAND_PREFIX"": ""/AGiXT"", ""WORKING_DIRECTORY"": ""./WORKSPACE"", ""WORKING_DIRECTORY_RESTRICTED"": ""True"", ""SEARXNG_INSTANCE_URL"": ""https://searx.work"", ""USE_BRIAN_TTS"": ""True"", ""ELEVENLABS_VOICE"": ""Josh"", ""SELENIUM_WEB_BROWSER"": ""chrome"", """": """"}}

4.Ask it to do anything or chat with it.
5.Behavior: Agent is unable to do anything, program is stuck trying to create memories."
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,exceeding  LLM content limit,/,An oversized context size is being sent oobabooga causing it to crash.,ST,/,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)
1.Set maximum token to 2000 in the Agent-LLM .env.
2.Make an API request to Oobabooga.
3.Obverse in the Oobabooga log that the context size is 2096."
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,Imprecise knowledge retrieval,/,Unable to retrieve data ,"CL,SL",commands/google.py,"1.Setup with docker.  (Local)
2.Run Gen and run Task Chain."
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,low-frequency interactivity,/,Local ChatGPT server connection randomly timing out,SL,(switch the provider to custom fixes this),"1.Locally run an OpenAI compatible API (Preferably RWKV Runner). See https://agixt.com/ for instructions.
2.Setup the agent
3.Create a conversation
4.Click ""Send"" in chat mode about 3-5 times and wait for response"
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,resource contention,/,Playwright Sync API inside the asyncio loop error,ST,commands/web_playwright.py,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)

1.Start an instruction using any of the playwright command.
2. Return this error:
COMMANDS:
{'scrape_text': {'url': 'REDACTED'}}

Error: It looks like you are using Playwright Sync API inside the asyncio loop.
Please use the Async API instead."
CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,inefficient memory management,/,Arm64 cannot allocate memory in static TLS block ,CL,"Add this to .bashrc file will fix this bug:
export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:$LD_PRELOAD","(Environment Type - Connection: Remote;  Environment Type - Container: Using Docker)

1.Start Docker Container Streamlit.
2.Access the webui on Streamlit.
3.Create Agent (Bard,Palm,gpt4free).
4.Go to tasks.
5.Create a task and start it.
6.Watch logs Error will show on the logs on streamlit backend."
ChuloAI/BrainChulo,https://github.com/ChuloAI/BrainChulo/tree/f2d6753177ef9940dcb6994da03fbf5fd323a3b3,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"app/prompt_templates/qa_agent.py
app/agents/base.py
app/prompts/guidance_choice.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
ChuloAI/BrainChulo,https://github.com/ChuloAI/BrainChulo/tree/f2d6753177ef9940dcb6994da03fbf5fd323a3b3,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"app/prompts/flow_guidance_cot.py
app/flow/flow.py
app/agents/base.py",Upload the pptx file.
ChuloAI/BrainChulo,https://github.com/ChuloAI/BrainChulo/tree/f2d6753177ef9940dcb6994da03fbf5fd323a3b3,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"app/main.py
app/conversations/document_based.py/ __init__(self)
app/prompt_templates/document_based_conversation.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,chat.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,chat.py,Upload the pptx file.
daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,chat.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"routes/message.py
tools/chat_openai.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"routes/message.py
tools/chat_openai.py",Upload the pptx file.
AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"chatbot/runner.py
const.py
tools/load_data.py
tools/chat_openai.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,src/single-pdf.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,src/single-pdf.py,Upload the pptx file.
edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,src/single-pdf.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
realrasengan/AIQA,https://github.com/realrasengan/AIQA/tree/fb86f8c48978bd218df62f6bf034463f0034b426,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"bot.js
aiqa.js","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
realrasengan/AIQA,https://github.com/realrasengan/AIQA/tree/fb86f8c48978bd218df62f6bf034463f0034b426,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"bot.js
aiqa.js",Upload the pptx file.
realrasengan/AIQA,https://github.com/realrasengan/AIQA/tree/fb86f8c48978bd218df62f6bf034463f0034b426,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"bot.js
aiqa.js","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f2,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"generative_app/core/chains/prompt.py
generative_app/core/chains/conversational_retrieval_over_code.py
generative_app/core/chains/llm.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f3,Missing LLM input format validation,/,"Can't upload a file(csv,png) to add it into context",IC,No such feature yet,"During chatting, users cannot upload files (such as docx, csv, png, etc.) to add new data into the context."
Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f4,exceeding  LLM content limit,case1,Better handle large context (chat history) ,ST,generative_app/core/chains/conversational_retrieval_over_code.py/class BaseConversationalRetrievalCodeChain,"1.Start a conversation with AppifyAi, to create an app.
2.As the generated app is growing the code is more and more complex and has a lot of lines and the chat history grows as well. In that case the context grows, the model the bot uses won't be able to process it all (and the price of the prompt will increase)."
Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f5,,case2,Can't handle long context docs,ST,"No such feature yet.
generative_app/core/chains/doc_retriever.py","Currently, AppifyAi doesn‘’t use LongContextReorder when retrieving Streamlit documents."
Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f6,lacking restrictions in prompt,/,Sometimes the LLM does not respond incorrect language,IC,generative_app/core/chains/prompt.py,"1.Start a conversation with AppifyAi, asking questions in a non-English language.
2.AppifyAi's responses do not use the user's language."
szpnygo/VecTextSearch,https://github.com/szpnygo/VecTextSearch/tree/ec6a1a2861b1399b3463026980c02507702d3508,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"history/chat3.md
demo/flutter/vec_text_search_demo/lib/src/widgets/search_result_list.dart","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
szpnygo/VecTextSearch,https://github.com/szpnygo/VecTextSearch/tree/ec6a1a2861b1399b3463026980c02507702d3508,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"demo/flutter/vec_text_search_demo/lib/main.dart  
history/chat1.md
demo/flutter/vec_text_search_demo/lib/src/widgets/search_bar.dart",Upload the pptx file.
szpnygo/VecTextSearch,https://github.com/szpnygo/VecTextSearch/tree/ec6a1a2861b1399b3463026980c02507702d3508,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"history/extension/content_script.js
history/extension/background_script.js","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"chatiq/prompt.py
chatiq/chat_chain.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"chatiq/prompt.py
chatiq/chat_chain.py",Upload the pptx file.
yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"chatiq/text_processor.py
chatiq/chatiq.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"projects/app/src/pages/api/core/ai/token.ts
files/models/Baichuan2/openai_api.py
python/ocr/surya/app.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,Missing LLM input format validation,case1,The analysis of PPT documents needs optimization,IC,packages/service/common/file/read/utils.ts/read,"Here's your request translated into English:

1. Use FsatGPT online: https://fastgpt.in/
2. Create a new knowledge base.
3. In the ""Dataset"" interface, select the ""Create/Import"" tab and choose ""Text Dataset.""
4. Upload the pptx file.
5. After parsing the pptx file, the order is chaotic."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case2,The interface for batch adding data to the collection reports error 500.,IC,"see docSite/content/docs/development/openai/dataset.md -->
request POST 'https://api.fastgpt.in/api/core/dataset/data/pushData'

billId cannot be an empty string, delete it if you don't need it(fixes this bug)","1. Call FastGPT OpenAPI: https://api.fastgpt.in/api/core/dataset/puahuData.
Call Body:
{
    ""collectionId"": ""66434de186542ddfe1187c56"",
    ""trainingMode"": ""chunk"",
    ""prompt"": """",
    ""billId"": """",
    ""data"":  [
            {
                ""q"": ""There's a pretty good restaurant near Xidian University in Xi'an.""
         }

    ]
}

2. Return Body:
{
    ""code"" :500,
    ""statusText"": """",
    ""message"": ""Cannot read properties of undifined (reading 'forEach')"",

     .......

}"
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case3,Sending pictures in the dialog box displays an error message: connect ECONNREFUSED 127.0.0.1:3000,IC,packages/service/core/chat/utils.ts/loadChatImgToBase64,"1.In the FastGPT UI, select ""Application"" and choose the GPT-4o model to start chatting.
2.Send an image to the chatbox.
3.Encounter the error “ECONNREFUSED ::1:3000”."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case4,"Upload CSV and Excel files to knowledge base, but the storage fails.",IC,"packages/service/worker/file/extension/csv.ts, xlsx.ts","1.Upload CSV or XLSX files to your knowledge base, selecting ""Direct Segmentation"" and ""Automatic"" in ""Data Processing.""
2.If you click ""Preview Source Text"" before uploading, the dataset appears empty after uploading and is not stored in the database.
3.If you do not click ""Preview Source Text"" before uploading, the upload is successful and the data is stored in the database."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case5,"When calling the API of a multimodal application, the order between text and images cannot be reflected in the message body.",IC,packages/service/core/workflow/dispatch/chat/openai.ts,"1. In ""Application Configuration,"" call a multimodal application's API (such as claude-3). When calling the API, the message body transmits images and text information in an ordered array format via content, thus maintaining the connection between text and images.
2. However, when passed into fastgpt, the message body automatically places all images at the front and then merges multiple text segments in content into one block of text. This causes the connection between images and text to be lost, affecting the response quality of the model."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case6,Uploads of csv files exceeding a certain compliant size result in errors.,"ST,IC",packages/service/worker/file/extension/csv.ts,"1.Upload a CSV file larger than 20MB to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation.""
3.Click upload and receive an error: ""The value of 'offset' is out of range."""
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case7,The tool invocation frequently inexplicably exceeds the token limit.,"ST,IC",packages/service/core/workflow/dispatch/agent/runTool/toolChoice.ts,"1.Use ""Tool call"" to access multiple knowledge bases.
2.Each knowledge base reference limit is below 3000 tokens.
3.Start a chat, and the questions call 2 knowledge bases, but it shows that the tokens exceed the model's limit of 16k."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case8,Importing.doc files to the knowledge base is not supported,IC,packages/service/worker/file,"1.Add new files to your knowledge base.
2.Found that importing .doc files is not supported."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case9,PDF file text extraction error,IC,packages/service/worker/file/extension/pdf.ts,"1.Upload this two-column PDF file to the knowledge base: Class management strategies for cultivating good behavior habits of primary and secondary school students_Chen Fang.pdf (download in https://github.com/labring/FastGPT/issues/621)
2.Select ""Direct Segmentation""
3.Preview the segmentation results and observe that the content extraction from the PDF is disorganized."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case10,Error reported when uploading a text dataset to the knowledge base.,ST,packages/service/common/file/gridfs/controller.ts/uploadFile,"1.In the UI, select ""Knowledge Base.""
2.Upload a CSV file larger than 100 MB to your knowledge base, and then selecting ""Direct Segmentation.""
3.Click upload, and an error will occur."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,knowledge misalignment,case1,"During file segmentation, if the number of characters under a level 3 heading (###) is less than 29, the level 3 heading will be lost, and its content will be directly included in the next segment.",IC,packages/service/worker/file/extension/docx.ts,"1.Upload a .docx file (containing level 3 headings) to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation"" and ""Automatic.""
3.In the segmentation results, if a level 3 heading (###) has fewer than 29 characters, the heading is lost, and its content is directly merged into the next segment. If it has 29 or more characters, the issue doesn't occur."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case2,Need to optimize the segmentation of table files when importing files into knowledge bases,"IC,SL","packages/service/worker/file/extension/xlsx.ts,csv.ts,pdf.ts","1.Upload spreadsheet files to your knowledge base.
2.Select ""Direct Segmentation.""
3.Review the segmentation results and observe that the segmentation is done directly by word count. For example, when chunking an EXCEL spreadsheet file, the text is truncated directly by word count, resulting in information from the same row being located in different chunks."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,Imprecise knowledge retrieval,case1,"In ""knowledge base search test"", for Q&A based on Excel spreadsheets, if the Excel file is large, the answers may be inaccurate.",IC,"packages/service/core/dataset/search/controller.ts
python/bdg-rerank/bdg-reranker-/app.py","1.Upload an xlsx file with around 10,000 rows to your knowledge base.
2.In ""Search Test,"" ask questions related to the content of this file.
3.In ""Knowledge Base Search Configuration,"" select different search modes to test the Q&A, but the test results are unsatisfactory, with inaccurate answers."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case2,Knowledge base search cannot accurately identify the number,IC,src/global/core/call/dataset,"1.Import the knowledge base with serial number/model data
2.We have tried Q&A training and document uploading to the knowledge base to ensure that the numbered keywords have been covered by the knowledge base multiple times.
3.Neither semantic retrieval nor full-text retrieval can accurately point to the relevant knowledge."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,,case3,"The bot's replies always contain garbled characters and consistently display ""retrieving file"".",IC,It's an issue with the user's gpt-4o model channel,"1.In the ""Workbench"", create a new ""Workflow"".
2.In the ""Question Classification"" module, select the AI model as gpt-4o.
3.After successfully creating the application, start the conversation.
4.The bot's responses in the conversation always contain garbled characters."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,Unclear context in prompt,/,"In ""workflow"", the knowledge base reference accessed by the HTTP module does not display the referenced content in the conversation.",IC,"projects/app/src/components/ChatBox
/ResponseTags.tsx/quoteList","1.(A conversational application) IN ""workflow"" , Integrate knowledge base citations into the HTTP module.
2.During debugging, when the bot answers questions related to the knowledge base, it does not cite references."
labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,Out-of-sync LLM downstream tasks,/,The user hope the HTTP module supports streaming responses from ChatGPT.,UI,No such feature yet.,"1.The user needs to provide several pieces of information in a chat to achieve a certain goal.
2.The user can send partial information each time, and the bot will sequentially receive the information provided by the user, gradually completing the necessary information to achieve the goal throughout the chat."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"projects/app/src/pages/api/core/ai/token.ts
files/models/ChatGLM2/openai_api.py
python/bge-rerank/bge-reranker-base/app.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,Missing LLM input format validation,case1,The analysis of PPT documents needs optimization,IC,packages/service/common/file/read,"Here's your request translated into English:

1. Use FsatGPT online: https://fastgpt.in/
2. Create a new knowledge base.
3. In the ""Dataset"" interface, select the ""Create/Import"" tab and choose ""Text Dataset.""
4. Upload the pptx file.
5. After parsing the pptx file, the order is chaotic."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case2,The interface for batch adding data to the collection reports error 500.,IC,"see docSite/content/docs/development/openai/dataset.md -->
request POST 'https://api.fastgpt.in/api/core/dataset/data/pushData'

billId cannot be an empty string, delete it if you don't need it(fixes this bug)","1. Call FastGPT OpenAPI: https://api.fastgpt.in/api/core/dataset/puahuData.
Call Body:
{
    ""collectionId"": ""66434de186542ddfe1187c56"",
    ""trainingMode"": ""chunk"",
    ""prompt"": """",
    ""billId"": """",
    ""data"":  [
            {
                ""q"": ""There's a pretty good restaurant near Xidian University in Xi'an.""
         }

    ]
}

2. Return Body:
{
    ""code"" :500,
    ""statusText"": """",
    ""message"": ""Cannot read properties of undifined (reading 'forEach')"",

     .......

}"
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case3,Sending pictures in the dialog box displays an error message: connect ECONNREFUSED 127.0.0.1:3000,IC,packages/service/core/chat/utils.ts/loadChatImgToBase64,"1.In the FastGPT UI, select ""Application"" and choose the GPT-4o model to start chatting.
2.Send an image to the chatbox.
3.Encounter the error “ECONNREFUSED ::1:3000”."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case4,"Upload CSV and Excel files to knowledge base, but the storage fails.",IC,"packages/service/worker/file/extension/csv.ts, xlsx.ts","1.Upload CSV or XLSX files to your knowledge base, selecting ""Direct Segmentation"" and ""Automatic"" in ""Data Processing.""
2.If you click ""Preview Source Text"" before uploading, the dataset appears empty after uploading and is not stored in the database.
3.If you do not click ""Preview Source Text"" before uploading, the upload is successful and the data is stored in the database."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case5,"When calling the API of a multimodal application, the order between text and images cannot be reflected in the message body.",IC,packages/service/core/workflow/dispatch/chat/openai.ts,"1. In ""Application Configuration,"" call a multimodal application's API (such as claude-3). When calling the API, the message body transmits images and text information in an ordered array format via content, thus maintaining the connection between text and images.
2. However, when passed into fastgpt, the message body automatically places all images at the front and then merges multiple text segments in content into one block of text. This causes the connection between images and text to be lost, affecting the response quality of the model."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case6,Uploads of csv files exceeding a certain compliant size result in errors.,"ST,IC",packages/service/worker/file/extension/csv.ts,"1.Upload a CSV file larger than 20MB to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation.""
3.Click upload and receive an error: ""The value of 'offset' is out of range."""
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case7,The tool invocation frequently inexplicably exceeds the token limit.,"ST,IC",packages/service/core/workflow/dispatch/agent/runTool/toolChoice.ts,"1.Use ""Tool call"" to access multiple knowledge bases.
2.Each knowledge base reference limit is below 3000 tokens.
3.Start a chat, and the questions call 2 knowledge bases, but it shows that the tokens exceed the model's limit of 16k."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case8,Importing.doc files to the knowledge base is not supported,IC,packages/service/worker/file,"1.Add new files to your knowledge base.
2.Found that importing .doc files is not supported."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case9,PDF file text extraction error,IC,packages/service/worker/file/extension/pdf.ts,"1.Upload this two-column PDF file to the knowledge base: Class management strategies for cultivating good behavior habits of primary and secondary school students_Chen Fang.pdf (download in https://github.com/labring/FastGPT/issues/621)
2.Select ""Direct Segmentation""
3.Preview the segmentation results and observe that the content extraction from the PDF is disorganized."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case10,Error reported when uploading a text dataset to the knowledge base.,ST,packages/service/common/file/gridfs/controller.ts/uploadFile,"1.In the UI, select ""Knowledge Base.""
2.Upload a CSV file larger than 100 MB to your knowledge base, and then selecting ""Direct Segmentation.""
3.Click upload, and an error will occur."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,knowledge misalignment,case1,"During file segmentation, if the number of characters under a level 3 heading (###) is less than 29, the level 3 heading will be lost, and its content will be directly included in the next segment.",IC,packages/service/worker/file/extension/docx.ts,"1.Upload a .docx file (containing level 3 headings) to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation"" and ""Automatic.""
3.In the segmentation results, if a level 3 heading (###) has fewer than 29 characters, the heading is lost, and its content is directly merged into the next segment. If it has 29 or more characters, the issue doesn't occur."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case2,Need to optimize the segmentation of table files when importing files into knowledge bases,"IC,SL","packages/service/worker/file/extension/xlsx.ts,csv.ts,pdf.ts","1.Upload spreadsheet files to your knowledge base.
2.Select ""Direct Segmentation.""
3.Review the segmentation results and observe that the segmentation is done directly by word count. For example, when chunking an EXCEL spreadsheet file, the text is truncated directly by word count, resulting in information from the same row being located in different chunks."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,Imprecise knowledge retrieval,case1,"In ""knowledge base search test"", for Q&A based on Excel spreadsheets, if the Excel file is large, the answers may be inaccurate.",IC,"packages/service/core/dataset/search/controller.ts
python/bdg-rerank/bdg-reranker-/app.py","1.Upload an xlsx file with around 10,000 rows to your knowledge base.
2.In ""Search Test,"" ask questions related to the content of this file.
3.In ""Knowledge Base Search Configuration,"" select different search modes to test the Q&A, but the test results are unsatisfactory, with inaccurate answers."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case2,Knowledge base search cannot accurately identify the number,IC,src/global/core/call/dataset,"1.Import the knowledge base with serial number/model data
2.We have tried Q&A training and document uploading to the knowledge base to ensure that the numbered keywords have been covered by the knowledge base multiple times.
3.Neither semantic retrieval nor full-text retrieval can accurately point to the relevant knowledge."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,,case3,"The bot's replies always contain garbled characters and consistently display ""retrieving file"".",IC,It's an issue with the user's gpt-4o model channel,"1.In the ""Workbench"", create a new ""Workflow"".
2.In the ""Question Classification"" module, select the AI model as gpt-4o.
3.After successfully creating the application, start the conversation.
4.The bot's responses in the conversation always contain garbled characters."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,Unclear context in prompt,/,"In ""workflow"", the knowledge base reference accessed by the HTTP module does not display the referenced content in the conversation.",IC,"projects/app/src/components/ChatBox
/ResponseTags.tsx/quoteList","1.(A conversational application) IN ""workflow"" , Integrate knowledge base citations into the HTTP module.
2.During debugging, when the bot answers questions related to the knowledge base, it does not cite references."
c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,Out-of-sync LLM downstream tasks,/,The user hope the HTTP module supports streaming responses from ChatGPT.,UI,No such feature yet.,"1.The user needs to provide several pieces of information in a chat to achieve a certain goal.
2.The user can send partial information each time, and the bot will sequentially receive the information provided by the user, gradually completing the necessary information to achieve the goal throughout the chat."
SaudM/GPT-LMU-APP,"client/src/utils/+491:505file.ts
client/src/utils/plugin/google.ts
client/src/utils/plugin/openai.ts",exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"client/src/utils/file.ts
client/src/utils/plugin/google.ts
client/src/utils/plugin/openai.ts","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,Missing LLM input format validation,case1,The analysis of PPT documents needs optimization,IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","Here's your request translated into English:

1. Use FsatGPT online: https://fastgpt.in/
2. Create a new knowledge base.
3. In the ""Dataset"" interface, select the ""Create/Import"" tab and choose ""Text Dataset.""
4. Upload the pptx file.
5. After parsing the pptx file, the order is chaotic."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case2,The interface for batch adding data to the collection reports error 500.,IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1. Call FastGPT OpenAPI: https://api.fastgpt.in/api/core/dataset/puahuData.
Call Body:
{
    ""collectionId"": ""66434de186542ddfe1187c56"",
    ""trainingMode"": ""chunk"",
    ""prompt"": """",
    ""billId"": """",
    ""data"":  [
            {
                ""q"": ""There's a pretty good restaurant near Xidian University in Xi'an.""
         }

    ]
}

2. Return Body:
{
    ""code"" :500,
    ""statusText"": """",
    ""message"": ""Cannot read properties of undifined (reading 'forEach')"",

     .......

}"
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case3,Sending pictures in the dialog box displays an error message: connect ECONNREFUSED 127.0.0.1:3000,IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.In the FastGPT UI, select ""Application"" and choose the GPT-4o model to start chatting.
2.Send an image to the chatbox.
3.Encounter the error “ECONNREFUSED ::1:3000”."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case4,"Upload CSV and Excel files to knowledge base, but the storage fails.",IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.Upload CSV or XLSX files to your knowledge base, selecting ""Direct Segmentation"" and ""Automatic"" in ""Data Processing.""
2.If you click ""Preview Source Text"" before uploading, the dataset appears empty after uploading and is not stored in the database.
3.If you do not click ""Preview Source Text"" before uploading, the upload is successful and the data is stored in the database."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case5,"When calling the API of a multimodal application, the order between text and images cannot be reflected in the message body.",IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1. In ""Application Configuration,"" call a multimodal application's API (such as claude-3). When calling the API, the message body transmits images and text information in an ordered array format via content, thus maintaining the connection between text and images.
2. However, when passed into fastgpt, the message body automatically places all images at the front and then merges multiple text segments in content into one block of text. This causes the connection between images and text to be lost, affecting the response quality of the model."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case6,Uploads of csv files exceeding a certain compliant size result in errors.,"ST,IC","client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.Upload a CSV file larger than 20MB to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation.""
3.Click upload and receive an error: ""The value of 'offset' is out of range."""
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case7,The tool invocation frequently inexplicably exceeds the token limit.,"ST,IC","client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.Use ""Tool call"" to access multiple knowledge bases.
2.Each knowledge base reference limit is below 3000 tokens.
3.Start a chat, and the questions call 2 knowledge bases, but it shows that the tokens exceed the model's limit of 16k."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case8,Importing.doc files to the knowledge base is not supported,IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.Add new files to your knowledge base.
2.Found that importing .doc files is not supported."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case9,PDF file text extraction error,IC,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.Upload this two-column PDF file to the knowledge base: Class management strategies for cultivating good behavior habits of primary and secondary school students_Chen Fang.pdf (download in https://github.com/labring/FastGPT/issues/621)
2.Select ""Direct Segmentation""
3.Preview the segmentation results and observe that the content extraction from the PDF is disorganized."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case10,Error reported when uploading a text dataset to the knowledge base.,ST,"client/src/api/fetch.ts
client/src/pages/api/user/inform/read.ts 
client/src/api/user.ts","1.In the UI, select ""Knowledge Base.""
2.Upload a CSV file larger than 100 MB to your knowledge base, and then selecting ""Direct Segmentation.""
3.Click upload, and an error will occur."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,knowledge misalignment,case1,"During file segmentation, if the number of characters under a level 3 heading (###) is less than 29, the level 3 heading will be lost, and its content will be directly included in the next segment.",IC,"client/src/utils/adapt.ts
client/src/service/utils/chat/index.ts
client/src/pages/kb/components/SelectFileModal.tsx","1.Upload a .docx file (containing level 3 headings) to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation"" and ""Automatic.""
3.In the segmentation results, if a level 3 heading (###) has fewer than 29 characters, the heading is lost, and its content is directly merged into the next segment. If it has 29 or more characters, the issue doesn't occur."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case2,Need to optimize the segmentation of table files when importing files into knowledge bases,"IC,SL","client/src/utils/adapt.ts
client/src/service/utils/chat/index.ts
client/src/pages/kb/components/SelectFileModal.tsx","1.Upload spreadsheet files to your knowledge base.
2.Select ""Direct Segmentation.""
3.Review the segmentation results and observe that the segmentation is done directly by word count. For example, when chunking an EXCEL spreadsheet file, the text is truncated directly by word count, resulting in information from the same row being located in different chunks."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,Imprecise knowledge retrieval,case1,"In ""knowledge base search test"", for Q&A based on Excel spreadsheets, if the Excel file is large, the answers may be inaccurate.",IC,"client/src/pages/model/index.tsx
client/src/api/model.ts
client/src/pages/model/components/ModelList.tsx","1.Upload an xlsx file with around 10,000 rows to your knowledge base.
2.In ""Search Test,"" ask questions related to the content of this file.
3.In ""Knowledge Base Search Configuration,"" select different search modes to test the Q&A, but the test results are unsatisfactory, with inaccurate answers."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case2,Knowledge base search cannot accurately identify the number,IC,"client/src/pages/model/index.tsx
client/src/api/model.ts
client/src/pages/model/components/ModelList.tsx","1.Import the knowledge base with serial number/model data
2.We have tried Q&A training and document uploading to the knowledge base to ensure that the numbered keywords have been covered by the knowledge base multiple times.
3.Neither semantic retrieval nor full-text retrieval can accurately point to the relevant knowledge."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,,case3,"The bot's replies always contain garbled characters and consistently display ""retrieving file"".",IC,"client/src/pages/model/index.tsx
client/src/api/model.ts
client/src/pages/model/components/ModelList.tsx","1.In the ""Workbench"", create a new ""Workflow"".
2.In the ""Question Classification"" module, select the AI model as gpt-4o.
3.After successfully creating the application, start the conversation.
4.The bot's responses in the conversation always contain garbled characters."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,Unclear context in prompt,/,"In ""workflow"", the knowledge base reference accessed by the HTTP module does not display the referenced content in the conversation.",IC,client/src/pages/api/openapi/chat/chat.ts,"1.(A conversational application) IN ""workflow"" , Integrate knowledge base citations into the HTTP module.
2.During debugging, when the bot answers questions related to the knowledge base, it does not cite references."
SaudM/GPT-LMU-APP,https://github.com/SaudM/GPT-LMU-APP/tree/882ad9818e49ca004959e17a0f7d2580ebfedb10,Out-of-sync LLM downstream tasks,/,The user hope the HTTP module supports streaming responses from ChatGPT.,UI,No such feature yet.,"1.The user needs to provide several pieces of information in a chat to achieve a certain goal.
2.The user can send partial information each time, and the bot will sequentially receive the information provided by the user, gradually completing the necessary information to achieve the goal throughout the chat."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"chat.py
Notebook/GPT-3_customvectorsearch.ipynb","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,Missing LLM input format validation,case1,The analysis of PPT documents needs optimization,IC,"chat.py
app.py","Here's your request translated into English:

1. Use FsatGPT online: https://fastgpt.in/
2. Create a new knowledge base.
3. In the ""Dataset"" interface, select the ""Create/Import"" tab and choose ""Text Dataset.""
4. Upload the pptx file.
5. After parsing the pptx file, the order is chaotic."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case2,The interface for batch adding data to the collection reports error 500.,IC,"chat.py
app.py","1. Call FastGPT OpenAPI: https://api.fastgpt.in/api/core/dataset/puahuData.
Call Body:
{
    ""collectionId"": ""66434de186542ddfe1187c56"",
    ""trainingMode"": ""chunk"",
    ""prompt"": """",
    ""billId"": """",
    ""data"":  [
            {
                ""q"": ""There's a pretty good restaurant near Xidian University in Xi'an.""
         }

    ]
}

2. Return Body:
{
    ""code"" :500,
    ""statusText"": """",
    ""message"": ""Cannot read properties of undifined (reading 'forEach')"",

     .......

}"
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case3,Sending pictures in the dialog box displays an error message: connect ECONNREFUSED 127.0.0.1:3000,IC,"chat.py
app.py","1.In the FastGPT UI, select ""Application"" and choose the GPT-4o model to start chatting.
2.Send an image to the chatbox.
3.Encounter the error “ECONNREFUSED ::1:3000”."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case4,"Upload CSV and Excel files to knowledge base, but the storage fails.",IC,"chat.py
app.py","1.Upload CSV or XLSX files to your knowledge base, selecting ""Direct Segmentation"" and ""Automatic"" in ""Data Processing.""
2.If you click ""Preview Source Text"" before uploading, the dataset appears empty after uploading and is not stored in the database.
3.If you do not click ""Preview Source Text"" before uploading, the upload is successful and the data is stored in the database."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case5,"When calling the API of a multimodal application, the order between text and images cannot be reflected in the message body.",IC,"chat.py
app.py","1. In ""Application Configuration,"" call a multimodal application's API (such as claude-3). When calling the API, the message body transmits images and text information in an ordered array format via content, thus maintaining the connection between text and images.
2. However, when passed into fastgpt, the message body automatically places all images at the front and then merges multiple text segments in content into one block of text. This causes the connection between images and text to be lost, affecting the response quality of the model."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case6,Uploads of csv files exceeding a certain compliant size result in errors.,"ST,IC","chat.py
app.py","1.Upload a CSV file larger than 20MB to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation.""
3.Click upload and receive an error: ""The value of 'offset' is out of range."""
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case7,The tool invocation frequently inexplicably exceeds the token limit.,"ST,IC","chat.py
app.py","1.Use ""Tool call"" to access multiple knowledge bases.
2.Each knowledge base reference limit is below 3000 tokens.
3.Start a chat, and the questions call 2 knowledge bases, but it shows that the tokens exceed the model's limit of 16k."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case8,Importing.doc files to the knowledge base is not supported,IC,"chat.py
app.py","1.Add new files to your knowledge base.
2.Found that importing .doc files is not supported."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case9,PDF file text extraction error,IC,"chat.py
app.py","1.Upload this two-column PDF file to the knowledge base: Class management strategies for cultivating good behavior habits of primary and secondary school students_Chen Fang.pdf (download in https://github.com/labring/FastGPT/issues/621)
2.Select ""Direct Segmentation""
3.Preview the segmentation results and observe that the content extraction from the PDF is disorganized."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case10,Error reported when uploading a text dataset to the knowledge base.,ST,"chat.py
app.py","1.In the UI, select ""Knowledge Base.""
2.Upload a CSV file larger than 100 MB to your knowledge base, and then selecting ""Direct Segmentation.""
3.Click upload, and an error will occur."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,knowledge misalignment,case1,"During file segmentation, if the number of characters under a level 3 heading (###) is less than 29, the level 3 heading will be lost, and its content will be directly included in the next segment.",IC,chat.py,"1.Upload a .docx file (containing level 3 headings) to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation"" and ""Automatic.""
3.In the segmentation results, if a level 3 heading (###) has fewer than 29 characters, the heading is lost, and its content is directly merged into the next segment. If it has 29 or more characters, the issue doesn't occur."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case2,Need to optimize the segmentation of table files when importing files into knowledge bases,"IC,SL",chat.py,"1.Upload spreadsheet files to your knowledge base.
2.Select ""Direct Segmentation.""
3.Review the segmentation results and observe that the segmentation is done directly by word count. For example, when chunking an EXCEL spreadsheet file, the text is truncated directly by word count, resulting in information from the same row being located in different chunks."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,Imprecise knowledge retrieval,case1,"In ""knowledge base search test"", for Q&A based on Excel spreadsheets, if the Excel file is large, the answers may be inaccurate.",IC,"chat.py
app.py","1.Upload an xlsx file with around 10,000 rows to your knowledge base.
2.In ""Search Test,"" ask questions related to the content of this file.
3.In ""Knowledge Base Search Configuration,"" select different search modes to test the Q&A, but the test results are unsatisfactory, with inaccurate answers."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case2,Knowledge base search cannot accurately identify the number,IC,"chat.py
app.py","1.Import the knowledge base with serial number/model data
2.We have tried Q&A training and document uploading to the knowledge base to ensure that the numbered keywords have been covered by the knowledge base multiple times.
3.Neither semantic retrieval nor full-text retrieval can accurately point to the relevant knowledge."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,,case3,"The bot's replies always contain garbled characters and consistently display ""retrieving file"".",IC,"chat.py
app.py","1.In the ""Workbench"", create a new ""Workflow"".
2.In the ""Question Classification"" module, select the AI model as gpt-4o.
3.After successfully creating the application, start the conversation.
4.The bot's responses in the conversation always contain garbled characters."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,Unclear context in prompt,/,"In ""workflow"", the knowledge base reference accessed by the HTTP module does not display the referenced content in the conversation.",IC,"config.py
chat.py","1.(A conversational application) IN ""workflow"" , Integrate knowledge base citations into the HTTP module.
2.During debugging, when the bot answers questions related to the knowledge base, it does not cite references."
Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,Out-of-sync LLM downstream tasks,/,The user hope the HTTP module supports streaming responses from ChatGPT.,UI,No such feature yet.,"1.The user needs to provide several pieces of information in a chat to achieve a certain goal.
2.The user can send partial information each time, and the bot will sequentially receive the information provided by the user, gradually completing the necessary information to achieve the goal throughout the chat."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"src/sagemaker.py
openai_server/server.py",Upload the pptx file.
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"src/gpt4all_llm.py
openai_server/server.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,inefficient memory management,case1,Processing of large PDFS (100k) is slow,SL,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to handle large PDFs.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Upload a large PDF document (around 100k pages or similar in size) for processing.
5. Monitor the processing time.
6. Observe if the processing is slow and note the time taken to process the large PDF.(for example, ValueError: invalid literal for int() with base 10: 'some_value')"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,,case2,"When the document query model handle the question to ask something about a table's content,bugs are easily to be triggered.",IC,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a document that contains tables.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query specifically asking about the content of a table in the uploaded document.
6. Observe if any bugs are triggered when the model handles the question about the table's content, such as errors or incorrect responses."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,Imprecise knowledge retrieval,/,Mismatch Between Query Response and Source Document Ordering in a Captive Network with Multiple ,"ST,TK","openai_server/server.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to connect to a captive network with multiple source documents.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Upload multiple source documents to the captive network.
5. Open the query interface in the web UI.
6. Enter a query related to the content of the source documents.
7. Observe if there is a mismatch between the query response and the ordering of the information in the source documents.(for example, ""TypeError: 'NoneType' object is not iterable"")"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,unnecessary LLM output,/,Sources have redundant information.,"IC,UI","src/output_parser.py
src/sagemaker.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload multiple source documents that contain redundant information.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded source documents.
6. Observe if the responses include redundant information from the multiple source documents.(for example, IndexError: list index out of range)"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,improper text embedding,/,Segmentation fault when inferencing with embeddings db of 4GB,ST,"src/tts.py
openai_server/server.py
openai_server/backend.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Prepare an embeddings database of approximately 4GB in size.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Load the 4GB embeddings database into the system.
5. Open the query interface in the web UI.
6. Enter a query that utilizes the embeddings database for inferencing.
7. Observe if a segmentation fault occurs during the inferencing process."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,privacy violation,/,an illegal memory access was encountered ,IS,src/create_data.py,"1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Prepare the system with a dataset or configuration that may trigger memory access issues.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that processes a large dataset or requires significant computational resources.
6. Observe if an error message indicating ""an illegal memory access was encountered"" occurs during the processing."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,insufficient history management,/,"without filtering data stuffed into context, it pushes LLM into bad outputs ",IC,"src/prompter.py
openai_server/server.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload or configure a dataset that includes noisy or irrelevant data.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that retrieves information from the dataset.
6. Observe if the LLM produces bad outputs due to the noisy or irrelevant data being included in the context without filtering."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,Incompatible LLM output format,case1,"PDF makes wraps lines, then LLM does same in response",IC,"src/output_parser.py
src/sagemaker.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that has wrapped lines due to formatting.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded PDF document.
6. Observe the response to see if the LLM replicates the line wrapping from the PDF in its output."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,,case2,Without Line breaks (\n) issue of extract response ,IC,"src/output_parser.py
src/sagemaker.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset that includes line breaks (`\n`).
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that extracts information from the uploaded document or dataset.
6. Observe the response to check if the extracted text includes line breaks (`\n`) as expected, or if there is an issue with handling line breaks in the response."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,resource contention,/,simultaneous tokenization for multi-process training causes errors,ST,"src/sagemaker.py
openai_server/server.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system for multi-process training.
3. Prepare a dataset for tokenization.
4. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
5. Initiate the multi-process training with the prepared dataset.
6. Observe if errors occur during the simultaneous tokenization process. Monitor the server logs or terminal output for error messages related to tokenization."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,lacking restrictions in prompt,case1,Limited Max output tokens,IC,"src/gpt4all_llm.py
openai_server/server.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the maximum output tokens limit in the configuration file.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed and lengthy response.
6. Observe if the response is cut off or limited in length due to the maximum output tokens limit configured.(for example, ModuleNotFoundError: No module named 'some_module')"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,,case2,Not getting the complete output (long context),IC,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset with a long context.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed response based on the long context.
6. Observe if the response is incomplete or cut off, indicating that the model is not providing the complete output for long context queries.(for example, MemoryError: Unable to allocate X.Y GiB for an array with shape (Z,) and data type float64)"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,Unclear context in prompt,case1,No DB is used despite it's existing,IC,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to connect to an existing database.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that should retrieve data from the database.
6. Observe if the response utilizes the database for retrieving information or if the database is not used despite its existence. Monitor server logs or terminal output for any indications that the database is not being accessed."
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,,case2,Incorrect context can lead to hallucinations,IC,"openai_server/server.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a large text document (e.g., a .txt or .pdf file) to the system. Ensure the document is large enough to test the application's capacity (e.g., 50+ pages).
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the uploaded document but introduce incorrect context in the question.
6. Observe the response to see if the model produces hallucinations or inaccurate information based on the incorrect context provided.(for example, RuntimeError: CUDA out of memory. Tried to allocate X GiB (GPU 0; Y GiB total capacity; Z GiB already allocated; A GiB free; B MiB cached))"
h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,,case3,"Hallucinations over generation. models tend to increasingly ramble, the developers need to control them. ",IC,"src/output_parser.py
src/sagemaker.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset with specific information.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to see if the model produces hallucinations or rambles beyond the scope of the query.
7. Note instances where the model's responses are increasingly off-topic or irrelevant, indicating the need for better control over generation."
Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"src/prompter.py
src/h2oai_pipeline.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"models/modelling_RW_falcon7b.py
src/gen.py",Upload the pptx file.
Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
ushakrishnan/SearchWithOpenAI,https://github.com/ushakrishnan/SearchWithOpenAI/tree/470a5d10f786efbac2feab18330c69365a937599,exceeding  LLM content limit,exceeding  LLM content limit,1. Glitch when indexing 32mb PDF file.,ST,"common/funs.py/addtostorepdf()
pages/3_Index_PDFnTXT_Into_ChromaDB_Store.py/start_capture()","1.Set up SearchWithOpenAI according to the README.md of this project: https://github.com/ushakrishnan/SearchWithOpenAI/README.md
2. Run: ""streamlit run Home.py""
(
(1)Prompt:
how wood cribs are fastened?
st.warningmethodstreamlit.delta_generator.AlertMixin.warning(body: 'SupportsStr', *, icon: Optional[str] = None) -> 'DeltaGenerator'
Display warning message.

(2)Parameters:
body : str
The warning text to display.
icon : str or None
An optional, keyword-only argument that specifies an emoji to use as
the icon for the alert. Shortcodes are not allowed, please use a
single character instead. E.g. ""🚨"", ""🔥"", ""🤖"", etc.
Defaults to None, which means no icon is displayed.

(3)Documents Persisted in Vector Database: please see: https://github.com/ushakrishnan/SearchWithOpenAI/issues/3)

3. Runs into this error:  IndexError: list index out of range"
ushakrishnan/SearchWithOpenAI,https://github.com/ushakrishnan/SearchWithOpenAI/tree/470a5d10f786efbac2feab18330c69365a937599,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,pages/200_Q&A_with_Azure_Open_AI.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
ushakrishnan/SearchWithOpenAI,https://github.com/ushakrishnan/SearchWithOpenAI/tree/470a5d10f786efbac2feab18330c69365a937599,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,pages/130_Talk_with_Open_AI.py,Upload the pptx file.
EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,absence of final output,absence of final output,The program cannot stop.,UI,"local_agi.py
local_agi_zh.py","1. Set up LocalAGI according to the README.md of this project: https://github.com/EmbraceAGI/LocalAGI/blob/main/README.md
2. Set `.env` with:
   LLM_MODEL=chatglm-6b
   OBJECTIVE=Implement a Python code to accept input and print it in the terminal
   INITIAL_TASK=Build a task list
3. And then LocalAGI starts creating and completing tasks in an infinite loop, never stopping."
EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,Imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL","local_agi.py
local_agi_mini.py","(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py"
EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,unnecessary LLM output,/,The task creation process generating too many unnecessary tasks,"IC,UI","local_agi.py
local_agi_zh.py","1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes"
EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,Incompatible LLM output format,/,Task list numbering continues to reset or get misordered.,IC,"local_agi.py
local_agi_zh.py","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder."
EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,insufficient history management,/,Task creation agent ignores task lists in previous task results.,IC,"local_agi.py
local_agi_mini.py
local_agi_zh.py","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,backend/api/quivr_api/modules/sync/dto/inputs.py,Upload the pptx file.
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,Unclear context in prompt,/,Knowledge articles are not being used ,IC,backend/supabase/migrations/local_20240107152745_ollama.sql,"1. Deploy locally following the instructions on https://github.com/QuivrHQ/quivr/README.md
2. Set  OLLAMA_API_BASE_URL  in .env. Added a bunch of pdf's about somthing.
3. Ask questions about the contents of the pdf you provided.
4. Quivr doesn't use the ducuments we provided to response. And if we ask to show sources it responds it has none."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,privacy violation,case1,potential security issue,IS,"backend/modules/chat/controller/chat_routes.py
","1. user-A create a account in http://localhost:3000
2. user-B create a account in http://localhost:3000 . Both user-A and user-B are from different org .
3. now from user-A create a chat called chat-A . lets chat-id is e8db74b2-ff14-4805-9374-9f2dab92f325.
4. Now goto user-B account and sent above mentioned request with chat-A id and see all the chat history ."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case2,potential security issue,IS,,"1. user-A create a account in http://localhost:3000
2. user-B create a account in http://localhost:3000 . Both user-A and user-B are from different org .
3. now from user-A create a chat called chat-A . lets chat-id is e8db74b2-ff14-4805-9374-9f2dab92f325.
4. Now goto user-B account and sent above mentioned request with chat-A id and see chat has been upload to user-A's chat .
5. Now if user-A goto his chat then he can see new chat message .
Here only drawback is that user-B(attacker) need to obtain the chat-id of different org somehow ."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case3,potential security issue,IS,backend/modules/brain/controller/brain_routes.py#L92,"1. first create a account in http://localhost:3000 and now create brain as much as your account limit .
2. Now try to create more brain and it will get error like maximum limit is reached:
(POST /brains/ HTTP/2
Host: localhost:3000
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0
Accept: application/json, text/plain, */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Referer: http://localhost:3000
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN
Content-Length: 73
Origin: http://localhost:3000

{""name"":""kkk"",""description"":""lklk"",""status"":""private"",""brain_type"":""doc""})

corresponding curl command for above request is:

curl -i -s -k -X $'POST'     -H $'Host: localhost:3000' -H $'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0' -H $'Accept: application/json, text/plain, */*' -H $'Accept-Language: en-US,en;q=0.5' -H $'Accept-Encoding: gzip, deflate, br' -H $'Referer: http://localhost/' -H $'Content-Type: application/json' -H $'Authorization: Bearer eyJhbGciOiJIUzI1NiIsImtpZCI6IkhNZUxUemZDM1ViS21hbVAiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJhdXRoZW50aWNhdGVkIiwiZXhwIjoxNzA3OTk1ODMxLCJpYXQiOjE3MDc5OTIyMzEsImlzcyI6Imh0dHBzOi8vZnFncGNpZnNmbWFtcHJ6bGR5aXYuc3VwYWJhc2UuY28vYXV0aC92MSIsInN1YiI6IjE2YTJkYjg1LTYyOWItNGQ5Mi05MDA0LTNlMDZiNTAwODY4ZiIsImVtYWlsIjoicGl5ZXRvMzU4MEBhbGlicnMuY29tIiwicGhvbmUiOiIiLCJhcHBfbWV0YWRhdGEiOnsicHJvdmlkZXIiOiJlbWFpbCIsInByb3ZpZGVycyI6WyJlbWFpbCJdfSwidXNlcl9tZXRhZGF0YSI6e30sInJvbGUiOiJhdXRoZW50aWNhdGVkIiwiYWFsIjoiYWFsMSIsImFtciI6W3sibWV0aG9kIjoib3RwIiwidGltZXN0YW1wIjoxNzA3Mjg4Njc5fV0sInNlc3Npb25faWQiOiI2NDRkODNjMC0wZTVhLTRlOWMtOGJiZC0zMmYwZWVhMjJmMjYifQ.Z4MQV-WTVbgTjBeJlhlAH9v1n4BMZQUQDTni5pjhPqM' -H $'Content-Length: 73' -H $'Origin: http://localhost' -H $'Sec-Fetch-Dest: empty' -H $'Sec-Fetch-Mode: cors' -H $'Sec-Fetch-Site: same-site' -H $'Te: trailers'     --data-binary $'{\""name\"":\""kkk\"",\""description\"":\""lklk\"",\""status\"":\""private\"",\""brain_type\"":\""doc\""}'     $'http://localhost:3000/brains/'"
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case4,potential security issue,IS,backend/routes/crawl_routes.py#L28C1-L73C19,"1. from admin(user-A) account goto http://localhost:3000/studio and create a new brain called brain-A .
2. now user-A goto Studio->Brain-A then goto knowledge and insert a url there.

here bellow request is sent to server:

POST /crawl?brain_id=812c7883-181c-40a4-826e-ff03569e0e66 HTTP/2
Host: localhost:3000
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0
Accept: application/json, text/plain, */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Referer: http://localhost:3000
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN
Content-Length: 79
Origin: http://localhost:3000
Account: TEST2

{""url"":""http://localhost"",""js"":false,""depth"":1,""max_pages"":100,""max_time"":60}

here url parameter take any input and send http request to that url. So, with this parameter value user can see request to internal network and make ssrf attack."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case5,potential security issue,IS,backend/routes/crawl_routes.py#L28C1-L73C19 ,"1. from admin account goto http://localhost:3000/studio and create a new brain called brain-A .
2. Add user-B to this brain with view permission . So, user-B has view only permission
3. Now user-B goto this brain-A url https://localhost:3000/studio/812c7883-181c-40a4-826e-ff03569e0e66 and here cant view the ""Knowledgebase"" . He gets permission denied .
4. Now user-B sent bellow request to add crawl knowledgebase to this brain""

POST /crawl?brain_id=812c7883-181c-40a4-826e-ff03569e0e66 HTTP/2
Host: localhost:3000
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0
Accept: application/json, text/plain, */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Referer: http://localhost:3000
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN
Content-Length: 79
Origin: http://localhost:3000
Account: TEST2

{""url"":""http://example.com"",""js"":false,""depth"":1,""max_pages"":100,""max_time"":60}

5. now admin goto brain-A knowledgebase and see a new crawl url has been added by user-B
So, user-B does not have edit permission in brain still user-B can add crawl url"
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case6,potential security issue,,frontend/app/studio/[brainId]/components/BrainManagementTabs/components/KnowledgeOrSecretsTab/components/KnowledgeTable/components/KnowledgeItem/components/CrawledKnowledgeItem.tsx,
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case7,potential security issue,,backend/modules/prompt/controller/prompt_routes.py#L25-L46,
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,improper text embedding,/,Timeout error during process of csv file of less than 1 mb,SL,cms/quivr/config/api.js (maybe),"1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Create a brain.Upload a csv file with more than 30 columns and 2000 rows to its knowledge.
3. After processing around 100 records, it is throwing an error in console: 

NFO:     127.0.0.1:40028 - ""GET /healthz HTTP/1.1"" 200 OK
worker        | [2024-01-09 12:11:16,239: INFO/ForkPoolWorker-8] HTTP Request: POST https://api.openai.com/v1/embeddings ""HTTP/1.1 200 OK""
worker        | [2024-01-09 12:11:29,314: INFO/ForkPoolWorker-8] HTTP Request: POST https://api.openai.com/v1/embeddings ""HTTP/1.1 200 OK""
backend-core  | INFO:     127.0.0.1:57230 - ""GET /healthz HTTP/1.1"" 200 OK
worker        | 2024-01-09 12:11:41,963 [ERROR] packages.embeddings.vectors: Error creating vector for document timed out
worker        | [2024-01-09 12:11:42,048: WARNING/ForkPoolWorker-8] Error processing file: 'NoneType' object is not iterable"
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,Imprecise knowledge retrieval,case1,Knowledge fetching is very slow,SL,"backend/models/databases/supabase/brains.py/delete_file_from_brain()
backend/routes/knowledge_routes.py","1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Create a brain, upload multiple files to its knowledge. Ask Quivr questions about the documents you uploaded.
3. Check if it takes too long to index files."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case2,Quivr sometimes cannot reply to the content of documents I upload.,IC,"backend/llm/qa_base.py
backend/models/databases/supabase/chats.py","1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Create a brain, add your knowledge. Ask Quivr questions about the documents you uploaded.
3. Observe whether the following condition occurs: Quivr sometimes cannot reply to the content of documents you upload. When you ask a question, the output content is empty, but the system log shows that the document content is being read normally."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,/,loading time too long if there are lots of knowledges,SL,"backend/models/brains.py
backend/routes/user_routes.py","1.Deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Feed about 20 files to your default brain.
3. Observe if it takes about 5 seconds to load a page every time you login in or refresh a page."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,insufficient history management,/,Quiver does not shows all the records that I aske to show in CSV file,IC,backend/tests/test_upload.py,"1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Upload a CSV-formatted bank statement onto Quivr Brain (contains at least 20 instances) and requested Quivr to display some specific instances where you made a payment of XXX.
3. Despite there being over 20 eligible instances , Quivr only presents 4 records."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,exceeding  LLM content limit,case1,Overflowing Error Message Due to Excessive Content Length ,,"frontend/app/globals.css
frontend/styles/blog.module.css","1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Upload url or ducuments to your knowledge, which will cause some errors.
3. Observe if error message is too lengthy, and overflow its container."
StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,,case2,Multiple Rounds of Coversation due to the token limitation,,"frontend/lib/config/defaultBrainConfig.ts/maxTokens
(still not working)","1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. User uploaded documents, asked Quivr write literature review.
3. Its answer structured as point 1, point 2, point3.. and point 3 was interrupted. The user asked it to continue to write on point3, but it started to write irrelevant content, ignored user's initial prompt."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,Unclear context in prompt,/, Getting weird responses,IC,(openai call: app/api/caht/openai/route.ts),"1.Model: Gpt-4o
Advance settings:
Temperature：2
Context Length:128000
Chats Include Profile Context: yes
Chats Include Workplace Instructions: yes
Embeddings Provider: OpenAI

2. Start chatting. The user asked for something, and he/she/they got some weird answer that not related to the question."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,insufficient history management,case1,Prompt in the chat bar disappears when the previous answer is finished ,IC,components/chat/chat-hooks/use-chat-handler.tsx,"1.Start chatting.
2.Type a new prompt while the chatbot is generating an answer to a previous one.
3.When the previous answer finishes to generate, the new prompt in the bar disappears and the chat bar refreshes. You will lose the contents that you have typed so far and need to start the process again."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case2,"When the AI finishes replying to a previous prompt, the content the user is typing is cleared.",IC,components/chat/chat-hooks/use-chat-handler.tsx,"1.Start chatting.
2.Type a new prompt while the chatbot is generating an answer to a previous one.
3.When the previous answer finishes to generate, the new prompt in the bar disappears and the chat bar refreshes. You will lose the contents that you have typed so far and need to start the process again."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case3,Dialog Context gets stuck ,"SL,IC","chat-ui.tsx 
chat-messages.tsx ","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in a Conversation: Start a chat session and continue it for several interactions.
3.Observe Context Freezing: Notice that after a few exchanges, the dialogue context freezes, and responses to new queries remain the same."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,unnecessary LLM output,/,When the assistant has no picture it shows a broken image on the chat,IC,components/chat/chat-input.tsx,"1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Remove Assistant's Picture: Configure the assistant to have no picture.
3.Initiate a Chat: Start a chat session where the assistant's message is displayed.
4.Observe the Broken Image: Notice that instead of a placeholder, a broken image icon appears."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,exceeding  LLM content limit,case1,context length exceeded message for GPT-3.5-turbo even for for few tokens,ST,app/api/chat/mistral/route.ts,"1.Set up Chatbot UI: Ensure you have the Chatbot UI project correctly set up and configured on your local environment.
2.Configure GPT-3.5-turbo: Modify the settings to use the GPT-3.5-turbo model.
3.Send a query: Input any query, even as short as one or two tokens.
4.Observe the error: Notice the error message indicating ""context length exceeded"" despite the short query length."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case2,Truncation of Chat Responses due to Exceeding Token Limit in Long Conversations,ST,"openai/route.ts
google/route.ts
mistral/route.ts
chat-ui.tsx
chat-messages.tsx
chat-input.tsx","1.Set Up Chatbot UI:Ensure that the Chatbot UI project is correctly set up and configured in your local environment according to the project documentation.
2.Engage in a Long Conversation:Initiate a conversation with the chatbot and continue interacting until the conversation becomes lengthy.
3.Observe Response Truncation:Monitor the chatbot’s responses. Specifically, check if new responses from the model are being truncated or cut off. This behavior may indicate that the token limit for the model's responses is being exceeded."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case3,Context length exceeded - but message is not that long,ST,"openai/route.ts
chat-ui.tsx
chat-messages.tsx","1. Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up and configured in your local environment.
2. Initiate a Long Conversation: Engage in a lengthy conversation using the GPT-4 API.
3. Input a Short Message: Enter a short message, such as ""my host supports PostgreSQL databases.""
4. Observe the Error: Notice the error indicating the context length exceeds the model's maximum limit, with a message similar to:
   [OpenAIError: This model's maximum context length is 8192 tokens. However, you requested 8554 tokens (7554 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.] {
     name: 'OpenAIError',
     type: 'invalid_request_error',
     param: 'messages',
     code: 'context_length_exceeded'
   }"
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case4,Bug Report: Long texts causing slowdowns and freezes,SL,"chat-ui.tsx
chat-messages.tsx
openai/route.ts","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in Long Text Generation: Initiate a conversation or code generation that results in lengthy texts.
3.Observe Performance: During the generation of long texts, notice slowdowns and freezes in the UI. The speed should return to normal after the process completes or is interrupted."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,,case5,Bug Report: Nothing generated after a long conversation," SL,IC","chat-ui.tsx
chat-messages.tsx","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in a Long Conversation: Continue a conversation until it becomes lengthy.
3.Observe Response Issue: Notice that after a long conversation, the AI stops generating replies. Neither the regenerate nor the re-edit functions work.
4.Start a New Conversation: Starting a new conversation works normally."
mckaywrigley/chatbot-ui,https://github.com/mckaywrigley/chatbot-ui/tree/f4ec4df77f650fa650535c6b934d4a1775cf491a,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,lib/build-prompt.ts,Upload the pptx file.
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,Unclear context in prompt,/, Getting weird responses,IC,"types/prompt.ts
components/Promptbar/components/Prompt.tsx
components/Chat/PromptList.tsx","1.Model: Gpt-4o
Advance settings:
Temperature：2
Context Length:128000
Chats Include Profile Context: yes
Chats Include Workplace Instructions: yes
Embeddings Provider: OpenAI

2. Start chatting. The user asked for something, and he/she/they got some weird answer that not related to the question."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,insufficient history management,case1,Prompt in the chat bar disappears when the previous answer is finished ,IC,"utils/app/clean.ts
agent/agentUtil.ts
pages/api/home/home.tsx","1.Start chatting.
2.Type a new prompt while the chatbot is generating an answer to a previous one.
3.When the previous answer finishes to generate, the new prompt in the bar disappears and the chat bar refreshes. You will lose the contents that you have typed so far and need to start the process again."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,,case2,"When the AI finishes replying to a previous prompt, the content the user is typing is cleared.",IC,"utils/app/clean.ts
agent/agentUtil.ts
pages/api/home/home.tsx","1.Start chatting.
2.Type a new prompt while the chatbot is generating an answer to a previous one.
3.When the previous answer finishes to generate, the new prompt in the bar disappears and the chat bar refreshes. You will lose the contents that you have typed so far and need to start the process again."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,unnecessary LLM output,/,When the assistant has no picture it shows a broken image on the chat,IC,"types/llmUsage.ts
agent/agent.ts
utils/server/storage.ts","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Remove Assistant's Picture: Configure the assistant to have no picture.
3.Initiate a Chat: Start a chat session where the assistant's message is displayed.
4.Observe the Broken Image: Notice that instead of a placeholder, a broken image icon appears."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,exceeding  LLM content limit,case1,context length exceeded message for GPT-3.5-turbo even for for few tokens,ST,"types/llmUsage.ts
components/Chat/ChatInputTokenCount.tsx","1.Set up Chatbot UI: Ensure you have the Chatbot UI project correctly set up and configured on your local environment.
2.Configure GPT-3.5-turbo: Modify the settings to use the GPT-3.5-turbo model.
3.Send a query: Input any query, even as short as one or two tokens.
4.Observe the error: Notice the error message indicating ""context length exceeded"" despite the short query length."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,,case2,Truncation of Chat Responses due to Exceeding Token Limit in Long Conversations,ST,"types/llmUsage.ts
components/Chat/ChatInputTokenCount.tsx","1.Set Up Chatbot UI:Ensure that the Chatbot UI project is correctly set up and configured in your local environment according to the project documentation.
2.Engage in a Long Conversation:Initiate a conversation with the chatbot and continue interacting until the conversation becomes lengthy.
3.Observe Response Truncation:Monitor the chatbot’s responses. Specifically, check if new responses from the model are being truncated or cut off. This behavior may indicate that the token limit for the model's responses is being exceeded."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,,case3,Context length exceeded - but message is not that long,ST,"types/llmUsage.ts
components/Chat/ChatInputTokenCount.tsx","1. Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up and configured in your local environment.
2. Initiate a Long Conversation: Engage in a lengthy conversation using the GPT-4 API.
3. Input a Short Message: Enter a short message, such as ""my host supports PostgreSQL databases.""
4. Observe the Error: Notice the error indicating the context length exceeds the model's maximum limit, with a message similar to:
   [OpenAIError: This model's maximum context length is 8192 tokens. However, you requested 8554 tokens (7554 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.] {
     name: 'OpenAIError',
     type: 'invalid_request_error',
     param: 'messages',
     code: 'context_length_exceeded'
   }"
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,,case4,Bug Report: Long texts causing slowdowns and freezes,SL,"types/llmUsage.ts
components/Chat/ChatInputTokenCount.tsx","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in Long Text Generation: Initiate a conversation or code generation that results in lengthy texts.
3.Observe Performance: During the generation of long texts, notice slowdowns and freezes in the UI. The speed should return to normal after the process completes or is interrupted."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,,case5,Bug Report: Nothing generated after a long conversation," SL,IC","types/llmUsage.ts
components/Chat/ChatInputTokenCount.tsx","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in a Long Conversation: Continue a conversation until it becomes lengthy.
3.Observe Response Issue: Notice that after a long conversation, the AI stops generating replies. Neither the regenerate nor the re-edit functions work.
4.Start a New Conversation: Starting a new conversation works normally."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,insufficient history management,/,Dialog Context gets stuck ,"SL,IC","utils/app/clean.ts
agent/agentUtil.ts","1.Set up Chatbot UI: Ensure the Chatbot UI project is correctly set up in your local environment.
2.Engage in a Conversation: Start a chat session and continue it for several interactions.
3.Observe Context Freezing: Notice that after a few exchanges, the dialogue context freezes, and responses to new queries remain the same."
dotneet/smart-chatbot-ui,https://github.com/dotneet/smart-chatbot-ui/tree/6a07b944b73ca53996a1a06cdf65ab19ed2a19ab,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"components/Input/InputText.tsx
components/Chat/ChatInput.tsx
components/Input/Checkbox.tsx",Upload the pptx file.
arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,Incompatible LLM output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/html_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly."
arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,Unclear context in prompt,/,text overflow in chat's title,IC,"frontend/src/components/Navigation.jsx
frontend/src/styles/Navigation.css","1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Initiate a Chat: Start a chat session within DocsGPT.
3.Create a Long Title: Generate a chat session title that is long enough to cause overflow.
4.Observe Text Overflow: Notice that the chat's title text overflows and is not contained within the title box."
arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"middleware.ts
pages/api/auth/[...nextauth].ts",Upload the pptx file.
arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,Incompatible LLM output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/rst_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly."
yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,Unclear context in prompt,/,text overflow in chat's title,IC,"application/app.py
scripts/ingest.py","1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Initiate a Chat: Start a chat session within DocsGPT.
3.Create a Long Title: Generate a chat session title that is long enough to cause overflow.
4.Observe Text Overflow: Notice that the chat's title text overflows and is not contained within the title box."
yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"frontend/src/conversation/ConversationInput.tsx
application/static/src/input.css",Upload the pptx file.
yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,scripts/old/ingest_rst.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,Incompatible LLM output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/html_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly."
DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,Unclear context in prompt,/,text overflow in chat's title,IC,"frontend/src/components/Navigation.jsx
frontend/src/styles/Navigation.css","1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Initiate a Chat: Start a chat session within DocsGPT.
3.Create a Long Title: Generate a chat session title that is long enough to cause overflow.
4.Observe Text Overflow: Notice that the chat's title text overflows and is not contained within the title box."
DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"middleware.ts
pages/api/auth/[...nextauth].ts",Upload the pptx file.
DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
chatchat-space/Langchain-Chatchat,https://github.com/chatchat-space/Langchain-Chatchat/tree/bc7f01925fe49c622cf7d6e2540c03fd5a0679e1,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","libs/chatchat-server/chatchat/server/localai_embeddings.py/embed_with_retry
libs/python-sdk/open_chatcaht/api/knowledge_base/knowledge_base_client.py/recreate_vector_store
libs/chatchat-server/chatchat/webui_pages/utils.py/recreate_vector_store
libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py/faiss_cache.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists."""
chatchat-space/Langchain-Chatchat,https://github.com/chatchat-space/Langchain-Chatchat/tree/bc7f01925fe49c622cf7d6e2540c03fd5a0679e1,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"frontend/src/database/models/topic.ts
Function: The query_knowledge_base method in the KnowledgeQuery class","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
chatchat-space/Langchain-Chatchat,https://github.com/chatchat-space/Langchain-Chatchat/tree/bc7f01925fe49c622cf7d6e2540c03fd5a0679e1,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,,"1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
chatchat-space/Langchain-Chatchat,https://github.com/chatchat-space/Langchain-Chatchat/tree/bc7f01925fe49c622cf7d6e2540c03fd5a0679e1,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"frontend/src/libs/agent-runtime/zhipu/authToken.ts
libs/chatchat-server/chatchat/server/memory/conversation_db_buffer_memory.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","chains/modules/embeddings.py
chains/local_doc_qa.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists."""
hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"chains/dialogue_answering/prompts.py
views/src/store/modules/prompt/index.ts","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"agent/custom_agent.py
models/llama_llm.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"models/llama_llm.py
models/chatglm_llm.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","chains/modules/embeddings.py
chains/modules/vectorstores.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists."""
ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"views/src/store/modules/prompt/index.ts
views/src/store/modules/prompt/helper.ts","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"models/base.py
models/llama_llm.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"models/llama_llm.py
models/base.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","server/knowledge_base/kb_service/base.py
server/knowledge_base/utils.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists."""
zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"chains/llmchain_with_history.py
server/chat/chat.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"server/chat/chat.py
chains/llmchain_with_history.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"server/chat/search_engine_chat.py
server/chat/knowledge_base_chat.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"private_gpt/server/embeddings/embeddings_router.py
private_gpt/server/chat/chat_service.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,exceeding  LLM content limit,case1,1.too many tokens ,ST,"private_gpt/settings/settings.py
private_gpt/components/llm/custom/sagemaker.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,The prompt size exceeds the context window size and cannot be processed.,ST,"private_gpt/settings/settings.py
private_gpt/components/llm/custom/sagemaker.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure the MODEL_N_CTX value is set in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Long Query: Input a query that exceeds the context window size.
5.Observe the Error: Notice the error message indicating that the prompt size exceeds the context window size and cannot be processed."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case3,ValueError: Requested tokens (733) exceed context window of 512,ST,"private_gpt/settings/settings.py
private_gpt/components/llm/custom/sagemaker.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Add a PDF: Upload a PDF document to the model.
3.Ask a Detailed Question: Input a question that requires a detailed response, resulting in more than 512 tokens.
4.Observe the Error: Notice the error message ""ValueError: Requested tokens (733) exceed context window of 512."""
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case4,ERROR: The prompt size exceeds the context window size and cannot be processed. ,ST,"private_gpt/settings/settings.py
private_gpt/components/llm/custom/sagemaker.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure MODEL_N_CTX in the .env file is set appropriately. Default might be 2048.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query with a size larger than the context window, such as 2614 tokens.
5.Observe the Error: Notice the error message ""ERROR: The prompt size exceeds the context window size and cannot be processed."""
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,Incompatible LLM output format,case1,Answers contain additional prompts with certain models ,IC,"private_gpt/components/llm/custom/sagemaker.py
scripts/extract_openapi.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,Trouble with newlines or lists in answers ,IC,"private_gpt/components/llm/custom/sagemaker.py
scripts/extract_openapi.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that requires a multiline or bullet-pointed response, such as ""Tell me 3 jokes"".
5. Observe the Response: Notice that the AI's response cuts off at newlines or fails to format lists properly."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case3,Answering weird @@@@ to any question,IC,"private_gpt/components/llm/custom/sagemaker.py
scripts/extract_openapi.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Upload a Document: Upload a document for processing.
5. Ask a Question: Enter a question that expects a multiline or bullet-pointed response, such as ""List 5 benefits of exercise.""
6. Observe the Response: Notice if the AI's response is truncated or incorrectly formatted."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case4,"The response contains the correct source document and relevant sections/pages, yet it does not meet my anticipated outcome. ",IC,"private_gpt/components/llm/custom/sagemaker.py
scripts/extract_openapi.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Perform the Environment Setup: Follow the setup instructions in the project's README.
3.Ask the Question: Input the question ""What is the most fundamental right in America?"".
4.Observe the Response: Notice that while the response references the correct document, it doesn't align with the source content accurately."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,Unclear context in prompt,case1,Sources are not being used,IC,private_gpt/components/llm/prompt_helper.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,source docs don't seem  to correspond to answer,IC,private_gpt/components/llm/prompt_helper.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Ingest Documents: Upload 1047 different PDF files (4GB) into the system.
3. Run the Server: Execute `python privateGPT.py`.
4. Ask Multiple Queries: Make several unrelated queries with subjects exclusive to different source files.
5. Observe the Sources: Notice that the responses always exhibit the same sources for every query, even when those sources have zero relation to the query context."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case3,Hallucination: Answers are not from the docs but from the model's own knowledge base ,IC,private_gpt/components/llm/prompt_helper.py,"1. Ensure PrivateGPT is set up and the server is running.
2. Open the web UI and navigate to the chat interface.
3. Enter the query `“Explain the concept of quantum entanglement in detail.”`
4. Submit the query.
5. Observe the response to check if it is incomplete or truncated, especially if it cuts off abruptly or lacks detailed explanation."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case4,"Is the ANSWER a pure-LLM answer OR context-based-answer ? 

Text formatting options in the chat editor are missing, causing difficulty in structuring messages.",IC,private_gpt/components/llm/prompt_helper.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Open the chat interface in the web UI.
3. Enter a query that requires a context-based answer, such as: ""Summarize the main points from the provided text.""
4. Observe the response to determine if it integrates context from previous messages or if it is purely based on the LLM's capabilities.
5. Note the missing text formatting options in the chat editor that affect the structuring of messages."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI",private_gpt/components/llm/prompt_helper.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,output length is too short,UI,private_gpt/components/llm/prompt_helper.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question like ""How to increase the output length?""
5. Observe the Response: Note that the response length varies and there is no fixed length or parameter to control the output length."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,Imprecise knowledge retrieval,/,ingest does not complete,"IC,SL",private_gpt/server/chunks/chunks_router.py,"1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,private_gpt/settings/settings.py,"1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,not enough space in the context's memory pool ,IC,private_gpt/settings/settings.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Pool: Set a large value for the `context_memory_pool_size` parameter in the configuration file.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query that generates a large amount of data, such as: ""Explain the history and significance of the French Revolution in detail.""
6. Observe if an error occurs indicating ""not enough space in the context's memory pool."""
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,private_gpt/settings/settings.py,"Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal."
imartinez/privateGPT,https://github.com/imartinez/privateGPT/tree/2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1,,case2,Semaphore leak crashes the program ·,ST,private_gpt/settings/settings.py,"1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure API Endpoint: Set the API endpoint in the configuration file to an incorrect or non-existent URL.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query and submit it.
6. Observe if there is an error message or if the server fails to respond due to the incorrect API endpoint."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"private_gpt/settings/settings.py
private_gpt/components/llm/custom/sagemaker.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,exceeding  LLM content limit,case1,1.too many tokens ,ST,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,The prompt size exceeds the context window size and cannot be processed.,ST,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure the MODEL_N_CTX value is set in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Long Query: Input a query that exceeds the context window size.
5.Observe the Error: Notice the error message indicating that the prompt size exceeds the context window size and cannot be processed."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case3,ValueError: Requested tokens (733) exceed context window of 512,ST,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Add a PDF: Upload a PDF document to the model.
3.Ask a Detailed Question: Input a question that requires a detailed response, resulting in more than 512 tokens.
4.Observe the Error: Notice the error message ""ValueError: Requested tokens (733) exceed context window of 512."""
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case4,ERROR: The prompt size exceeds the context window size and cannot be processed. ,ST,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure MODEL_N_CTX in the .env file is set appropriately. Default might be 2048.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query with a size larger than the context window, such as 2614 tokens.
5.Observe the Error: Notice the error message ""ERROR: The prompt size exceeds the context window size and cannot be processed."""
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,Incompatible LLM output format,case1,Answers contain additional prompts with certain models ,IC,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,Trouble with newlines or lists in answers ,IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that requires a multiline or bullet-pointed response, such as ""Tell me 3 jokes"".
5. Observe the Response: Notice that the AI's response cuts off at newlines or fails to format lists properly."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case3,Answering weird @@@@ to any question,IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Upload a Document: Upload a document for processing.
5. Ask a Question: Enter a question that expects a multiline or bullet-pointed response, such as ""List 5 benefits of exercise.""
6. Observe the Response: Notice if the AI's response is truncated or incorrectly formatted."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case4,"The response contains the correct source document and relevant sections/pages, yet it does not meet my anticipated outcome. ",IC,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Perform the Environment Setup: Follow the setup instructions in the project's README.
3.Ask the Question: Input the question ""What is the most fundamental right in America?"".
4.Observe the Response: Notice that while the response references the correct document, it doesn't align with the source content accurately."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,Unclear context in prompt,case1,Sources are not being used,IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,source docs don't seem  to correspond to answer,IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Ingest Documents: Upload 1047 different PDF files (4GB) into the system.
3. Run the Server: Execute `python privateGPT.py`.
4. Ask Multiple Queries: Make several unrelated queries with subjects exclusive to different source files.
5. Observe the Sources: Notice that the responses always exhibit the same sources for every query, even when those sources have zero relation to the query context."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case3,Hallucination: Answers are not from the docs but from the model's own knowledge base ,IC,"app.py
privateGPT.py","1. Ensure PrivateGPT is set up and the server is running.
2. Open the web UI and navigate to the chat interface.
3. Enter the query `“Explain the concept of quantum entanglement in detail.”`
4. Submit the query.
5. Observe the response to check if it is incomplete or truncated, especially if it cuts off abruptly or lacks detailed explanation."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case4,"Is the ANSWER a pure-LLM answer OR context-based-answer ? 

Text formatting options in the chat editor are missing, causing difficulty in structuring messages.",IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Open the chat interface in the web UI.
3. Enter a query that requires a context-based answer, such as: ""Summarize the main points from the provided text.""
4. Observe the response to determine if it integrates context from previous messages or if it is purely based on the LLM's capabilities.
5. Note the missing text formatting options in the chat editor that affect the structuring of messages."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,output length is too short,UI,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question like ""How to increase the output length?""
5. Observe the Response: Note that the response length varies and there is no fixed length or parameter to control the output length."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,Imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,not enough space in the context's memory pool ,IC,"streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Pool: Set a large value for the `context_memory_pool_size` parameter in the configuration file.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query that generates a large amount of data, such as: ""Explain the history and significance of the French Revolution in detail.""
6. Observe if an error occurs indicating ""not enough space in the context's memory pool."""
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"streamlit_app.py
app.py
privateGPT.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal."
menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,,case2,Semaphore leak crashes the program ·,ST,"streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure API Endpoint: Set the API endpoint in the configuration file to an incorrect or non-existent URL.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query and submit it.
6. Observe if there is an error message or if the server fails to respond due to the incorrect API endpoint."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"privateGPT.py
ingest.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,exceeding  LLM content limit,case1,1.too many tokens ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,The prompt size exceeds the context window size and cannot be processed.,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure the MODEL_N_CTX value is set in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Long Query: Input a query that exceeds the context window size.
5.Observe the Error: Notice the error message indicating that the prompt size exceeds the context window size and cannot be processed."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case3,ValueError: Requested tokens (733) exceed context window of 512,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Add a PDF: Upload a PDF document to the model.
3.Ask a Detailed Question: Input a question that requires a detailed response, resulting in more than 512 tokens.
4.Observe the Error: Notice the error message ""ValueError: Requested tokens (733) exceed context window of 512."""
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case4,ERROR: The prompt size exceeds the context window size and cannot be processed. ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure MODEL_N_CTX in the .env file is set appropriately. Default might be 2048.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query with a size larger than the context window, such as 2614 tokens.
5.Observe the Error: Notice the error message ""ERROR: The prompt size exceeds the context window size and cannot be processed."""
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,Incompatible LLM output format,case1,Answers contain additional prompts with certain models ,IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,Trouble with newlines or lists in answers ,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that requires a multiline or bullet-pointed response, such as ""Tell me 3 jokes"".
5. Observe the Response: Notice that the AI's response cuts off at newlines or fails to format lists properly."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case3,Answering weird @@@@ to any question,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Upload a Document: Upload a document for processing.
5. Ask a Question: Enter a question that expects a multiline or bullet-pointed response, such as ""List 5 benefits of exercise.""
6. Observe the Response: Notice if the AI's response is truncated or incorrectly formatted."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case4,"The response contains the correct source document and relevant sections/pages, yet it does not meet my anticipated outcome. ",IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Perform the Environment Setup: Follow the setup instructions in the project's README.
3.Ask the Question: Input the question ""What is the most fundamental right in America?"".
4.Observe the Response: Notice that while the response references the correct document, it doesn't align with the source content accurately."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,Unclear context in prompt,case1,Sources are not being used,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,source docs don't seem  to correspond to answer,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Ingest Documents: Upload 1047 different PDF files (4GB) into the system.
3. Run the Server: Execute `python privateGPT.py`.
4. Ask Multiple Queries: Make several unrelated queries with subjects exclusive to different source files.
5. Observe the Sources: Notice that the responses always exhibit the same sources for every query, even when those sources have zero relation to the query context."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case3,Hallucination: Answers are not from the docs but from the model's own knowledge base ,IC,"privateGPT.py
ingest.py","1. Ensure PrivateGPT is set up and the server is running.
2. Open the web UI and navigate to the chat interface.
3. Enter the query `“Explain the concept of quantum entanglement in detail.”`
4. Submit the query.
5. Observe the response to check if it is incomplete or truncated, especially if it cuts off abruptly or lacks detailed explanation."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case4,"Is the ANSWER a pure-LLM answer OR context-based-answer ? 

Text formatting options in the chat editor are missing, causing difficulty in structuring messages.",IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Open the chat interface in the web UI.
3. Enter a query that requires a context-based answer, such as: ""Summarize the main points from the provided text.""
4. Observe the response to determine if it integrates context from previous messages or if it is purely based on the LLM's capabilities.
5. Note the missing text formatting options in the chat editor that affect the structuring of messages."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,output length is too short,UI,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question like ""How to increase the output length?""
5. Observe the Response: Note that the response length varies and there is no fixed length or parameter to control the output length."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,Imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,not enough space in the context's memory pool ,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Pool: Set a large value for the `context_memory_pool_size` parameter in the configuration file.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query that generates a large amount of data, such as: ""Explain the history and significance of the French Revolution in detail.""
6. Observe if an error occurs indicating ""not enough space in the context's memory pool."""
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"privateGPT.py
ingest.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal."
AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,,case2,Semaphore leak crashes the program ·,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure API Endpoint: Set the API endpoint in the configuration file to an incorrect or non-existent URL.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query and submit it.
6. Observe if there is an error message or if the server fails to respond due to the incorrect API endpoint."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,Missing LLM input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"privateGPT.py
ingest.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,exceeding  LLM content limit,case1,1.too many tokens ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,The prompt size exceeds the context window size and cannot be processed.,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure the MODEL_N_CTX value is set in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Long Query: Input a query that exceeds the context window size.
5.Observe the Error: Notice the error message indicating that the prompt size exceeds the context window size and cannot be processed."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case3,ValueError: Requested tokens (733) exceed context window of 512,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Add a PDF: Upload a PDF document to the model.
3.Ask a Detailed Question: Input a question that requires a detailed response, resulting in more than 512 tokens.
4.Observe the Error: Notice the error message ""ValueError: Requested tokens (733) exceed context window of 512."""
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case4,ERROR: The prompt size exceeds the context window size and cannot be processed. ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Ensure MODEL_N_CTX in the .env file is set appropriately. Default might be 2048.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query with a size larger than the context window, such as 2614 tokens.
5.Observe the Error: Notice the error message ""ERROR: The prompt size exceeds the context window size and cannot be processed."""
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,Incompatible LLM output format,case1,Answers contain additional prompts with certain models ,IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,Trouble with newlines or lists in answers ,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that requires a multiline or bullet-pointed response, such as ""Tell me 3 jokes"".
5. Observe the Response: Notice that the AI's response cuts off at newlines or fails to format lists properly."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case3,Answering weird @@@@ to any question,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Upload a Document: Upload a document for processing.
5. Ask a Question: Enter a question that expects a multiline or bullet-pointed response, such as ""List 5 benefits of exercise.""
6. Observe the Response: Notice if the AI's response is truncated or incorrectly formatted."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case4,"The response contains the correct source document and relevant sections/pages, yet it does not meet my anticipated outcome. ",IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Perform the Environment Setup: Follow the setup instructions in the project's README.
3.Ask the Question: Input the question ""What is the most fundamental right in America?"".
4.Observe the Response: Notice that while the response references the correct document, it doesn't align with the source content accurately."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,Unclear context in prompt,case1,Sources are not being used,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,source docs don't seem  to correspond to answer,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Ingest Documents: Upload 1047 different PDF files (4GB) into the system.
3. Run the Server: Execute `python privateGPT.py`.
4. Ask Multiple Queries: Make several unrelated queries with subjects exclusive to different source files.
5. Observe the Sources: Notice that the responses always exhibit the same sources for every query, even when those sources have zero relation to the query context."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case3,Hallucination: Answers are not from the docs but from the model's own knowledge base ,IC,"privateGPT.py
ingest.py","1. Ensure PrivateGPT is set up and the server is running.
2. Open the web UI and navigate to the chat interface.
3. Enter the query `“Explain the concept of quantum entanglement in detail.”`
4. Submit the query.
5. Observe the response to check if it is incomplete or truncated, especially if it cuts off abruptly or lacks detailed explanation."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case4,"Is the ANSWER a pure-LLM answer OR context-based-answer ? 

Text formatting options in the chat editor are missing, causing difficulty in structuring messages.",IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Open the chat interface in the web UI.
3. Enter a query that requires a context-based answer, such as: ""Summarize the main points from the provided text.""
4. Observe the response to determine if it integrates context from previous messages or if it is purely based on the LLM's capabilities.
5. Note the missing text formatting options in the chat editor that affect the structuring of messages."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,output length is too short,UI,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question like ""How to increase the output length?""
5. Observe the Response: Note that the response length varies and there is no fixed length or parameter to control the output length."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,Imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,not enough space in the context's memory pool ,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Pool: Set a large value for the `context_memory_pool_size` parameter in the configuration file.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query that generates a large amount of data, such as: ""Explain the history and significance of the French Revolution in detail.""
6. Observe if an error occurs indicating ""not enough space in the context's memory pool."""
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"privateGPT.py
ingest.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal."
REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,,case2,Semaphore leak crashes the program ·,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure API Endpoint: Set the API endpoint in the configuration file to an incorrect or non-existent URL.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a query and submit it.
6. Observe if there is an error message or if the server fails to respond due to the incorrect API endpoint."
camel-ai/camel,https://github.com/camel-ai/camel/tree/71d466d9d67d1d7230ebfa1be6db95f5c53e38d9,privacy violation,/,Support Swagger/OpenAPI 2.0 and 3.0 (.yaml) without the need for security authentication,IS,test/agents/test_deductive_reasoner_agent.py,"1. Set up Camel: Ensure the project is correctly set up in your local environment.
2. Configure Swagger/OpenAPI: Prepare a Swagger/OpenAPI 2.0 or 3.0 `.yaml` file without security authentication settings.
3. Run Camel: Execute `python camel.py` or the relevant command to start the server.
4. Open the Swagger/OpenAPI interface in the web UI.
5. Import the `.yaml` file.
6. Observe if the file is correctly loaded and displayed without requiring security authentication."
camel-ai/camel,https://github.com/camel-ai/camel/tree/71d466d9d67d1d7230ebfa1be6db95f5c53e38d9,sketchy error handling,/,Multi-agent Compatibility Score of Role Assignment ,ST,camel/toolkits/twitter_toolkit.py,"1. Set up Camel: Ensure the project is correctly set up in your local environment.
2. Configure Multi-Agent Environment: Set up multiple agents with defined roles in the Camel system.
3. Run Camel: Execute `python camel.py` or the relevant command to start the server.
4. Open the role assignment interface in the web UI.
5. Assign roles to the agents and configure their interactions.
6. Observe if the system correctly calculates and displays the Multi-Agent Compatibility Score for the role assignments."
mattzcarey/code-review-gpt,https://github.com/mattzcarey/code-review-gpt/tree/87bce4d443c869eb3da09e5a2c11b1a79c5e5d6a,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"packages/code-review-gpt/src/review/prompt/constructPrompt/batchFiles/utils/createPromptFiles.ts
services/core/functions/webhook/src/prompts/buildPrompt.ts","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
mattzcarey/code-review-gpt,https://github.com/mattzcarey/code-review-gpt/tree/87bce4d443c869eb3da09e5a2c11b1a79c5e5d6a,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"packages/code-review-gpt/src/common/ci/utils.ts
packages/code-review-gpt/src/config.ts","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
mattzcarey/code-review-gpt,https://github.com/mattzcarey/code-review-gpt/tree/87bce4d443c869eb3da09e5a2c11b1a79c5e5d6a,unnecessary LLM output,/,Extremely polite and sometimes verbose output,"IC,UI","packages/code-review-gpt/src/review/llm/askAI.ts
packages/code-review-gpt/src/review/llm/PriorityQueue.ts","1. Set up Code Review GPT: Ensure the project is correctly set up in your local environment.
2. Run Code Review GPT: Execute `python code_review_gpt.py` or the relevant command to start the server.
3. Open the chat interface or input field.
4. Enter a query that typically requires a concise response, such as: ""Explain the purpose of a `for` loop in Python.""
5. Observe the output: Notice if the response is excessively polite or verbose, deviating from a succinct and clear answer."
chenhunghan/code-review-private-ai,https://github.com/chenhunghan/code-review-private-ai/tree/17f00ec43ba35f3914fea322ad84f4b1a888500f,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"src/review/prompt/templates.ts
src/review/prompt/getDiffFiles.ts","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
chenhunghan/code-review-private-ai,https://github.com/chenhunghan/code-review-private-ai/tree/17f00ec43ba35f3914fea322ad84f4b1a888500f,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,src/review/ci/commentOnPR.ts,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
chenhunghan/code-review-private-ai,https://github.com/chenhunghan/code-review-private-ai/tree/17f00ec43ba35f3914fea322ad84f4b1a888500f,unnecessary LLM output,/,Extremely polite and sometimes verbose output,"IC,UI","utils/build.js
src/review/llm/AIModel.ts
src/review/index.ts","1. Set up Code Review GPT: Ensure the project is correctly set up in your local environment.
2. Run Code Review GPT: Execute `python code_review_gpt.py` or the relevant command to start the server.
3. Open the chat interface or input field.
4. Enter a query that typically requires a concise response, such as: ""Explain the purpose of a `for` loop in Python.""
5. Observe the output: Notice if the response is excessively polite or verbose, deviating from a succinct and clear answer."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"letta/main.py
",Upload the pptx file.
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"paper_experiments/doc_qa_task/doc_qa.py
letta/schemas/openai/chat_completion_response.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,knowledge misalignment,/,Attaching data to agents is not working as expected,IC,"letta/settings.py
letta/embeddings.py
letta/server/rest_api/interface.py","1.Set up MemGPT:Ensure MemGPT is correctly set up and running in your local environment.
2.Modify Configuration:Open the config.yaml file located in the project directory.
Set the max_memory parameter to 1024MB and the memory_limit parameter to 512MB.
3.Run a Memory-Intensive Task:Execute a memory-intensive task that exceeds the memory_limit parameter. For example, run the command:python run_task.py --task heavy_computation
4.Monitor Memory Usage:Observe the memory usage of the MemGPT process. Use a tool like htop or psutil to monitor memory consumption.
5.Observe the Error:Note if an error is thrown indicating that the memory usage has exceeded the limit. The error message should be similar to:MemoryError: Exceeded memory limit of 512MB.
6.Check Logs:Review the application logs for any additional error messages or stack traces related to the memory limit."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,improper text embedding,case1,Timeout errors on MemGPT embedding endpoint ,"SL,TK","letta/settings.py
letta/embeddings.py
letta/server/rest_api/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Start MemGPT Server: Execute `python memgpt.py` or the relevant command to run the server.
3. Open the embedding endpoint in the API interface or client.
4. Send an embedding request to the endpoint.
5. Observe if a timeout error occurs or if the request fails to complete within the expected time frame."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,,case2,Errors: on: memgpt load directory / embeddings / api: validation error: inputs must have less than 512 tokens. LOCAL: sources are not compatible with this agent's embedding model,"ST,IC","letta/settings.py
letta/embeddings.py
letta/server/rest_api/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Embedding Model: Verify the configuration settings for the embedding model and ensure it is properly set up.
3. Load Data: Attempt to load data from the specified directory (`/embeddings/api`).
4. Ensure Token Limit: Prepare data inputs with fewer than 512 tokens.
5. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
6. Observe Errors: Check if validation errors occur related to token limits or compatibility issues, and ensure that the sources are compatible with the agent's embedding model."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,inefficient memory management,/,memgpt server fails once context memory is full (when it calls archival_memory_insert) ,ST,"letta/schemas/memory.py
letta/memory.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set the memory context to a specific limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions to fill up the context memory.
6. Observe if the server fails or encounters issues when the context memory is full, especially during calls to `archival_memory_insert`."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,exceeding  LLM content limit,case1,Memory context limit exceeded ,ST,"letta/constants.py
letta/system.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,,case2,Track token use with local LLMs,ST,"letta/constants.py
letta/system.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Local LLMs: Set up and configure local LLMs in the project.
3. Enable Token Tracking: Adjust configuration settings or code to enable token usage tracking for local LLMs.
4. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
5. Open the chat or interaction interface.
6. Input a series of queries or commands and observe if token usage is correctly tracked and reported."
cpacker/MemGPT,https://github.com/cpacker/MemGPT/tree/6e2c92e3abda617b9efc96c88b582195579717c8,,case3,Lack of safeguard on tokens returned by external functions,ST,"letta/constants.py
letta/system.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure External Functions: Set up external functions that return tokens in the system.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Invoke external functions that return tokens.
6. Observe if there are any safeguards or validations on the tokens returned by these external functions, and check for potential issues or vulnerabilities related to token handling."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"memgpt/cli/cli_load.py
memgpt/memory.py",Upload the pptx file.
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py
memgpt/constants.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,knowledge misalignment,/,Attaching data to agents is not working as expected,IC,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1.Set up MemGPT:Ensure MemGPT is correctly set up and running in your local environment.
2.Modify Configuration:Open the config.yaml file located in the project directory.
Set the max_memory parameter to 1024MB and the memory_limit parameter to 512MB.
3.Run a Memory-Intensive Task:Execute a memory-intensive task that exceeds the memory_limit parameter. For example, run the command:python run_task.py --task heavy_computation
4.Monitor Memory Usage:Observe the memory usage of the MemGPT process. Use a tool like htop or psutil to monitor memory consumption.
5.Observe the Error:Note if an error is thrown indicating that the memory usage has exceeded the limit. The error message should be similar to:MemoryError: Exceeded memory limit of 512MB.
6.Check Logs:Review the application logs for any additional error messages or stack traces related to the memory limit."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,improper text embedding,case1,Timeout errors on MemGPT embedding endpoint ,SL,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/server/rest_api/openai_assistants/assistants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Start MemGPT Server: Execute `python memgpt.py` or the relevant command to run the server.
3. Open the embedding endpoint in the API interface or client.
4. Send an embedding request to the endpoint.
5. Observe if a timeout error occurs or if the request fails to complete within the expected time frame."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,,case2,Errors: on: memgpt load directory / embeddings / api: validation error: inputs must have less than 512 tokens. LOCAL: sources are not compatible with this agent's embedding model,"ST,IC","memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/server/rest_api/openai_assistants/assistants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Embedding Model: Verify the configuration settings for the embedding model and ensure it is properly set up.
3. Load Data: Attempt to load data from the specified directory (`/embeddings/api`).
4. Ensure Token Limit: Prepare data inputs with fewer than 512 tokens.
5. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
6. Observe Errors: Check if validation errors occur related to token limits or compatibility issues, and ensure that the sources are compatible with the agent's embedding model."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,inefficient memory management,/,memgpt server fails once context memory is full (when it calls archival_memory_insert) ,ST,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set the memory context to a specific limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions to fill up the context memory.
6. Observe if the server fails or encounters issues when the context memory is full, especially during calls to `archival_memory_insert`."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,exceeding  LLM content limit,case1,Memory context limit exceeded ,ST,"memgpt/data_types.py
memgpt/constants.py
memgpt/metadata.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,,case2,Track token use with local LLMs,ST,"memgpt/data_types.py
memgpt/constants.py
memgpt/metadata.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Local LLMs: Set up and configure local LLMs in the project.
3. Enable Token Tracking: Adjust configuration settings or code to enable token usage tracking for local LLMs.
4. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
5. Open the chat or interaction interface.
6. Input a series of queries or commands and observe if token usage is correctly tracked and reported."
starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,,case3,Lack of safeguard on tokens returned by external functions,ST,"memgpt/data_types.py
memgpt/constants.py
memgpt/metadata.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure External Functions: Set up external functions that return tokens in the system.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Invoke external functions that return tokens.
6. Observe if there are any safeguards or validations on the tokens returned by these external functions, and check for potential issues or vulnerabilities related to token handling."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"memgpt/main.py
memgpt/cli/cli_load.py
memgpt/data_sources/connectors.py",Upload the pptx file.
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"memgpt/models/openai.py
paper_experiments/doc_qa_task/doc_qa.py
memgpt/models/chat_completion_response.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,knowledge misalignment,/,Attaching data to agents is not working as expected,IC,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1.Set up MemGPT:Ensure MemGPT is correctly set up and running in your local environment.
2.Modify Configuration:Open the config.yaml file located in the project directory.
Set the max_memory parameter to 1024MB and the memory_limit parameter to 512MB.
3.Run a Memory-Intensive Task:Execute a memory-intensive task that exceeds the memory_limit parameter. For example, run the command:python run_task.py --task heavy_computation
4.Monitor Memory Usage:Observe the memory usage of the MemGPT process. Use a tool like htop or psutil to monitor memory consumption.
5.Observe the Error:Note if an error is thrown indicating that the memory usage has exceeded the limit. The error message should be similar to:MemoryError: Exceeded memory limit of 512MB.
6.Check Logs:Review the application logs for any additional error messages or stack traces related to the memory limit."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,improper text embedding,case1,Timeout errors on MemGPT embedding endpoint ,SL,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Start MemGPT Server: Execute `python memgpt.py` or the relevant command to run the server.
3. Open the embedding endpoint in the API interface or client.
4. Send an embedding request to the endpoint.
5. Observe if a timeout error occurs or if the request fails to complete within the expected time frame."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,,case2,Errors: on: memgpt load directory / embeddings / api: validation error: inputs must have less than 512 tokens. LOCAL: sources are not compatible with this agent's embedding model,"ST,IC","memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Embedding Model: Verify the configuration settings for the embedding model and ensure it is properly set up.
3. Load Data: Attempt to load data from the specified directory (`/embeddings/api`).
4. Ensure Token Limit: Prepare data inputs with fewer than 512 tokens.
5. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
6. Observe Errors: Check if validation errors occur related to token limits or compatibility issues, and ensure that the sources are compatible with the agent's embedding model."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,inefficient memory management,/,memgpt server fails once context memory is full (when it calls archival_memory_insert) ,ST,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set the memory context to a specific limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions to fill up the context memory.
6. Observe if the server fails or encounters issues when the context memory is full, especially during calls to `archival_memory_insert`."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,exceeding  LLM content limit,case1,Memory context limit exceeded ,ST,"memgpt/data_types.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,,case2,Track token use with local LLMs,ST,"memgpt/data_types.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Local LLMs: Set up and configure local LLMs in the project.
3. Enable Token Tracking: Adjust configuration settings or code to enable token usage tracking for local LLMs.
4. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
5. Open the chat or interaction interface.
6. Input a series of queries or commands and observe if token usage is correctly tracked and reported."
goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,,case3,Lack of safeguard on tokens returned by external functions,ST,"memgpt/data_types.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure External Functions: Set up external functions that return tokens in the system.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Invoke external functions that return tokens.
6. Observe if there are any safeguards or validations on the tokens returned by these external functions, and check for potential issues or vulnerabilities related to token handling."
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da3,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"pentestgpt/prompts/prompt_class.py
pentestgpt/prompts/prompt_class_v1.py
pentestgpt/utils/pentest_gpt.py",Upload the pptx file.
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da4,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"pentestgpt/utils/prompt_select.py
pentestgpt/prompts/prompt_class_v2.py
pentestgpt/utils/pentest_gpt.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da3,exceeding  LLM content limit,/,Memory context limit exceeded ,ST,"pentestgpt/utils/llm_api.py
pentestgpt/utils/chatgpt.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da3,insufficient history management,/,Test history is not properly handled,IC,"pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py
pentestgpt/utils/APIs/gpt4all_api.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed."
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da4,sketchy error handling,case1,Add handler for repeated commands.,"IC,SL",pentestgpt/utils/chatgpt.py,"1.Set up PentestGPT: Ensure PentestGPT is correctly set up and running in your local environment following the setup instructions in the project's README.md.
2.Configure the Environment: Open the config.py file located in the project directory.Set the API_KEY parameter with your valid API key.Ensure other configuration parameters are set to their default values unless specified otherwise.
3.Initiate a Scan: Start a pentesting scan using the following command:python pentest.py --target example.com
4.Monitor the Output: Observe the output in the terminal and note if the scan process initiates correctly.
5.Trigger the Error: During the scan process, attempt to run a specific module or test that requires elevated privileges or specific network access. For example:python pentest.py --target example.com --module network_scan
6.Observe the Error: Note if an error occurs indicating a failure in the scan process. The error message should be similar to:PermissionError: [Errno 13] Permission denied
Alternatively, the error might relate to missing dependencies or incorrect configuration, such as:ModuleNotFoundError: No module named 'some_required_module'
7.Check Logs: Review the application logs and console output for any additional error messages or stack traces related to the failure."
GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da3,,case2,Executing potentially harmful codes in terminal,IS,pentestgpt/utils/chatgpt.py,"1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Terminal Access: Verify that the system has access to the terminal or command-line interface.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the terminal interface in the web UI.
5. Input a command that could be potentially harmful, such as: `rm -rf /` or any command that might pose a risk.
6. Observe if the system executes the command and handles potentially harmful inputs appropriately, including any safeguards or error handling in place."
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"pentestgpt/utils/pentest_gpt.py
pentestgpt/utils/pentest_gpt_rebuilt.py",Upload the pptx file.
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"pentestgpt/utils/prompt_select.py
pentestgpt/prompts/prompt_class_v2.py
pentestgpt/utils/pentest_gpt.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,exceeding  LLM content limit,/,Memory context limit exceeded ,ST,"pentestgpt/utils/llm_api.py
pentestgpt/utils/chatgpt.py
pentestgpt/prompts/prompt_class_v1.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,insufficient history management,/,Test history is not properly handled,IC,"pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py
pentestgpt/utils/APIs/gpt4all_api.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed."
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,sketchy error handling,case1,Add handler for repeated commands.,"IC,SL","pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py","1.Set up PentestGPT: Ensure PentestGPT is correctly set up and running in your local environment following the setup instructions in the project's README.md.
2.Configure the Environment: Open the config.py file located in the project directory.Set the API_KEY parameter with your valid API key.Ensure other configuration parameters are set to their default values unless specified otherwise.
3.Initiate a Scan: Start a pentesting scan using the following command:python pentest.py --target example.com
4.Monitor the Output: Observe the output in the terminal and note if the scan process initiates correctly.
5.Trigger the Error: During the scan process, attempt to run a specific module or test that requires elevated privileges or specific network access. For example:python pentest.py --target example.com --module network_scan
6.Observe the Error: Note if an error occurs indicating a failure in the scan process. The error message should be similar to:PermissionError: [Errno 13] Permission denied
Alternatively, the error might relate to missing dependencies or incorrect configuration, such as:ModuleNotFoundError: No module named 'some_required_module'
7.Check Logs: Review the application logs and console output for any additional error messages or stack traces related to the failure."
0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,,case2,Executing potentially harmful codes in terminal,IS,"pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Terminal Access: Verify that the system has access to the terminal or command-line interface.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the terminal interface in the web UI.
5. Input a command that could be potentially harmful, such as: `rm -rf /` or any command that might pose a risk.
6. Observe if the system executes the command and handles potentially harmful inputs appropriately, including any safeguards or error handling in place."
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"prompts/prompt_class_old.py
utils/pentest_gpt.py
prompts/prompt_class.py",Upload the pptx file.
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"prompts/prompt_class_old.py
prompts/prompt_class.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,exceeding  LLM content limit,,Memory context limit exceeded ,ST,"utils/chatgpt_api.py
utils/chatgpt.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,insufficient history management,/,Test history is not properly handled,IC,"utils/chatgpt_browser.py
utils/chatgpt_api.py
utils/chatgpt.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed."
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,sketchy error handling,case1,Add handler for repeated commands.,"IC,SL","utils/chatgpt_api.py
utils/chatgpt.py","1.Set up PentestGPT: Ensure PentestGPT is correctly set up and running in your local environment following the setup instructions in the project's README.md.
2.Configure the Environment: Open the config.py file located in the project directory.Set the API_KEY parameter with your valid API key.Ensure other configuration parameters are set to their default values unless specified otherwise.
3.Initiate a Scan: Start a pentesting scan using the following command:python pentest.py --target example.com
4.Monitor the Output: Observe the output in the terminal and note if the scan process initiates correctly.
5.Trigger the Error: During the scan process, attempt to run a specific module or test that requires elevated privileges or specific network access. For example:python pentest.py --target example.com --module network_scan
6.Observe the Error: Note if an error occurs indicating a failure in the scan process. The error message should be similar to:PermissionError: [Errno 13] Permission denied
Alternatively, the error might relate to missing dependencies or incorrect configuration, such as:ModuleNotFoundError: No module named 'some_required_module'
7.Check Logs: Review the application logs and console output for any additional error messages or stack traces related to the failure."
vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,,case2,Executing potentially harmful codes in terminal,IS,"utils/chatgpt_api.py
utils/chatgpt.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Terminal Access: Verify that the system has access to the terminal or command-line interface.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the terminal interface in the web UI.
5. Input a command that could be potentially harmful, such as: `rm -rf /` or any command that might pose a risk.
6. Observe if the system executes the command and handles potentially harmful inputs appropriately, including any safeguards or error handling in place."
Devansh968/Chat-with-Pdf-using-Qdrant-vector-database,https://github.com/Devansh968/Chat-with-Pdf-using-Qdrant-vector-database/tree/771195f36a735a6f4ac1b63edc04661c11a67f3f,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"site-packages/rich/console.py
-/Lib/site-packages/streamlit/elements/number_input.py
-/Lib/site-packages/streamlit/elements/camera_input.py",Upload the pptx file.
Devansh968/Chat-with-Pdf-using-Qdrant-vector-database,https://github.com/Devansh968/Chat-with-Pdf-using-Qdrant-vector-database/tree/771195f36a735a6f4ac1b63edc04661c11a67f3f,Unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'"
Devansh968/Chat-with-Pdf-using-Qdrant-vector-database,https://github.com/Devansh968/Chat-with-Pdf-using-Qdrant-vector-database/tree/771195f36a735a6f4ac1b63edc04661c11a67f3f,exceeding  LLM content limit,/,Memory context limit exceeded ,ST,"/Lib/site-packages/markdown_it/token.py
-/Lib/site-packages/packaging/_tokenizer.py
-/Lib/site-packages/sqlalchemy/orm/path_registry.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly."
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"/Lib/site-packages/rich/console.py
/Lib/site-packages/streamlit/elements/number_input.py
/Lib/site-packages/streamlit/elements/camera_input.py",Upload the pptx file.
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,lacking restrictions in prompt,case1,Limiting input length to prevent out of memory issues,"ST,UI","/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py
","chatiq/prompt.py
chatiq/chat_chain.py"
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,,case2,Prompting in another language sometimes gives an English answer,IC,"/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py
","chatiq/text_processor.py
chatiq/chatiq.py"
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,Unclear context in prompt,case1,Query giving wrong citations and answers out of the document,IC,"/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py
","projects/app/src/pages/api/core/ai/token.ts
files/models/ChatGLM2/openai_api.py
python/bge-rerank/bge-reranker-base/app.py"
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,,case2,When trained on PDF I get results other than the content of the PDF ,ST,"/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py
",packages/service/common/file/read
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,,case3,can't limit Chatdoc responses only to documents added,IC,"/Lib/site-packages/click/termui.py
-/Lib/site-packages/rich/prompt.py
-/Lib/site-packages/langchain/chains/prompt_selector.py
-/Lib/site-packages/langchain/schema/prompt_template.py
","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe if the response includes information not contained in the uploaded documents."
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,knowledge misalignment,/,Error when parquet files get too big / function for splitting?,ST,"/Lib/site-packages/openai/api_resources/embedding.py
-/Lib/site-packages/openai/embeddings_utils.py
-/Lib/site-packages/langchain/vectorstores/pgembedding.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Prepare a large Parquet file for testing.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Upload the large Parquet file using the web UI.
5. Observe if an error occurs when handling the large Parquet file.
6. Check if there is a function or option for splitting the Parquet file to avoid errors."
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,Imprecise knowledge retrieval,/,"Asking a question gives ""Index not found, please create an instance before querying""",ST,"/Lib/site-packages/langchain/retrievers/merger_retriever.py
-/Lib/site-packages/langchain/chains/qa_with_sources/retrieval.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a question related to the content of the uploaded document.For example, ""Summarize the key points of the document.""
6. Observe if the response includes the error message ""Index not found, please create an instance before querying.""or ""ValueError: invalid literal for int() with base 10: 'some_value'"""
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,exceeding  LLM content limit,case1,Responses get cut off in the middle.,IC,"Lib/site-packages/markdown_it/token.py
-/Lib/site-packages/packaging/_tokenizer.py
-/Lib/site-packages/sqlalchemy/orm/path_registry.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it gets cut off in the middle."
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,,case2,The answers get cut off in the middle when it gives longer answers,IC,"Lib/site-packages/markdown_it/token.py
-/Lib/site-packages/packaging/_tokenizer.py
-/Lib/site-packages/sqlalchemy/orm/path_registry.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed, longer response.
6. Observe the response to check if it gets cut off in the middle when providing longer answers."
marella/chatdocs,https://github.com/marella/chatdocs/tree/ff0f962972ba65f0bccbf2e81875a95bae7473b5,,case3,The response of a query is incomplete.There is no way to continue lost response currently through a new prompt and splitting the prompts into smaller chunks resulted in a lost of context when responding.,IC,"Lib/site-packages/markdown_it/token.py
-/Lib/site-packages/packaging/_tokenizer.py
-/Lib/site-packages/sqlalchemy/orm/path_registry.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed response.
6. Observe if the response is incomplete and whether there is currently no way to continue the lost response through a new prompt.
7. Attempt to split the query into smaller chunks and note if this results in a loss of context in the responses."
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,chatdocs/chat.py,Upload the pptx file.
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,lacking restrictions in prompt,case1,Limiting input length to prevent out of memory issues,"ST,UI",chatdocs/chat.py,"chatiq/prompt.py
chatiq/chat_chain.py"
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,,case2,Prompting in another language sometimes gives an English answer,IC,chatdocs/chat.py,"chatiq/text_processor.py
chatiq/chatiq.py"
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,Unclear context in prompt,case1,Query giving wrong citations and answers out of the document,IC,chatdocs/chat.py,"projects/app/src/pages/api/core/ai/token.ts
files/models/ChatGLM2/openai_api.py
python/bge-rerank/bge-reranker-base/app.py"
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,,case2,When trained on PDF I get results other than the content of the PDF ,ST,chatdocs/chat.py,packages/service/common/file/read
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,,case3,can't limit Chatdoc responses only to documents added,IC,chatdocs/chat.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe if the response includes information not contained in the uploaded documents."
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,knowledge misalignment,/,Error when parquet files get too big / function for splitting?,ST,"chatdocs/embeddings.py
chatdocs/add.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Prepare a large Parquet file for testing.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Upload the large Parquet file using the web UI.
5. Observe if an error occurs when handling the large Parquet file.
6. Check if there is a function or option for splitting the Parquet file to avoid errors."
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,Imprecise knowledge retrieval,/,"Asking a question gives ""Index not found, please create an instance before querying""",ST,"chatdocs/embeddings.py
chatdocs/add.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a question related to the content of the uploaded document.For example, ""Summarize the key points of the document.""
6. Observe if the response includes the error message ""Index not found, please create an instance before querying.""or ""ValueError: invalid literal for int() with base 10: 'some_value'"""
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,exceeding  LLM content limit,case1,Responses get cut off in the middle.,IC,"chatdocs/llms.py
chatdocs/ui.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it gets cut off in the middle."
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,,case2,The answers get cut off in the middle when it gives longer answers,IC,"chatdocs/llms.py
chatdocs/ui.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed, longer response.
6. Observe the response to check if it gets cut off in the middle when providing longer answers."
ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,,case3,The response of a query is incomplete.There is no way to continue lost response currently through a new prompt and splitting the prompts into smaller chunks resulted in a lost of context when responding.,IC,"chatdocs/llms.py
chatdocs/ui.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed response.
6. Observe if the response is incomplete and whether there is currently no way to continue the lost response through a new prompt.
7. Attempt to split the query into smaller chunks and note if this results in a loss of context in the responses."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"chatdocs/chat.py
chatdocs/pages/embeddings_viz.py
chatdocs/document_loaders/nougat_loader.py",Upload the pptx file.
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,lacking restrictions in prompt,case1,Limiting input length to prevent out of memory issues,"ST,UI",chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Configure Input Length Limit: Set a specific limit for input length in the configuration file.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the input interface in the web UI.
5. Enter a lengthy input that exceeds the configured limit.
6. Observe if the system properly enforces the input length limit and prevents out-of-memory issues."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,,case2,Prompting in another language sometimes gives an English answer,IC,chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query in a language other than English.
6. Observe if the response is given in English instead of the language of the query."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,Unclear context in prompt,case1,Query giving wrong citations and answers out of the document,IC,chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it includes wrong citations or answers that are not from the document."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,,case2,When trained on PDF I get results other than the content of the PDF ,ST,chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for training.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded PDF document, e.g Summarize the key points of the document..
6. Observe the results to check if the responses include information that is not from the content of the PDF document.(similar to AttributeError: 'NoneType' object has no attribute 'some_method')"
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,,case3,can't limit Chatdoc responses only to documents added,IC,chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe if the response includes information not contained in the uploaded documents."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,knowledge misalignment,/,Error when parquet files get too big / function for splitting?,ST,"chatdocs/pages/embeddings_viz.py
chatdocs/embeddings.py
chatdocs/vectorstores.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Prepare a large Parquet file for testing.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Upload the large Parquet file using the web UI.
5. Observe if an error occurs when handling the large Parquet file.
6. Check if there is a function or option for splitting the Parquet file to avoid errors."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,Imprecise knowledge retrieval,/,"Asking a question gives ""Index not found, please create an instance before querying""",ST,"chatdocs/ui.py
chatdocs/chains.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a question related to the content of the uploaded document.For example, ""Summarize the key points of the document.""
6. Observe if the response includes the error message ""Index not found, please create an instance before querying.""or ""ValueError: invalid literal for int() with base 10: 'some_value'"""
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,exceeding  LLM content limit,case1,Responses get cut off in the middle.,IC,"chatdocs/chat.py
chatdocs/ui.py
chatdocs/llms.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it gets cut off in the middle."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,,case2,The answers get cut off in the middle when it gives longer answers,IC,"chatdocs/chat.py
chatdocs/ui.py
chatdocs/llms.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed, longer response.
6. Observe the response to check if it gets cut off in the middle when providing longer answers."
Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,,case3,The response of a query is incomplete.There is no way to continue lost response currently through a new prompt and splitting the prompts into smaller chunks resulted in a lost of context when responding.,IC,"chatdocs/chat.py
chatdocs/ui.py
chatdocs/llms.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed response.
6. Observe if the response is incomplete and whether there is currently no way to continue the lost response through a new prompt.
7. Attempt to split the query into smaller chunks and note if this results in a loss of context in the responses."
Haste171/langchain-chatbot,https://github.com/Haste171/langchain-chatbot/tree/f3999530d57eb8069d8b911024785d82ac5d003d,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,handlers/base.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Haste172/langchain-chatbot,https://github.com/Haste171/langchain-chatbot/tree/f3999530d57eb8069d8b911024785d82ac5d003d,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"ui/main.py
startup.py
endpoints/chat.py","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
Haste171/langchain-chatbot,https://github.com/Haste171/langchain-chatbot/tree/f3999530d57eb8069d8b911024785d82ac5d003d,Out-of-sync LLM downstream tasks,/,Rate limit while ingesting,ST,handlers/base.py,"1. Set up LangChain Chatbot: Ensure the project is correctly set up in your local environment.
2. Configure Rate Limits: Set up any rate limit configurations in the environment or settings file.
3. Prepare a batch of documents for ingestion.
4. Run LangChain Chatbot: Execute `python langchain_chatbot.py` or the relevant command to start the server.
5. Start the ingestion process for the batch of documents.
6. Observe if the system encounters rate limit issues during the ingestion process."
Haste172/langchain-chatbot,https://github.com/Haste171/langchain-chatbot/tree/f3999530d57eb8069d8b911024785d82ac5d003d,insufficient history management,/,chat history in streamlit doesn't seem to work,IC,"endpoints/chat.py
handlers/base.py
ui/main.py","1. Set up LangChain Chatbot: Ensure the project is correctly set up in your local environment.
2. Run LangChain Chatbot: Execute `python langchain_chatbot.py` or the relevant command to start the server.
3. Open the Streamlit interface in your web browser.
4. Start a chat session by entering a query in the chat interface.
5. Continue the chat with multiple queries to build a chat history.
6. Observe if the chat history is correctly displayed and updated in the Streamlit interface."
Haste173/langchain-chatbot,https://github.com/Haste171/langchain-chatbot/tree/f3999530d57eb8069d8b911024785d82ac5d003d,exceeding  LLM content limit,/,Token Limitation,ST,handlers/base.py,"1. Set up LangChain Chatbot: Ensure the project is correctly set up in your local environment.
2. Configure Token Limit: Set a specific token limit in the configuration file.
3. Run LangChain Chatbot: Execute `python langchain_chatbot.py` or the relevant command to start the server.
4. Open the chat interface in the Streamlit web UI.
5. Enter a long query or multiple queries that exceed the configured token limit.
6. Observe if the system correctly handles the token limit and if any errors or truncation issues occur."
iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"model/base.py
model/chatglm_llm.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,web.py,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,"model/chatglm_llm.py
web.py","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
mayooear/gpt4-pdf-chatbot-langchain,https://github.com/mayooear/gpt4-pdf-chatbot-langchain/tree/66d183f6b207fa1d92153a430c620c58b01e9b1c,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"utils/makechain.ts
pages/index.tsx
pages/api/chat.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
mayooear/gpt5-pdf-chatbot-langchain,https://github.com/mayooear/gpt4-pdf-chatbot-langchain/tree/66d183f6b207fa1d92153a430c620c58b01e9b1c,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,utils/makechain.ts,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
mayooear/gpt5-pdf-chatbot-langchain,https://github.com/mayooear/gpt4-pdf-chatbot-langchain/tree/66d183f6b207fa1d92153a430c620c58b01e9b1c,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"utils/makechain.ts
pages/index.tsx
utils/cn.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
mayooear/gpt6-pdf-chatbot-langchain,https://github.com/mayooear/gpt4-pdf-chatbot-langchain/tree/66d183f6b207fa1d92153a430c620c58b01e9b1c,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,ingest-data.ts,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
davideuler/gpt4-pdf-chatbot-langchain-chromadb,https://github.com/davideuler/gpt4-pdf-chatbot-langchain-chromadb/tree/5c0f8daba056483afebf9e8a46a988e07c58ee99,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,utils/makechain.ts,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
davideuler/gpt4-pdf-chatbot-langchain-chromadb,https://github.com/davideuler/gpt4-pdf-chatbot-langchain-chromadb/tree/5c0f8daba056483afebf9e8a46a988e07c58ee99,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"pages/index.tsx
pages/api/chat.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
davideuler/gpt4-pdf-chatbot-langchain-chromadb,https://github.com/davideuler/gpt4-pdf-chatbot-langchain-chromadb/tree/5c0f8daba056483afebf9e8a46a988e07c58ee99,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"pages/index.tsx
utils/cn.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
davideuler/gpt4-pdf-chatbot-langchain-chromadb,https://github.com/davideuler/gpt4-pdf-chatbot-langchain-chromadb/tree/5c0f8daba056483afebf9e8a46a988e07c58ee99,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,utils/makechain.ts,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
groovybits/gaib,https://github.com/groovybits/gaib/tree/887e8edf7f6ce112af4790ac5f45c629b05e3225,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"config/personalityPrompts.ts
pages/api/openai.ts","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
groovybits/gaib,https://github.com/groovybits/gaib/tree/887e8edf7f6ce112af4790ac5f45c629b05e3225,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"pages/api/chat.ts
scripts/twitchChat.ts
config/personalityPrompts.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
groovybits/gaib,https://github.com/groovybits/gaib/tree/887e8edf7f6ce112af4790ac5f45c629b05e3225,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"scripts/twitchChat.ts
scripts/ingest-data.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
groovybits/gaib,https://github.com/groovybits/gaib/tree/887e8edf7f6ce112af4790ac5f45c629b05e3225,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,"components/TokensDropdown.tsx
utils/makechain.ts
pages/api/chat.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
sagarsaija/gpt4-pdf-chatbot-langchain-chroma,https://github.com/sagarsaija/gpt4-pdf-chatbot-langchain-chroma/tree/259d5cb6510b0c06b93389810671d73482f9a37c,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,utils/makechain.ts,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
sagarsaija/gpt4-pdf-chatbot-langchain-chroma,https://github.com/sagarsaija/gpt4-pdf-chatbot-langchain-chroma/tree/259d5cb6510b0c06b93389810671d73482f9a37c,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"pages/index.tsx
pages/api/chat.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
sagarsaija/gpt4-pdf-chatbot-langchain-chroma,https://github.com/sagarsaija/gpt4-pdf-chatbot-langchain-chroma/tree/259d5cb6510b0c06b93389810671d73482f9a37c,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"pages/index.tsx
utils/cn.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
sagarsaija/gpt4-pdf-chatbot-langchain-chroma,https://github.com/sagarsaija/gpt4-pdf-chatbot-langchain-chroma/tree/259d5cb6510b0c06b93389810671d73482f9a37c,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,utils/makechain.ts,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
oshoura/IslamAI,https://github.com/oshoura/IslamAI/tree/d39fad996040c1fe7917b330996620380b9028af,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,utils/makechain.ts,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
oshoura/IslamAI,https://github.com/oshoura/IslamAI/tree/d39fad996040c1fe7917b330996620380b9028af,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"pages/index.tsx
pages/api/chat.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
oshoura/IslamAI,https://github.com/oshoura/IslamAI/tree/d39fad996040c1fe7917b330996620380b9028af,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"pages/index.tsx
utils/cn.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
oshoura/IslamAI,https://github.com/oshoura/IslamAI/tree/d39fad996040c1fe7917b330996620380b9028af,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,utils/makechain.ts,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws,https://github.com/arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws/tree/5259ae39cd23892c3d0030b98ae99148fb5b7284,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,utils/makechain.ts,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws,https://github.com/arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws/tree/5259ae39cd23892c3d0030b98ae99148fb5b7284,insufficient history management,/,Chat history is not included in the prompt (model is not aware of previous interactions),IC,"pages/index.tsx
pages/api/chat.ts
utils/makechain.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query and observe the response.
6. Enter a follow-up query that relies on the context of the previous interaction.
7. Observe if the response demonstrates awareness of the previous interactions or if it treats each query as an isolated interaction.(similar to ""AttributeError: 'NoneType' object has no attribute 'extract_text'"")"
arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws,https://github.com/arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws/tree/5259ae39cd23892c3d0030b98ae99148fb5b7284,Missing LLM input format validation,/,This project can't accept images read from PDF. ,IC,"pages/index.tsx
utils/makechain.ts
utils/cn.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure."
arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws,https://github.com/arndvs/gpt4-langchain-ingest-api-data-private-chroma-aws/tree/5259ae39cd23892c3d0030b98ae99148fb5b7284,exceeding  LLM content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,"config/chroma.ts
pages/api/chat.ts
scripts/ingest-chroma-via-api.ts","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes."""
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"frontend/src/components/WorkspaceChat/ChatContainer/PromptInput/index.jsx
server/utils/AiProviders/groq/index.js
server/utils/helpers/chat/convertTo.js","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"server/utils/EmbeddingEngines/openAi/index.js
frontend/src/components/WorkspaceChat/ChatContainer/PromptInput/index.jsx",Upload the pptx file.
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,improper text embedding,/,User Query embeddings are being chunked per character when using LM Studio embedding models,"IC,TK","frontend/src/utils/paths.js
frontend/src/components/EmbeddingSelection/CohereOptions/index.jsx","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure LM Studio Embedding Models: Ensure the LM Studio embedding models are properly integrated.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a user query for embedding.
6. Observe the embedding process to check if the query embeddings are being chunked per character instead of by word or sentence.(for example,KeyError: 'API_KEY') 
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure. Pay special attention to errors related to missing environment variables or incorrect configurations."
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,exceeding  LLM content limit,case1,"Azure OpenAI Embed API says: ""Too many inputs. The max number of inputs is 16.""",ST,"server/utils/helpers/tiktoken.js
collector/utils/tokenizer/index.js","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure Azure OpenAI Embed API: Integrate the Azure OpenAI Embed API with the project.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Prepare a batch of inputs larger than 16.
6. Submit the batch of inputs to the Azure OpenAI Embed API.
7. Observe if an error occurs with the message ""Too many inputs. The max number of inputs is 16.""or ""IndexError: list index out of range"""
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,,case2,OpenAI 400 Error on long(ish) chat history,ST,"server/utils/helpers/tiktoken.js
collector/utils/tokenizer/index.js","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure OpenAI API: Ensure the OpenAI API is properly integrated.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Engage in a chat session by entering multiple queries and responses to build a long chat history.
6. Observe if an OpenAI 400 Error occurs when the chat history becomes long, indicating the issue with handling extended chat sessions.(for example, FileNotFoundError: [Errno 2] No such file or directory: 'path/to/document')"
Mintplex-Labs/anything-llm,https://github.com/Mintplex-Labs/anything-llm/tree/ebd3a62866e9e17b07a4a973b74329c0297fb78a,,case3,"Selecting more docs to embed force-restarts the server, embedding only one doc",ST,"server/utils/helpers/tiktoken.js
collector/utils/tokenizer/index.js","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
3. Open the document embedding interface in the web UI.
4. Select and upload multiple documents for embedding.
5. Observe if the server force-restarts during the process.
6. Check if only one document is embedded after the restart, despite multiple documents being selected initially.(for example, ""TypeError: 'NoneType' object is not iterable"")"
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"server/utils/chats/index.js
frontend/src/components/WorkspaceChat/ChatContainer/ChatHistory/PromptReply/index.jsx","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"collector/watch.py
server/utils/vectorDbProviders/lance/index.js
frontend/src/components/WorkspaceChat/ChatContainer/PromptInput/index.jsx",Upload the pptx file.
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,improper text embedding,/,User Query embeddings are being chunked per character when using LM Studio embedding models,IC,server/utils/vectorDbProviders/lance/index.js,"1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure LM Studio Embedding Models: Ensure the LM Studio embedding models are properly integrated.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a user query for embedding.
6. Observe the embedding process to check if the query embeddings are being chunked per character instead of by word or sentence.(for example,KeyError: 'API_KEY') 
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure. Pay special attention to errors related to missing environment variables or incorrect configurations."
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,exceeding  LLM content limit,case1,"Azure OpenAI Embed API says: ""Too many inputs. The max number of inputs is 16.""",ST,"frontend/src/models/system.js
collector/scripts/utils.py
collector/scripts/link.py","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure Azure OpenAI Embed API: Integrate the Azure OpenAI Embed API with the project.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Prepare a batch of inputs larger than 16.
6. Submit the batch of inputs to the Azure OpenAI Embed API.
7. Observe if an error occurs with the message ""Too many inputs. The max number of inputs is 16.""or ""IndexError: list index out of range"""
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,,case2,OpenAI 400 Error on long(ish) chat history,ST,"frontend/src/models/system.js
collector/scripts/utils.py
collector/scripts/link.py","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Configure OpenAI API: Ensure the OpenAI API is properly integrated.
3. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Engage in a chat session by entering multiple queries and responses to build a long chat history.
6. Observe if an OpenAI 400 Error occurs when the chat history becomes long, indicating the issue with handling extended chat sessions.(for example, FileNotFoundError: [Errno 2] No such file or directory: 'path/to/document')"
frasergr/anything-llm,https://github.com/frasergr/anything-llm/tree/5fa61458724fd019a15e243b8db8e71bead950b5,,case3,"Selecting more docs to embed force-restarts the server, embedding only one doc",ST,"frontend/src/models/system.js
collector/scripts/utils.py
collector/scripts/link.py","1. Set up Anything-LLM: Ensure the project is correctly set up in your local environment.
2. Run Anything-LLM: Execute `python app.py` or the relevant command to start the server.
3. Open the document embedding interface in the web UI.
4. Select and upload multiple documents for embedding.
5. Observe if the server force-restarts during the process.
6. Check if only one document is embedded after the restart, despite multiple documents being selected initially.(for example, ""TypeError: 'NoneType' object is not iterable"")"
alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,unnecessary LLM output,/,The last sentence in the generated output is repeated multiple times. ,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires generating a detailed response.
6. Observe the generated output to check if the last sentence is repeated multiple times."
alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,exceeding  LLM content limit,/,"InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6777 tokens. Please reduce the length of the messages",ST,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a very detailed or lengthy query that is likely to exceed the model's maximum context length.
6. Observe if an `InvalidRequestError` occurs with a message indicating that the model's maximum context length of 4097 tokens has been exceeded by your messages, resulting in 6777 tokens."
alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,Missing LLM input format validation,/,Every time you have to reload the PDF file,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the uploaded PDF document.
6. After receiving the response, navigate away from the query interface or refresh the page.
7. Return to the query interface.
8. Attempt to enter another query related to the initially uploaded PDF document.
9. Observe if you need to reload the PDF file every time you want to query it again."
aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,unnecessary LLM output,/,The last sentence in the generated output is repeated multiple times. ,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires generating a detailed response.
6. Observe the generated output to check if the last sentence is repeated multiple times."
aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,exceeding  LLM content limit,/,"InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6777 tokens. Please reduce the length of the messages",ST,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a very detailed or lengthy query that is likely to exceed the model's maximum context length.
6. Observe if an `InvalidRequestError` occurs with a message indicating that the model's maximum context length of 4097 tokens has been exceeded by your messages, resulting in 6777 tokens."
aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,Missing LLM input format validation,/,Every time you have to reload the PDF file,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the uploaded PDF document.
6. After receiving the response, navigate away from the query interface or refresh the page.
7. Return to the query interface.
8. Attempt to enter another query related to the initially uploaded PDF document.
9. Observe if you need to reload the PDF file every time you want to query it again."
mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"utils.py
chat.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"chat.py
question_answer_docs.py",Upload the pptx file.
mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"chat.py
ingest.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"app/models/llms.py
app/models/completion_models.py
app/utils/chat/commands/prompt.py
app/models/base_models.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"app/utils/chat/text_generations/__init__.py
app/models/base_models.py
app/models/llms.py",Upload the pptx file.
c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"app/utils/auth/token.py
app/models/base_models.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,app.py,Upload the pptx file.
dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,exceeding  LLM content limit,/,can't upload multiples files and ask Q&A from.,ST,app.py,"1. Set up PDFChat: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple PDF files.
5. Enter a query that requires information from the content of the uploaded PDF files.
6. Observe if the application fails to handle multiple file uploads and if you encounter issues when asking questions based on the content of the multiple files."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,exceeding  LLM content limit,case1,can't upload multiples csv files for the chatbot,ST,"src/modules/robby_sheet/table_tool.py
src/modules/chatbot.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,,case2,Exceeds token limit,ST,"src/modules/robby_sheet/table_tool.py
src/modules/chatbot.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a lengthy query or a series of queries that are likely to exceed the token limit.
6. Observe if an error occurs indicating that the token limit has been exceeded."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,,case3,Maximum context length error,ST,"src/modules/robby_sheet/table_tool.py
src/modules/chatbot.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a series of queries or a single lengthy query that is likely to exceed the maximum context length.
6. Observe if an error message appears indicating that the maximum context length has been exceeded."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,Imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"tuto_chatbot_csv.py
src/modules/embedder.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,Missing LLM input format validation,case1,Problem loading XLSX file,IC,tuto_chatbot_csv.py,"1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)"
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,,case2,Add text file support,ST,tuto_chatbot_csv.py,"1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload a text (.txt) file.
5. Observe if the application successfully accepts and processes the text file.
6. Enter a query related to the content of the uploaded text file to verify if the chatbot can handle and respond based on the text file's content."
yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,Unclear context in prompt,/,Limit the answers to the file only.,IC,"src/modules/layout.py
src/modules/history.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')"
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,exceeding  LLM content limit,case1,can't upload multiples csv files for the chatbot,ST,modules/chatbot.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot."
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,,case2,Exceeds token limit,ST,modules/chatbot.py,"1. Set up : Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a lengthy query or a series of queries that are likely to exceed the token limit.
6. Observe if an error occurs indicating that the token limit has been exceeded."
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,,case3,Maximum context length error,ST,modules/chatbot.py,"1. Set up : Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a series of queries or a single lengthy query that is likely to exceed the maximum context length.
6. Observe if an error message appears indicating that the maximum context length has been exceeded."
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,Imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"modules/chatbot.py
modules/embedder.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file."
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,Missing LLM input format validation,case1,Problem loading XLSX file,IC,modules/chatbot.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)"
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,,case2,Add text file support,ST,modules/chatbot.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload a text (.txt) file.
5. Observe if the application successfully accepts and processes the text file.
6. Enter a query related to the content of the uploaded text file to verify if the chatbot can handle and respond based on the text file's content."
gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,Unclear context in prompt,/,Limit the answers to the file only.,IC,modules/history.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')"
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,exceeding  LLM content limit,case1,can't upload multiples csv files for the chatbot,ST,"src/modules/chatbot.py
src/modules/sidebar.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot."
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,,case2,Exceeds token limit,ST,"src/modules/chatbot.py
src/modules/sidebar.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a lengthy query or a series of queries that are likely to exceed the token limit.
6. Observe if an error occurs indicating that the token limit has been exceeded."
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,,case3,Maximum context length error,ST,"src/modules/chatbot.py
src/modules/sidebar.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a document or dataset for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a series of queries or a single lengthy query that is likely to exceed the maximum context length.
6. Observe if an error message appears indicating that the maximum context length has been exceeded."
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,Imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"src/modules/chatbot.py
src/modules/embedder.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file."
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,Missing LLM input format validation,case1,Problem loading XLSX file,IC,"src/modules/layout.py
src/modules/utils.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)"
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,,case2,Add text file support,ST,"src/modules/layout.py
src/modules/utils.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload a text (.txt) file.
5. Observe if the application successfully accepts and processes the text file.
6. Enter a query related to the content of the uploaded text file to verify if the chatbot can handle and respond based on the text file's content."
chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,Unclear context in prompt,/,Limit the answers to the file only.,IC,"src/modules/layout.py
src/modules/chatbot.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')"
Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"openapi.yaml
main.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"openapi.yaml
main.py",Upload the pptx file.
Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"openapi.yaml
main.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"openapi.yaml
main.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,Missing LLM input format validation,/,The analysis of PPT documents needs optimization,IC,"openapi.yaml
main.py",Upload the pptx file.
benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"openapi.yaml
main.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
codeacme17/examor,https://github.com/codeacme17/examor/tree/891ed727b711c6b59e3c6899f7fa08f0df773c30,Imprecise knowledge retrieval,/,"After installation, there are no errors when importing the Markdown file, but the questions cannot be extracted. ",ST,"server/llm_services/langchain_llm.py
server/llm_services/pure_llm.py","1. Set up Examor: Ensure the project is correctly set up in your local environment.
2. Install the application by following the provided installation instructions.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the import interface in the web UI.
5. Attempt to import a Markdown file containing questions.
6. Observe if there are no errors during the import process.
7. Try to extract questions from the imported Markdown file.
8. Observe if the questions are not extracted despite the successful import."
codeacme17/examor,https://github.com/codeacme17/examor/tree/891ed727b711c6b59e3c6899f7fa08f0df773c30,Missing LLM input format validation,/,An error message is displayed when the md file is uploaded,ST,"next/components/ui/input.tsx
next/components/share/password-input.tsx","1. Set up Examor: Ensure the project is correctly set up in your local environment.
2. Install the application by following the provided installation instructions.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Attempt to upload a Markdown (.md) file.
6. Observe if an error message is displayed during the upload process."
codeacme17/examor,https://github.com/codeacme17/examor/tree/891ed727b711c6b59e3c6899f7fa08f0df773c30,exceeding  LLM content limit,/,"If the ORGANIZATION string set in the profile is too long, the profile update fails.",ST,"server/loaders/share.py
next/langchain/loader/share.ts","1. Set up Examor: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the profile settings interface in the web UI.
4. Enter a very long string for the ORGANIZATION field in the profile.
5. Attempt to save the profile update.
6. Observe if the profile update fails when the ORGANIZATION string is too long."
junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,Missing LLM input format validation,/,pdf with only pictures and no text will cause the list index out of range,IC,"main.py
convo_qa_chain.py","1. Set up IncarnaMind: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains only pictures and no text.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded PDF document.
6. Observe if a ""list index out of range"" error occurs when processing the PDF with only pictures and no text.(for example, AttributeError: 'NoneType' object has no attribute 'some_method')"
junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"toolkit/prompts.py
convo_qa_chain.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"toolkit/together_api_llm.py
docs2db.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,Missing LLM input format validation,/,File type detection for code documentation ,ST,"pr_agent/algo/utils.py
pr_agent/servers/bitbucket_app.py
","1. Set up PR-Agent: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the code documentation interface in the web UI.
4. Upload various types of code files (e.g., .py, .js, .java, .cpp).
5. Observe if the application correctly detects and categorizes the file types for code documentation purposes.(for example, ValueError: Invalid URL 'None')"
Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"pr_agent/algo/token_handler.py
pr_agent/tools/pr_questions.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,"pr_agent/algo/token_handler.py
pr_agent/algo/utils.py","Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"
SolaceDev/pr-agent,https://github.com/SolaceDev/pr-agent/tree/5c768572233adb5fb9eadef21a184410b30b5603,Missing LLM input format validation,/,File type detection for code documentation ,ST,"pr_agent/algo/utils.py
pr_agent/algo/ai_handlers/langchain_ai_handler.py","1. Set up PR-Agent: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the code documentation interface in the web UI.
4. Upload various types of code files (e.g., .py, .js, .java, .cpp).
5. Observe if the application correctly detects and categorizes the file types for code documentation purposes.(for example, ValueError: Invalid URL 'None')"
SolaceDev/pr-agent,https://github.com/SolaceDev/pr-agent/tree/5c768572233adb5fb9eadef21a184410b30b5603,Unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,docs/docs/tools/custom_prompt.md,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?"""
SolaceDev/pr-agent,https://github.com/SolaceDev/pr-agent/tree/5c768572233adb5fb9eadef21a184410b30b5603,exceeding  LLM content limit,/,Requested tokens exceed context window of 2048,ST,pr_agent/algo/token_handler.py,"Here's your request translated into English:

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk"